{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-30 17:21:34,017 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2019-07-30 17:21:45,383 : INFO : Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import logging\n",
    "import traceback\n",
    "import html\n",
    "import matplotlib.pyplot as plt\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from collections import defaultdict\n",
    "from six import iteritems\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings, BertEmbeddings #import only what is needed everyt time?\n",
    "from flair.data import Sentence\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Parameters\n",
    "TRAINING = False\n",
    "UPDATE_EMBEDDINGS = False\n",
    "\n",
    "#Naming the directories for the models\n",
    "dir_name = './models/'\n",
    "corpus_name = os.path.join(dir_name, 'corpus_feedly.mm') #Stemmed Version\n",
    "dict_name = os.path.join(dir_name, 'dictionary_feedly.dict')\n",
    "index_tfidf_name = os.path.join(dir_name, 'fd_index_tfidf.index')\n",
    "index_tfidf_model_name = os.path.join(dir_name, 'fd_model_tfidf.model')\n",
    "index_lsi_name = os.path.join(dir_name, 'fd_index_lsi.index')\n",
    "index_lsi_model_name = os.path.join(dir_name, 'fd_model_lsi.model')\n",
    "index_lda_name = os.path.join(dir_name, 'fd_index_lda.index')\n",
    "index_lda_model_name = os.path.join(dir_name, 'fd_model_lda.model')\n",
    "corpus_name_nst = os.path.join(dir_name, 'corpus_feedly_nst.mm') #Not-Stemmed Version\n",
    "dict_name_nst = os.path.join(dir_name, 'dictionary_feedly_nst.dict')\n",
    "index_tfidf_name_nst = os.path.join(dir_name, 'fd_index_tfidf_nst.index')\n",
    "index_tfidf_model_name_nst = os.path.join(dir_name, 'fd_model_tfidf_nst.model')\n",
    "index_lsi_name_nst = os.path.join(dir_name, 'fd_index_lsi_nst.index')\n",
    "index_lsi_model_name_nst = os.path.join(dir_name, 'fd_model_lsi_nst.model')\n",
    "index_lda_name_nst = os.path.join(dir_name, 'fd_index_lda_nst.index')\n",
    "index_lda_model_name_nst = os.path.join(dir_name, 'fd_model_lda_nst.model')\n",
    "index_WE_model_cs_name_glove = os.path.join(dir_name, 'fd_model_we_cosine_similarity_glove.dat')\n",
    "index_WE_model_eu_name_glove = os.path.join(dir_name, 'fd_model_we_euclidean_distance_glove.dat')\n",
    "index_WE_model_eu_name_paper = os.path.join(dir_name, 'fd_model_we_euclidean_distance_paper_method.dat')\n",
    "index_WE_model_cs_name_flair = os.path.join(dir_name, 'fd_model_we_cosine_similarity_flair.dat')\n",
    "index_WE_model_eu_name_flair = os.path.join(dir_name, 'fd_model_we_euclidean_distance_flair.dat')\n",
    "index_WE_model_cs_name_bert = os.path.join(dir_name, 'fd_model_we_cosine_similarity_bert.dat')\n",
    "index_WE_model_eu_name_bert = os.path.join(dir_name, 'fd_model_we_euclidean_distance_bert.dat')\n",
    "index_WE_model_cs_name_bert_glove = os.path.join(dir_name, 'fd_model_we_cosine_similarity_bert_glove.dat')\n",
    "index_WE_model_eu_name_bert_glove = os.path.join(dir_name, 'fd_model_we_euclidean_distance_bert_glove.dat')\n",
    "index_WE_model_cs_name_flair_glove_news = os.path.join(dir_name, 'fd_model_we_cosine_similarity_flair_glove_news.dat')\n",
    "index_WE_model_eu_name_flair_glove_news = os.path.join(dir_name, 'fd_model_we_euclidean_distance_flair_glove_news.dat')\n",
    "index_WE_model_cs_name_flair_glove_multi = os.path.join(dir_name, 'fd_model_we_cosine_similarity_flair_glove_multi.dat')\n",
    "index_WE_model_eu_name_flair_glove_multi = os.path.join(dir_name, 'fd_model_we_euclidean_distance_flair_glove_multi.dat')\n",
    "index_WE_model_cs_title = os.path.join(dir_name, 'fd_model_we_cs_title.dat')\n",
    "index_WE_model_eu_title = os.path.join(dir_name, 'fd_model_we_eu_title.dat')\n",
    "glovefilename = os.path.join(dir_name, 'glove.42B.300d.txt')\n",
    "\n",
    "if not os.path.isdir(dir_name):\n",
    "    os.mkdir(dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for preprocessing and cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_punctuations = '‘' + '’' + '‚' + '„' + '…' + '``' + '“' + '”' + '£' + '€' + '¥' + '¢' + '₹' + '₱' + '₩' + '฿' + '₫' + '₪' + '‰' + '†' + '‡' + '•' + '¤' + '§' + '©' + '®' + '™' + '℠' + '«' + '»' + '¸' + '·' + '¯' + '¦' + '—'\n",
    "punctuation_marks_extended = string.punctuation.replace('-','') + extended_punctuations\n",
    "def extract_punctuation(text):\n",
    "    \"\"\"\n",
    "    Purpose: Clean the text from any punctuation mark, currency and special symbol.\n",
    "    Input: <List>. List of tokens from a text.\n",
    "    Output: <List>. Cleaned list of tokens from a text.\n",
    "    \"\"\"\n",
    "    processed_punctuation = []\n",
    "    acronymregex = re.compile(r'([A-z]{1}\\.)([A-z]{1}\\.)+') #check for acronyms with punctuation: r'([A-z]{1}\\.)([A-z]{1}\\.)+\\Z'\n",
    "    for word in text:\n",
    "        processed = False\n",
    "        if (len(word) > 1):\n",
    "            if acronymregex.match(word):\n",
    "                word = word.replace('.', '')\n",
    "                processed_punctuation.append(word)\n",
    "            else:\n",
    "                for punctmark in punctuation_marks_extended:\n",
    "                    if word.startswith(punctmark):\n",
    "                        word = word.replace(punctmark,'')\n",
    "                    if word.endswith(punctmark):\n",
    "                        word = word.replace(punctmark,'')\n",
    "                    if '/' not in word:\n",
    "                        if len(word) > 1:\n",
    "                            subwords = word.split(punctmark)\n",
    "                        if(len(subwords) > 1):\n",
    "                            processed = True\n",
    "                            for subword in subwords:\n",
    "                                if (len(subword)> 1): processed_punctuation.append(subword)\n",
    "                    else:\n",
    "                        processed = True\n",
    "                        break\n",
    "                if(processed == False): processed_punctuation.append(word)\n",
    "        #Note: I'm not letting pass one-letter words: unlikely to have a meaning and likely to be a stopword or punctuation mark\n",
    "    return processed_punctuation\n",
    "\n",
    "def convert_numbers_to_specialkey(content):\n",
    "    \"\"\"\n",
    "    Purpose: Transform any sum of money to a common token, so they convey the same meaning.\n",
    "    Input: <String>. Plain text, prefiltered of markups in this case.\n",
    "    Output: <String>. Text where any mention to amounts of money is substituted for the token 'amountofmoney'.\n",
    "    \"\"\"\n",
    "    amount = re.compile(r'([\\$€¥£¢₹₱₩฿₫₪]{1}[0-9]+(,[0-9]+)?)') #modify any amount of money in the document\n",
    "    content = re.sub(amount,'amountofmoney',content)\n",
    "    #numbers = re.compile(r'([0-9]+(,[0-9]+)?([a-z]{1,2})?)') #modify any number quantity in the document\n",
    "    #content = re.sub(numbers,'number',content)\n",
    "    return content\n",
    "\n",
    "def extract_markups(text):\n",
    "    \"\"\"\n",
    "    Purpose: Clean a text from markup <tag> elements.\n",
    "    Input: <String>. Plain text.\n",
    "    Output: <String>. Text cleaned from markups.\n",
    "    Note: It is rather a simple one, as it doesn't distinguish <tag> from <tag>(Stuff)</tag>.\n",
    "    \"\"\"\n",
    "    markups = re.compile(r'(<.*?>)') #remove markups\n",
    "    cltext = re.sub(markups,'',text)\n",
    "    return cltext\n",
    "\n",
    "def extract_stopwords(text):\n",
    "    \"\"\"\n",
    "    Purpose: Remove those words that are general in meaning and do not convey any specific context or topic.\n",
    "    Input: <List>. List of tokens which include stop words.\n",
    "    Output: <List>. List of tokens which do not include stop words.\n",
    "    \"\"\"\n",
    "    return [word for word in text if word not in stopwords.words('english')]\n",
    "\n",
    "def stem_words(text):\n",
    "    \"\"\"\n",
    "    Purpose: Transform words to their lexemes.\n",
    "             This is used for unifying words like 'fast', 'faster' and 'fastest', for example, into one word.\n",
    "             So words with different forms that mean the same semmantic meaning are unified.\n",
    "    Input: <List>. List of tokens.\n",
    "    Output: <List>. List of stemmed tokens.\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = []\n",
    "    for word in text:\n",
    "        stemmed_words.append(stemmer.stem(word))\n",
    "    return stemmed_words\n",
    "\n",
    "def cleanClipText(cliptext):\n",
    "    \"\"\"\n",
    "    Purpose: Filtering the elements to UTF-16. Tkinter cannot represent items outside this range, and causes an error.\n",
    "    Input: <String>. A word that can contain UTF-32 characters.\n",
    "    Output: <String>. Word without the characters out of representable range in Tkinter.\n",
    "    \"\"\"\n",
    "    #Removing all characters > 65535 (that's the range for tcl)\n",
    "    cliptext = \"\".join([c for c in cliptext if ord(c) <= 65535])\n",
    "    return cliptext\n",
    "\n",
    "def final_clean(text_list):\n",
    "    \"\"\"\n",
    "    Purpose: Do a final sweep over the tokens in search for elements that could have passed the filters.\n",
    "    Input: <List>. List of preprocessed tokens from the texts.\n",
    "    Output: <List>. List of cleaned preprocessed tokens from the texts.\n",
    "    \"\"\"\n",
    "    clean_text = []\n",
    "    final_text = []\n",
    "    for word in text_list:\n",
    "        processed = False\n",
    "        if '/' not in word:\n",
    "            for punctmark in punctuation_marks_extended:\n",
    "                if word.startswith(punctmark):\n",
    "                    word = word.replace(punctmark,'')\n",
    "                if word.endswith(punctmark):\n",
    "                    word = word.replace(punctmark,'')\n",
    "                subwords = word.split(punctmark)\n",
    "                if(len(subwords) > 1):\n",
    "                    processed = True\n",
    "                    for subword in subwords:\n",
    "                        if (len(subword)> 1): clean_text.append(subword)\n",
    "            if(processed == False):\n",
    "                if(len(word) >= 2): clean_text.append(word) #clean remaining single letters and white-spaces\n",
    "    for word in clean_text:\n",
    "        clean_word = cleanClipText(word)\n",
    "        final_text.append(clean_word)\n",
    "    return final_text\n",
    "\n",
    "def preprocess_text(article):\n",
    "    \"\"\"\n",
    "    Purpose: Main function for preprocessing a text.\n",
    "    Input: <String>. Raw plain text coming from a source. In this case, HTML source code.\n",
    "    Output: <List>. List of clean representable tokens that convey meaning from the raw text.\n",
    "    \"\"\"\n",
    "    \n",
    "    content = html.unescape(article) #clean unwanted html hexadecimal entities\n",
    "    content = extract_markups(content) #\n",
    "    content = convert_numbers_to_specialkey(content)\n",
    "    content = word_tokenize(content.lower()) #tokenize words\n",
    "    content = extract_punctuation(content) #remove punctuation marks\n",
    "    content = final_clean(content)\n",
    "    content_clean = extract_stopwords(content) #remove stopwords (english)\n",
    "    return content_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for loading information into Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_articlesNeo4j():\n",
    "    \"\"\"\n",
    "    Purpose: This will load a list of articles in JSON format into Neo4j from the import folder.\n",
    "    Input: JSON file coming from the Feedly.com API.\n",
    "    Output: The articles will be imported in the active database in Neo4j as nodes.\n",
    "    Note: The JSON file must follow the same structure that Feedly.com provides.\n",
    "    \"\"\"\n",
    "    \n",
    "    queryLoadFeedlyArticles = \"\"\"\n",
    "    CALL apoc.load.json('file:///all_complete_articles.json') YIELD value\n",
    "    UNWIND value.items AS item\n",
    "    MERGE (a:Article:_AI {id:item.id})\n",
    "    SET a.created = item.crawled,\n",
    "        a.image = item.visual.url,\n",
    "        a.title = trim(item.title),\n",
    "        a.author = trim(item.author),\n",
    "        a.content = coalesce(item.content.content,item.fullContent),    \n",
    "        a.summary = item.summary.content,\n",
    "        a.url = [],\n",
    "        a.url = a.url + coalesce(item.canonicalUrl,[]),\n",
    "        a.highlightedText = []\n",
    "    FOREACH (annotation IN item.annotations |\n",
    "        SET a.highlightedText = a.highlightedText + annotation.highlight.text\n",
    "    )\n",
    "    FOREACH (alt IN item.alternate |\n",
    "        SET a.url = a.url + alt.href\n",
    "    )\n",
    "\n",
    "    FOREACH (tag IN item.tags |\n",
    "        FOREACH(ignoreMe IN CASE WHEN left(tag.label,3) = \"FA.\" THEN [1] ELSE [] END |\n",
    "            MERGE (lfa:LtsFocusArea:_AI {name:trim(substring(tag.label,3))})\n",
    "            MERGE (a)-[r:RELATES_TO]->(lfa)\n",
    "        )\n",
    "    )\n",
    "    FOREACH (tag IN item.tags |\n",
    "        FOREACH(ignoreMe IN CASE WHEN left(tag.label,3) = \"HS.\" THEN [1] ELSE [] END |\n",
    "            MERGE (hs:HorizonScanningArea:_AI {name:trim(substring(tag.label,3))})\n",
    "            MERGE (a)-[r:IS_AN_INSTANCE_OF]->(hs)\n",
    "        )\n",
    "    )\n",
    "    FOREACH (tag IN item.tags |\n",
    "        FOREACH(ignoreMe IN CASE WHEN left(tag.label,3) = \"MT.\" THEN [1] ELSE [] END |\n",
    "            MERGE (mt:Megatrend:_AI {name:trim(substring(tag.label,3))})\n",
    "            MERGE (a)-[r:RELATES_TO]->(mt)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    WITH count(*) AS ignored\n",
    "\n",
    "    MATCH (a:Article:_AI)\n",
    "    WHERE a.highlightedText = []\n",
    "    SET a.highlightedText = NULL\n",
    "\n",
    "    WITH count(*) AS ignored\n",
    "\n",
    "    MATCH (a:Article:_AI)\n",
    "    WHERE a.url = []\n",
    "    SET a.url = NULL\n",
    "    \"\"\"\n",
    "    graph.run(queryLoadFeedlyArticles)\n",
    "\n",
    "def preprocess_articlesNeo4j():\n",
    "    \"\"\"\n",
    "    Purpose: Clean the database from bad examples. Relocate the content in articles where the content is in summary, etc.\n",
    "    Input: None.\n",
    "    Output: The database is updated.\n",
    "    Note: Some properties may be modified, and some articles deleted (empty articles, mostly).\n",
    "    \"\"\"\n",
    "    \n",
    "    graph.run(\"MATCH (m:Article:_AI) WHERE NOT EXISTS(m.summary) AND NOT EXISTS(m.content) DETACH DELETE m\")\n",
    "    graph.run(\"MATCH (m:Article:_AI) WHERE length(m.summary)>length(m.content) SET m.content = m.summary\")\n",
    "    graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.summary) AND NOT EXISTS(m.content) SET m.content = m.summary\")\n",
    "\n",
    "def process_documentsNeo4j():\n",
    "    \"\"\"\n",
    "    Purpose: Preprocess the content of the articles existing in the database.\n",
    "    Input: None.\n",
    "    Output: The database is updated. The processed text will be stored in the properties: 'preprocessed' and 'preprocessed_stemmed' of the nodes.\n",
    "    \"\"\"\n",
    "    preprocess_query = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.content) AND NOT EXISTS(m.preprocessed) RETURN m.content AS content, id(m) AS node_id\")\n",
    "    for item in preprocess_query:\n",
    "        processed_doc = preprocess_text(item['content'])\n",
    "        query = \"MATCH (m:Article:_AI) WHERE id(m) = $node_id SET m.preprocessed = $preproc\"\n",
    "        parameters = {'node_id': item['node_id'], 'preproc': processed_doc}\n",
    "        graph.run(query, parameters=parameters)\n",
    "        \n",
    "    preprocessstem_query = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) AND NOT EXISTS(m.preprocessed_stemmed) RETURN m.preprocessed AS preproc, id(m) AS node_id\")\n",
    "    for art in preprocessstem_query:\n",
    "        processed_stem_doc = stem_words(art['preproc'])\n",
    "        query = \"MATCH (m:Article:_AI) WHERE id(m) = $node_id SET m.preprocessed_stemmed = $preproc_stem\"\n",
    "        parameters = {'node_id': art['node_id'], 'preproc_stem': processed_stem_doc}\n",
    "        graph.run(query, parameters=parameters)\n",
    "    \n",
    "    #Clean the graph from empty articles\n",
    "    graph.run(\"MATCH (n:Article:_AI) WHERE EXISTS(n.content) AND n.preprocessed=[] DETACH DELETE n\")\n",
    "    graph.run(\"MATCH (n:Article:_AI) WHERE EXISTS(n.preprocessed) AND n.preprocessed_stemmed=[] DETACH DELETE n\")\n",
    "\n",
    "def clean_empty_processed_docs():\n",
    "    \"\"\"\n",
    "    Purpose: Clean the database from bad examples. In this case, empty content articles.\n",
    "    Input: None.\n",
    "    Output: The database in Neo4j is updated.\n",
    "    \"\"\"\n",
    "    graph.run(\"MATCH (n:Article:_AI) WHERE NOT EXISTS(n.content) DETACH DELETE n\")\n",
    "\n",
    "def evaluate_keywordsNeo4j():\n",
    "    \"\"\"\n",
    "    Purpose: Evaluate the key words defining the articles in the database.\n",
    "    Input: None.\n",
    "    Output: The database is updated in Neo4j. The results are stored in the node property: 'keywords'.\n",
    "    Note: This uses the TF-IDF model. Make sure it is up and running.\n",
    "    \"\"\"\n",
    "    \n",
    "    preprocess_query = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) AND NOT EXISTS(m.keywords) RETURN m.preprocessed_stemmed AS preproc_stem, id(m) AS node_id\")\n",
    "    for item in preprocess_query:\n",
    "        ptext_bow = corpus_memory_friendly.dictionary.doc2bow(item['preproc_stem'])\n",
    "        ptext_tfidf = tfidf[ptext_bow] #this is in local memory currently\n",
    "        #tfidf_doc = [item for item in ptext_tfidf]\n",
    "        keyw = sorted(ptext_tfidf, key=lambda item: -item[1])\n",
    "        idwords = []\n",
    "        idwordsview = []\n",
    "        for word in keyw:\n",
    "            (idw,tf) = word\n",
    "            if(tf >= 0.099):\n",
    "                idwords.append(idw)\n",
    "                if(tf >= 0.125):\n",
    "                    idwordsview.append(idw)\n",
    "        if not idwords:\n",
    "            (idw,tf) = keyw[0] #at least put one keyword\n",
    "            idwords.append(idw)\n",
    "            idwordsview.append(idw)\n",
    "\n",
    "        keywords = [corpus_memory_friendly.dictionary[idword] for idword in idwords]\n",
    "        keywords_viewer = [corpus_memory_friendly.dictionary[idwordv] for idwordv in idwordsview]\n",
    "        query = \"MATCH (m:Article:_AI) WHERE id(m) = $node_id SET m.keywords = $keywords, m.keywords_viewer = $keywords_view\"\n",
    "        parameters = {'node_id': item['node_id'], 'keywords': keywords, 'keywords_view': keywords_viewer}\n",
    "        graph.run(query, parameters=parameters)\n",
    "    \n",
    "def evaluate_keywordsNeo4jNST(): #NST stands for --> Non-STemmed\n",
    "    \"\"\"\n",
    "    Purpose: Evaluate the key words (Non-stemmed) defining the articles in the database.\n",
    "    Input: None.\n",
    "    Output: The database is updated in Neo4j. The results are stored in the node property: 'keywords_nst'.\n",
    "    Note: This uses the NST TF-IDF model. Make sure it is up and running.\n",
    "    \"\"\"\n",
    "    preprocess_query = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) AND NOT EXISTS(m.keywords_nst) RETURN m.preprocessed AS preproc, id(m) AS node_id\")\n",
    "    for item in preprocess_query:\n",
    "        ptext_bow = corpus_memory_friendly_NST.dictionary.doc2bow(item['preproc'])\n",
    "        ptext_tfidf = tfidf_nst[ptext_bow] #this is in local memory currently\n",
    "        keyw = sorted(ptext_tfidf, key=lambda item: -item[1])\n",
    "        idwords = []\n",
    "        idwordsview = []\n",
    "        for word in keyw:\n",
    "            (idw,tf) = word\n",
    "            if(tf >= 0.099):\n",
    "                idwords.append(idw)\n",
    "                if(tf >= 0.125):\n",
    "                    idwordsview.append(idw)\n",
    "        if not idwords:\n",
    "            (idw,tf) = keyw[0] #at least put one keyword\n",
    "            idwords.append(idw)\n",
    "            idwordsview.append(idw)\n",
    "\n",
    "        keywords = [corpus_memory_friendly_NST.dictionary[idword] for idword in idwords]\n",
    "        keywords_viewer = [corpus_memory_friendly_NST.dictionary[idwordv] for idwordv in idwordsview]\n",
    "        query = \"MATCH (m:Article:_AI) WHERE id(m) = $node_id SET m.keywords_nst = $keywords, m.keywords_viewer_nst = $keywords_view\"\n",
    "        parameters = {'node_id': item['node_id'], 'keywords': keywords, 'keywords_view': keywords_viewer}\n",
    "        graph.run(query, parameters=parameters)\n",
    "        \n",
    "def create_LiteId_documents():\n",
    "    \"\"\"\n",
    "    Purpose: Create a lite version of id's, only for the articles and that is sequential.\n",
    "    Input: None.\n",
    "    Output: The database in Neo4j is updated. The results are stored in the node property: 'liteId'.\n",
    "    Note: This serves to identify articles when using the models. It is very important that are sequential,\n",
    "          and that those id's coincide with the rows and columns of the similarity matrix for each article.\n",
    "          For more information, visit the gensim.similarities.docsim documentation:\n",
    "          https://radimrehurek.com/gensim/similarities/docsim.html at July 1st, 2019.\n",
    "    \"\"\"\n",
    "    \n",
    "    queryliteId = \"\"\"\n",
    "    MATCH (n:Article:_AI)\n",
    "    WITH range(coalesce(max(n.liteId)+1,0),count(n)-1,1) AS enum\n",
    "\n",
    "    MATCH (n:Article:_AI)\n",
    "    WHERE NOT EXISTS(n.liteId)\n",
    "    WITH enum, range(0,count(n)-1,1) AS index, collect(id(n)) AS id\n",
    "    UNWIND index AS indexes\n",
    "    WITH id[indexes] AS IDs, enum[indexes] AS ENUMs\n",
    "\n",
    "    MATCH (n:Article:_AI)\n",
    "    WHERE id(n)=IDs\n",
    "    SET n.liteId=ENUMs\n",
    "    \"\"\"\n",
    "    graph.run(queryliteId)\n",
    "    \n",
    "def check_documents():\n",
    "    \"\"\"\n",
    "    Purpose: Check the coherence of the Lite ID's.\n",
    "    Input: None.\n",
    "    Output:\n",
    "    Note: If the LiteID's are incoherent, it will raise a warning and the application will not let you continue.\n",
    "          This is for protecting the well-functioning. An incoherence here will provide bad recommendations, or even runtime errors, in some cases.\n",
    "          That is a very important link (as explained when creating the LiteID's).\n",
    "          Note that this is simple right now. It only checks that the amount of nodes with LiteID are the equal to the number of Article nodes\n",
    "          and that the largest LiteID in the database is the same as it should be for the amount of Article nodes in the database.\n",
    "          It does not check if the LiteID's are completely sequential or if there are duplicates. So in some missused cases, the coherence check\n",
    "          may come through, when the LiteID's are not sequential. This may cause Errors. Please, check this oftenly when modifying this property or creating a new graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    check = False\n",
    "    num_docs_liteid = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.liteId) RETURN count(m) AS count_liteid\").data()\n",
    "    last_num_liteid = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.liteId) RETURN m.liteId AS lite_id ORDER BY m.liteId DESC LIMIT 1\").data()\n",
    "    num_docs_total = graph.run(\"MATCH (m:Article:_AI) RETURN count(m) AS count_total\").data()\n",
    "    if(num_docs_liteid[0]['count_liteid'] == num_docs_total[0]['count_total']) and ((last_num_liteid[0]['lite_id']+1) == num_docs_total[0]['count_total']): check = True\n",
    "    return check\n",
    "\n",
    "def process_wordembeddingsNeo4j():\n",
    "    \"\"\"\n",
    "    Purpose: Store the pre-trained word embeddings from GloVe in the graph.\n",
    "    Input: CSV file downloaded from GloVe (https://nlp.stanford.edu/projects/glove/, at July 1st, 2019).\n",
    "    Output: The graph database is updated with a new type of node :Word. The word vectors are stored in the node property 'embedding'.\n",
    "    Note: The CSV file must contain a header called 'header'. This is for the GloVe vectors with 300 dimensions.\n",
    "          Please, modify the code if you are going to use a different pre-trained model.\n",
    "    \"\"\"\n",
    "    \n",
    "    query = \"\"\"\n",
    "    USING PERIODIC COMMIT 20000\n",
    "    LOAD CSV WITH HEADERS FROM 'file:///glove.42B.300d.csv' AS csvLine FIELDTERMINATOR \"↨\"\n",
    "    MERGE (w:Word:_AI {name:split(csvLine.header,\" \")[0]})\n",
    "    ON CREATE SET w.embedding = [x IN split(csvLine.header,' ')[1..301] | toFloat(x)]\n",
    "    \"\"\"\n",
    "    graph.run(\"CREATE CONSTRAINT ON (word:Word) ASSERT word.name IS UNIQUE\")\n",
    "    # Take into account that the character \" starting at the beginning of a line breaks the query\n",
    "    # replace every double quot at the beginning of a line for something else, like a single quot\n",
    "    graph.run(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Data and create a Dictionary of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpusDashNeo(object):\n",
    "    \"\"\"\n",
    "    Purpose: This is a class that represents both the pre-processed corpus (articles) and the dictionary of words.\n",
    "             Loads those from disk, if they exist. If not, the class creates and saves them in the computer.\n",
    "    Input:   None.\n",
    "    Output:  Object which is iterable and yields the pre-processed corpus. It can also be done tu use dictionary functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        if (os.path.isfile(dict_name) and not TRAINING):\n",
    "            self.process = False  \n",
    "        else:\n",
    "            self.process = True\n",
    "        \n",
    "        if self.process:\n",
    "            try:\n",
    "                print(\"Creating dictionary...\")\n",
    "                query_corpus = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) RETURN m.preprocessed_stemmed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "                self.dictionary = corpora.Dictionary(article['preprocessed'] for article in query_corpus)\n",
    "                once_ids = (tokenid for tokenid, docfreq in iteritems(self.dictionary.dfs) if docfreq == 1)\n",
    "                self.dictionary.filter_tokens(once_ids)\n",
    "                self.dictionary.compactify()\n",
    "                self.dictionary.save(dict_name)\n",
    "                print(\"Dictionary created.\")\n",
    "                print(self.dictionary)\n",
    "            except Exception as e:\n",
    "                print(\"Failed at creating the dictionary. Please check the dictionary generator.\")\n",
    "                print(\"Type of error: \" + str(e))\n",
    "                print(traceback.format_exc())\n",
    "            else:\n",
    "                try:\n",
    "                    corpora.MmCorpus.serialize(corpus_name, self)\n",
    "                except Exception as e:\n",
    "                    print(\"Error at serializing the corpus in memory. Please check the code snippet at the corpus serializer.\")\n",
    "                \n",
    "                try:\n",
    "                    self.__load_corpus()\n",
    "                except Exception as e:\n",
    "                    print(\"There was an error loading the corpus. Please, check the code.\")\n",
    "                else:\n",
    "                    self.process = False\n",
    "        \n",
    "        else:\n",
    "            try:\n",
    "                print(\"Loading dictionary...\")\n",
    "                self.dictionary = corpora.Dictionary.load(dict_name)\n",
    "                print(\"Dictionary loaded.\")\n",
    "                print(self.dictionary)\n",
    "            except Exception as e:\n",
    "                print(\"Failed at loading the dictionary. Please check the dictionary file.\")\n",
    "                print(\"Type of error: \" + str(e))\n",
    "                print(traceback.format_exc())\n",
    "            else:\n",
    "                self.__load_corpus()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.process:\n",
    "            try:\n",
    "                query_corpus = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) RETURN m.preprocessed_stemmed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "                for idc,art in enumerate(query_corpus):\n",
    "                    print(\"Building model: \" + str(idc+1) + \"/\" + str(self.dictionary.num_docs), end='\\r') # + \"\\r\"\n",
    "                    yield self.dictionary.doc2bow(art['preprocessed'])\n",
    "                print('\\n')\n",
    "            except Exception as e:\n",
    "                print(\"Failed at processing the corpus. Please check the transformation dict-->corpus and/or the Neo4j query.\")\n",
    "                print(\"Check also that Neo4j is open and running.\")\n",
    "                print(\"Type of error: \" + str(e))\n",
    "                print(traceback.format_exc())\n",
    "                print(\"Should I run an old file?\")\n",
    "                # Run an old file if it exists and fails?\n",
    "        else:\n",
    "            try:\n",
    "                for artitem in self.corpus:\n",
    "                    yield artitem\n",
    "            except Exception as e:\n",
    "                print(\"The generator has failed at yielding the corpus documents. Check the iterator of the corpus.\")\n",
    "                print(\"Type of error: \" + str(e))\n",
    "                print(traceback.format_exc())\n",
    "                \n",
    "    def __load_corpus(self):\n",
    "        try:\n",
    "            print(\"Loading corpus...\")\n",
    "            self.corpus = corpora.MmCorpus(corpus_name)\n",
    "            print(\"Corpus loaded.\")\n",
    "            print(self.corpus)\n",
    "        except Exception as e:\n",
    "            print(\"Failed at loading the corpus. Please check that the corpus file is correct.\")\n",
    "            print(\"Type of error: \" + str(e))\n",
    "            print(traceback.format_exc())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpusNeoNST(object):\n",
    "    \"\"\"\n",
    "    Purpose: (Non-Stemmed Version) This is a class that represents both the pre-processed corpus (articles) and the dictionary of words.\n",
    "             Loads those from disk, if they exist. If not, the class creates and saves them in the computer.\n",
    "    Input:   None.\n",
    "    Output:  Object which is iterable and yields the pre-processed corpus. It can also be done tu use dictionary functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        if (os.path.isfile(dict_name_nst) and not TRAINING):\n",
    "            self.process = False  \n",
    "        else:\n",
    "            self.process = True\n",
    "        \n",
    "        if self.process:\n",
    "            try:\n",
    "                print(\"Creating dictionary not stemmed...\")\n",
    "                query_corpus = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.preprocessed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "                self.dictionary = corpora.Dictionary(article['preprocessed'] for article in query_corpus)\n",
    "                once_ids = (tokenid for tokenid, docfreq in iteritems(self.dictionary.dfs) if docfreq == 1)\n",
    "                self.dictionary.filter_tokens(once_ids)\n",
    "                self.dictionary.compactify()\n",
    "                self.dictionary.save(dict_name_nst)\n",
    "                print(\"Dictionary created.\")\n",
    "                print(self.dictionary)\n",
    "            except Exception as e:\n",
    "                print(\"Failed at creating the dictionary. Please check the dictionary generator.\")\n",
    "                print(\"Type of error: \" + str(e))\n",
    "                print(traceback.format_exc())\n",
    "            else:\n",
    "                try:\n",
    "                    corpora.MmCorpus.serialize(corpus_name_nst, self)\n",
    "                except Exception as e:\n",
    "                    print(\"Error at serializing the corpus in memory. Please check the code snippet at the corpus serializer.\")\n",
    "                \n",
    "                try:\n",
    "                    self.__load_corpus()\n",
    "                except Exception as e:\n",
    "                    print(\"There was an error loading the corpus. Please, check the code.\")\n",
    "                else:\n",
    "                    self.process = False\n",
    "        \n",
    "        else:\n",
    "            try:\n",
    "                print(\"Loading dictionary...\")\n",
    "                self.dictionary = corpora.Dictionary.load(dict_name_nst)\n",
    "                print(\"Dictionary loaded.\")\n",
    "                print(self.dictionary)\n",
    "            except Exception as e:\n",
    "                print(\"Failed at loading the dictionary. Please check the dictionary file.\")\n",
    "                print(\"Type of error: \" + str(e))\n",
    "                print(traceback.format_exc())\n",
    "            else:\n",
    "                self.__load_corpus()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.process:\n",
    "            try:\n",
    "                query_corpus = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.preprocessed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "                for idc,art in enumerate(query_corpus):\n",
    "                    print(\"Building model: \" + str(idc+1) + \"/\" + str(self.dictionary.num_docs), end='\\r') # + \"\\r\"\n",
    "                    yield self.dictionary.doc2bow(art['preprocessed'])\n",
    "                print('\\n')\n",
    "            except Exception as e:\n",
    "                print(\"Failed at processing the corpus. Please check the transformation dict-->corpus and/or the Neo4j query.\")\n",
    "                print(\"Check also that Neo4j is open and running.\")\n",
    "                print(\"Type of error: \" + str(e))\n",
    "                print(traceback.format_exc())\n",
    "                print(\"Should I run an old file?\")\n",
    "                # Run an old file if it exists and fails?\n",
    "        else:\n",
    "            try:\n",
    "                for artitem in self.corpus:\n",
    "                    yield artitem\n",
    "            except Exception as e:\n",
    "                print(\"The generator has failed at yielding the corpus documents. Check the iterator of the corpus.\")\n",
    "                print(\"Type of error: \" + str(e))\n",
    "                print(traceback.format_exc())\n",
    "                \n",
    "    def __load_corpus(self):\n",
    "        try:\n",
    "            print(\"Loading corpus...\")\n",
    "            self.corpus = corpora.MmCorpus(corpus_name_nst)\n",
    "            print(\"Corpus loaded.\")\n",
    "            print(self.corpus)\n",
    "        except Exception as e:\n",
    "            print(\"Failed at loading the corpus. Please check that the corpus file is correct.\")\n",
    "            print(\"Type of error: \" + str(e))\n",
    "            print(traceback.format_exc())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "class WMD(object):\n",
    "    \"\"\"\n",
    "    Purpose: The class computes an optimization process called Word Moving Distance (WMD)\n",
    "    Input:   None.\n",
    "    Output:  It computes the minimum euclidean distance between two articles using the WMD method.\n",
    "    Note:    For more information, read the paper: \"From Word Embeddings To Document Distances\", by Matt J. Kusner et al. (2015)\n",
    "             The words should not be stemmed for this method, as GloVe do not contain embeddings for stemmed words.\n",
    "             Make sure that you are using the NST-version of the models to compute this one.\n",
    "             \n",
    "    Note2:   This is currently computing the Relaxed Word Moving Distance (RWMD) version, for a reduced computing time.\n",
    "             If you would like to use the standard version of the WMD, use the commented second constraint in the code.\n",
    "             Beware that the computation time will increase and the convergence might not happen during optimization.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaded = False\n",
    "    def load_docs(self, text1, text2):\n",
    "        #Parameters\n",
    "        MAX_WORDS = 20 # tunable: could be 20, 30, 50, 70... also using TF-IDF or not\n",
    "        USING_TFIDF = True\n",
    "        \n",
    "        if(USING_TFIDF):\n",
    "            bow_1 = corpus_memory_friendly_NST.dictionary.doc2bow(text1)\n",
    "            bow_1 = tfidf_nst[bow_1] # OBS! careful you have to make sure this is the non-stemmed one all the time!\n",
    "            bow_2 = corpus_memory_friendly_NST.dictionary.doc2bow(text2)\n",
    "            bow_2 = tfidf_nst[bow_2] # OBS! careful you have to make sure this is the non-stemmed one all the time!\n",
    "        else:\n",
    "            # dict_1 --> nbow_1 --> self.d_1\n",
    "            dict_1 = corpora.Dictionary([text1])\n",
    "            bow_1 = dict_1.doc2bow(text1)\n",
    "            # dict_2 --> nbow_2 --> self.d_2\n",
    "            dict_2 = corpora.Dictionary([text2])\n",
    "            bow_2 = dict_2.doc2bow(text2)\n",
    "            \n",
    "        bow_1 = sorted(bow_1, key=lambda x: -x[1])[:MAX_WORDS]\n",
    "        bow_2 = sorted(bow_2, key=lambda x: -x[1])[:MAX_WORDS]\n",
    "        \n",
    "        idw_1 = np.array([it for it,val in bow_1])\n",
    "        nbow_1 = np.array([val for it,val in bow_1])\n",
    "        self.d_1 = nbow_1/np.sum(nbow_1)\n",
    "        idw_2 = np.array([it for it,val in bow_2])\n",
    "        nbow_2 = np.array([val for it,val in bow_2])\n",
    "        self.d_2 = nbow_2/np.sum(nbow_2)\n",
    "        n = len(self.d_1) #length of Text 1\n",
    "        m = len(self.d_2) #length of Text 2\n",
    "        keep_list_1 = list(range(n))\n",
    "        keep_list_2 = list(range(m))\n",
    "        \n",
    "        c = np.random.rand(n,m)*10000\n",
    "        for pos1, idword1 in enumerate(idw_1):\n",
    "            if(USING_TFIDF): word1 = corpus_memory_friendly_NST.dictionary[idword1]\n",
    "            else: word1 = dict_1[idword1]\n",
    "            emb1 = graph.run(\"MATCH (m:Word:_AI {name: $word_1}) RETURN m.embedding AS embedding LIMIT 1\", parameters={'word_1': word1}).data()\n",
    "            if(emb1):\n",
    "                emb1 = emb1[0]['embedding']\n",
    "                for pos2, idword2 in enumerate(idw_2):\n",
    "                    if(USING_TFIDF): word2 = corpus_memory_friendly_NST.dictionary[idword2]\n",
    "                    else: word2 = dict_2[idword2]\n",
    "                    emb2 = graph.run(\"MATCH (m:Word:_AI {name: $word_2}) RETURN m.embedding AS embedding LIMIT 1\", parameters={'word_2': word2}).data()\n",
    "                    if(emb2):\n",
    "                        emb2 = emb2[0]['embedding']\n",
    "                        dist = euclidean_distances([emb1,emb2])[0,1]\n",
    "                        c[pos1,pos2] = dist\n",
    "                    else:\n",
    "                        if(pos2 in keep_list_2): keep_list_2.remove(pos2)\n",
    "            else: \n",
    "                if(pos1 in keep_list_1): keep_list_1.remove(pos1)\n",
    "        \n",
    "        c = c[keep_list_1,:]\n",
    "        c = c[:,keep_list_2]\n",
    "        \n",
    "        # custom dictionary for the two documents?\n",
    "        self.n,self.m = c.shape #length of Text1, Text2\n",
    "        \n",
    "        self.d_1 = np.ones(self.n) #this is only for single word texts: this needs to be bow\n",
    "        self.d_2 = np.ones(self.n) #this is only for single word texts: this needs to be bow\n",
    "\n",
    "        self.c = np.transpose(c.flatten())\n",
    "        self.loaded = True\n",
    "        \n",
    "    #####################\n",
    "    ## Paper Doc Dist. ##\n",
    "    #####################\n",
    "    #Objective\n",
    "    def __objective(self,T):\n",
    "        cost_function = np.dot(T,self.c)\n",
    "        return cost_function\n",
    "\n",
    "    #Constraints\n",
    "    def __constr1(self,x):\n",
    "        jc = x.reshape(self.n,self.m)\n",
    "        jc = -np.sum(jc, axis=1)\n",
    "        l = np.add(jc,self.d_1)\n",
    "        return l\n",
    "\n",
    "    def __constr2(self,x):\n",
    "        jc2 = x.reshape(self.n,self.m)\n",
    "        jc2 = -np.sum(jc2, axis=0)\n",
    "        l2 = np.add(jc2,self.d_2)\n",
    "        return l2\n",
    "    \n",
    "    #Calculate the document distances\n",
    "    def calculate_distance(self):\n",
    "        if(self.loaded):\n",
    "            cons = [{'type': 'eq', 'fun': self.__constr1}] # , {'type': 'eq', 'fun': constr2}\n",
    "            T0 = np.random.rand(self.n,self.m)\n",
    "            T0 = T0.flatten()\n",
    "            b = (0.0,None) # Bounds for the transformations\n",
    "            bnds = (b,)*self.n*self.m\n",
    "            sol = minimize(self.__objective, T0, method='SLSQP', bounds=bnds, constraints=cons, tol=1e-4)\n",
    "            return sol\n",
    "        else:\n",
    "            print(\"You need to load the documents you want to calculate the distance of first.\\nUse the function 'load_docs(doc1,doc2)' for that.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npapertest = WMD()\\ndoc_1 = graph.run(\"MATCH (m:Article:_AI)-[]-(n:LtsFocusArea:_AI {name:\\'Autonomous Drive\\'}) RETURN m.title AS title, m.preprocessed AS prep LIMIT 1\").data()[0]\\ndoc_2 = graph.run(\"MATCH (m:Article:_AI)-[]-(n:Megatrend:_AI {name:\\'Technology development\\'}) RETURN m.title AS title, m.preprocessed AS prep LIMIT 1\").data()[0]\\nprint(doc_1[\\'title\\'], doc_2[\\'title\\'])\\npapertest.load_docs(doc_2[\\'prep\\'], doc_1[\\'prep\\'])\\ndistance = papertest.calculate_distance()\\nprint(\"Distance: \", distance.fun)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "papertest = WMD()\n",
    "doc_1 = graph.run(\"MATCH (m:Article:_AI)-[]-(n:LtsFocusArea:_AI {name:'Autonomous Drive'}) RETURN m.title AS title, m.preprocessed AS prep LIMIT 1\").data()[0]\n",
    "doc_2 = graph.run(\"MATCH (m:Article:_AI)-[]-(n:Megatrend:_AI {name:'Technology development'}) RETURN m.title AS title, m.preprocessed AS prep LIMIT 1\").data()[0]\n",
    "print(doc_1['title'], doc_2['title'])\n",
    "papertest.load_docs(doc_2['prep'], doc_1['prep'])\n",
    "distance = papertest.calculate_distance()\n",
    "print(\"Distance: \", distance.fun)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_recommendations_single_article_Neo4j(art, recom, algorithm):\n",
    "    \"\"\"\n",
    "    Purpose: The function stores the recommendations based on an algorithm for one certain article defined. \n",
    "    Input:   art        - Refers to the current document liteID to store the recommendations for.\n",
    "             recom      - Refers to the recommendations performed by the algorithm for the 'art' document.\n",
    "             algorithm  - Refers to the algorithm used to create the recommendation.\n",
    "    Output:  None. The recommendations will be stored in the graph database.\n",
    "    \"\"\"\n",
    "    recommendations = []\n",
    "    for rank,recomid in enumerate(recom):\n",
    "        recommendations.append({'recom': recomid, 'rank': rank+1})\n",
    "        \n",
    "    query=\"\"\"\n",
    "    WITH $list_recommendations AS list\n",
    "    UNWIND list AS recommendations\n",
    "    MATCH (sou:Article:_AI {liteId: $query_liteid})\n",
    "    MATCH (rec:Article:_AI {liteId: recommendations.recom})\n",
    "    MERGE (sou)-[REL:%s]->(rec)\n",
    "    SET REL.rank = recommendations.rank\n",
    "    \"\"\" % (algorithm)\n",
    "    \n",
    "    graph.run(query, parameters={'query_liteid': art, 'list_recommendations': recommendations})\n",
    "    \n",
    "\n",
    "def store_recommendations_Neo4j():\n",
    "    \"\"\"\n",
    "    Purpose: The function will estimate the recommendations for each one of the articles in the database using the models.\n",
    "    Input:   None. Uses the database.\n",
    "    Output:  None. The recommendations will be stored in the graph database.\n",
    "    \"\"\"\n",
    "    all_artic = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.liteId AS lite_id ORDER BY m.liteId ASC\").data()\n",
    "    for article in all_artic:\n",
    "        #TF-IDF\n",
    "        rec = find_similardocs_tfidf(doc=article['lite_id'], return_results=True)\n",
    "        store_recommendations_single_article_Neo4j(article['lite_id'],rec,algorithm='TF_IDF')\n",
    "        \n",
    "        #LSA\n",
    "        rec = find_similardocs_lsi(doc=article['lite_id'], return_results=True)\n",
    "        store_recommendations_single_article_Neo4j(article['lite_id'],rec,algorithm='LSA')\n",
    "        \n",
    "        #LDA\n",
    "        #rec = find_similardocs_lda(doc=article['lite_id'], return_results=True)\n",
    "        #store_recommendations_single_article_Neo4j(article['lite_id'],rec,algorithm='LDA')\n",
    "        \n",
    "        #Word Embeddings\n",
    "        rec = find_similardocs_WE(doc=article['lite_id'], return_results=True)\n",
    "        store_recommendations_single_article_Neo4j(article['lite_id'],rec,algorithm='WORD_EMBEDDINGS')\n",
    "        \n",
    "def merge_recommendations_Neo4j():\n",
    "    \"\"\"\n",
    "    Purpose: The function will merge the recommendations among the articles in the database using the algorithms' recommendations.\n",
    "    Input:   None. Uses the database.\n",
    "    Output:  None. The recommendations will be stored in the graph database as a :RELATES_TO.\n",
    "    \"\"\"\n",
    "    query=\"\"\"\n",
    "    MATCH (a1:Article:_AI)-->(a2:Article_AI)\n",
    "    WHERE a1 <> a2\n",
    "    OPTIONAL MATCH (a1)-[r1:WORD_EMBEDDINGS]->(a2)\n",
    "    OPTIONAL MATCH (a1)-[r2:LSA]->(a2)\n",
    "    OPTIONAL MATCH (a1)-[r3:TF_IDF]->(a2)\n",
    "    MERGE (a1)-[com:RELATES_TO]->(a2)\n",
    "    SET com.weight = (toFloat((10-coalesce(r1.rank,10)) + (10-coalesce(r2.rank,10)) + (10-coalesce(r3.rank,10)))) / 27\n",
    "    \"\"\"\n",
    "    graph.run(query) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GUI Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import webbrowser\n",
    "from tkinter import font\n",
    "from PIL import Image, ImageTk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dictionary...\n",
      "Dictionary loaded.\n",
      "Dictionary(6878 unique tokens: ['2025', '32', '3d', 'abstract', 'accord']...)\n",
      "Loading corpus...\n",
      "Corpus loaded.\n",
      "MmCorpus(374 documents, 6878 features, 97868 non-zero entries)\n",
      "Loading dictionary...\n",
      "Dictionary loaded.\n",
      "Dictionary(10169 unique tokens: ['2025', '32', '3d', 'according', 'across']...)\n",
      "Loading corpus...\n",
      "Corpus loaded.\n",
      "MmCorpus(374 documents, 10169 features, 107162 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "root = tk.Tk()\n",
    "root.title(\"Horizon Scanning AI GUI\")\n",
    "root.resizable(False,False)\n",
    "HEIGHT = 922\n",
    "WIDTH = 786\n",
    "\n",
    "TEXT_FONT = \"Volvo Serif Pro\"\n",
    "FONT_ARTICLES = 'Volvo Novum'\n",
    "FONT_NOT_FOUND = \"Volvo Novum Medium\"\n",
    "no_article = -1\n",
    "no_article_sim = -1\n",
    "VIEW_DOCUMENT = False\n",
    "COMPUTE_COHERENCE = False\n",
    "COMPUTE_RECALL = True\n",
    "ocult_train = True\n",
    "algorithm_training = False\n",
    "\n",
    "#Messages through GUI List of typical functions\n",
    "def not_implemented_message():\n",
    "    output_screen['text'] = \"This method has not been implemented yet.\\nPlease, give time to the engineer responsible.\"\n",
    "\n",
    "def showing_recommendations(algorithm):\n",
    "    output_screen['text'] = \"Showing recommendations based on:\\n\" + str(algorithm)\n",
    "    \n",
    "def not_valid_message():\n",
    "    output_screen['text'] = \"The number entered is not valid.\\nPlease, enter a valid amount.\"\n",
    "    \n",
    "def algorithm_not_found_message():\n",
    "    output_screen['text'] = \"It seems the engineer responsible has a non-existing algorithm.\\nPlease, review this problem.\"\n",
    "    \n",
    "def article_not_selected_message():\n",
    "    output_screen['text'] = \"Please, select the article you want to find similarities to.\"\n",
    "    \n",
    "def cannot_be_implemented():\n",
    "    output_screen['text'] = \"This method cannot be implemented\\nfrom the backend currently.\\nPlease, use Neo4j for such analysis.\"\n",
    "    \n",
    "def clean_spacelines(text):\n",
    "    rgx_clsp = re.compile(r'(\\n)+')\n",
    "    tgx_cltb = re.compile(r'(\\t)+')\n",
    "    sgx_clsp = re.compile(r'(\\s)+')\n",
    "    cltext = re.sub(rgx_clsp,'\\n',text)\n",
    "    cltext = re.sub(tgx_cltb,'\\t',cltext)\n",
    "    cltext = re.sub(sgx_clsp,' ',cltext)\n",
    "    return cltext\n",
    "\n",
    "\n",
    "#Functions for calculating relations, distances and training models based on metrics\n",
    "def train_algorithm():\n",
    "    \"\"\"\n",
    "    Purpose: The main function to handle training of algorithms.\n",
    "    Input:   None.\n",
    "    Output:  None. The models will be saved in disk, if applicable.\n",
    "    Note:    Some algorithms are using pre-trained models. However this function will still calculate the distance and relations\n",
    "             existing among the articles (euclidean distance or cosine similarity).\n",
    "    \"\"\"\n",
    "    global algorithm_training, stemmed_tfidf\n",
    "    if(algorithmvariable.get()==\"Word Embeddings\"):\n",
    "        output_screen['text'] = \"Training for Word Embeddings...\\nPlease wait, it may take long.\"\n",
    "        train_wordembeddings()\n",
    "        output_screen['text'] = \"Training completed.\"\n",
    "    elif(algorithmvariable.get()==\"TF-IDF\"):\n",
    "        pass\n",
    "    elif(algorithmvariable.get()==\"Doc2vec\"):\n",
    "        not_implemented_message()\n",
    "    elif(algorithmvariable.get()==\"LSA\"):\n",
    "        algorithm_training = True\n",
    "        if(COMPUTE_RECALL):\n",
    "            stemmed_tfidf = True\n",
    "            recall_scores, RBP_scores, RBPacc_scores, start, step, stop = maximum_recall_score('lsa')\n",
    "            stemmed_tfidf = False\n",
    "            recall_scores_nst, RBP_scores_nst, RBPacc_scores_nst, start, step, stop = maximum_recall_score('lsa')\n",
    "            x = range(start, stop+1, step)\n",
    "            plt.plot(x, recall_scores, 'g', x, recall_scores_nst, 'y')\n",
    "            plt.xlabel(\"Number of Topics\")\n",
    "            plt.ylabel(\"Recall score\")\n",
    "            plt.legend((\"Stemmed-words\", \"NST-words\"), loc='best')\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(x, RBP_scores, 'b', x, RBP_scores_nst, 'c')\n",
    "            plt.xlabel(\"Number of Topics\")\n",
    "            plt.ylabel(\"Rank-biased precision (RBP)\")\n",
    "            plt.legend((\"Stemmed-words\", \"NST-words\"), loc='best')\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(x, RBPacc_scores, 'r', x, RBPacc_scores_nst, 'm')\n",
    "            plt.xlabel(\"Number of Topics\")\n",
    "            plt.ylabel(\"Rank-biased precision x recall (RBPacc)\")\n",
    "            plt.legend((\"Stemmed-words\", \"NST-words\"), loc='best')\n",
    "            plt.show()\n",
    "        if(COMPUTE_COHERENCE): coherence_model('lsa')\n",
    "        algorithm_training = False\n",
    "    elif(algorithmvariable.get()==\"LDA\"):\n",
    "        algorithm_training = True\n",
    "        if(COMPUTE_RECALL):\n",
    "            stemmed_tfidf = True\n",
    "            recall_scores, RBP_scores, RBPacc_scores, start, step, stop = maximum_recall_score('lda')\n",
    "            stemmed_tfidf = False\n",
    "            recall_scores_nst, RBP_scores_nst, RBPacc_scores_nst, start, step, stop = maximum_recall_score('lda')\n",
    "            x = range(start, stop+1, step)\n",
    "            plt.plot(x, recall_scores, 'g', x, recall_scores_nst, 'y')\n",
    "            plt.xlabel(\"Number of Topics\")\n",
    "            plt.ylabel(\"Recall score\")\n",
    "            plt.legend((\"Stemmed-words\", \"NST-words\"), loc='best')\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(x, RBP_scores, 'b', x, RBP_scores_nst, 'c')\n",
    "            plt.xlabel(\"Number of Topics\")\n",
    "            plt.ylabel(\"Rank-biased precision (RBP)\")\n",
    "            plt.legend((\"Stemmed-words\", \"NST-words\"), loc='best')\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(x, RBPacc_scores, 'r', x, RBPacc_scores_nst, 'm')\n",
    "            plt.xlabel(\"Number of Topics\")\n",
    "            plt.ylabel(\"Rank-biased precision x recall (RBPacc)\")\n",
    "            plt.legend((\"Stemmed-words\", \"NST-words\"), loc='best')\n",
    "            plt.show()\n",
    "        if(COMPUTE_COHERENCE): coherence_model('lda')\n",
    "        algorithm_training = False\n",
    "    elif(algorithmvariable.get()==\"Ensemble Method\"):\n",
    "        cannot_be_implemented()\n",
    "    elif(algorithmvariable.get()==\"Community Finding\"):\n",
    "        cannot_be_implemented()\n",
    "    else:\n",
    "        algorithm_not_found_message()\n",
    "\n",
    "    \n",
    "def train_wordembeddings():\n",
    "    \"\"\"\n",
    "    Purpose: Compute the similarity matrix to relate documents based on Word Embeddings\n",
    "    Input:   Word Embeddings, pre-processed documents\n",
    "    Output:  Serialized similarity matrix for documents\n",
    "    \n",
    "    Modes:   paper - Document distance by word embeddings using the Word Moving Distance method.\n",
    "                (Note!: This method takes a lot of time! Beware of this.\n",
    "                 Also, I am saving the results as we go in a txt file, so we can retrieve\n",
    "                 the computed results and not start from the beginning if it breaks).\n",
    "             title - Document distance and similarity of documents by word embeddings\n",
    "                     of words appearing in the Title.\n",
    "             glove - Document distance and similarity of documents by word embeddings using GloVe embeddings.\n",
    "             berglove - Document distance and similarity of documents by word embeddings using GloVe and BERT\n",
    "                        pre-trained embeddings.\n",
    "             \n",
    "    \"\"\"\n",
    "    mode = 'paper' #choose from\n",
    "    num_articles = graph.run(\"MATCH (n:Article:_AI) WHERE EXISTS(n.preprocessed) RETURN count(n) AS total\").data()[0]['total']\n",
    "    \n",
    "    def serialization(documents_embeddings, file_name_cosine_similarity, file_name_euclidean_distance):\n",
    "        ################################\n",
    "        ###      Serialization       ###\n",
    "        ################################\n",
    "        if(isinstance(documents_embeddings,list) and isinstance(file_name_cosine_similarity,str) and isinstance(file_name_euclidean_distance,str)):\n",
    "            try:\n",
    "                print(\"Initializing serialization...\")\n",
    "                #Cosine similarity:\n",
    "                cosim = cosine_similarity(X=documents_embeddings)\n",
    "                cosim.dump(file_name_cosine_similarity)\n",
    "\n",
    "                #Euclidean distance: to tune in many ways\n",
    "                euclidean = euclidean_distances(X=documents_embeddings)\n",
    "                euclidean.dump(file_name_euclidean_distance)\n",
    "                print(\"Finished serialization.\")\n",
    "            except:\n",
    "                print(\"Something went wrong during serialization.\")\n",
    "        else:\n",
    "            print(\"Bad format for the serialization. Please, check the data structure inputted for this.\")\n",
    "        \n",
    "        \n",
    "    if(mode=='glove'):\n",
    "        embed_dim = graph.run(\"MATCH (m:Word:_AI) RETURN size(m.embedding) AS size LIMIT 1\").data()[0]['size']\n",
    "        documents = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.liteId AS lite_id, m.preprocessed AS preprocessed ORDER BY m.liteId ASC\").data()\n",
    "        total_percentage_glove = 0.0\n",
    "        documents_embeddings = [] #this is the WE-based Doc2Vec for the documents in the database\n",
    "        for doc in documents:\n",
    "            sum_embedding = np.zeros(shape=(embed_dim), dtype=float)\n",
    "            count_words = 0\n",
    "            total_words = 0\n",
    "            for word in doc['preprocessed']:\n",
    "                we = graph.run(\"MATCH (n:Word:_AI {name: $word}) RETURN n.embedding AS embedding\", parameters={'word':word}).data()\n",
    "                total_words += 1\n",
    "                if we:\n",
    "                    sum_embedding += we[0]['embedding']\n",
    "                    count_words += 1\n",
    "            if(count_words > 0):\n",
    "                total_percentage_glove += count_words/total_words\n",
    "                doc_embed = sum_embedding/count_words\n",
    "            else: doc_embed = sum_embedding*count_words\n",
    "            documents_embeddings.append(doc_embed)\n",
    "        serialization(documents_embeddings=documents_embeddings,file_name_cosine_similarity=index_WE_model_cs_name_glove, file_name_euclidean_distance=index_WE_model_eu_name_glove)\n",
    "        print(\"Percentage of words in GloVe: \" + str(total_percentage_glove/num_articles))           \n",
    "    \n",
    "    \n",
    "    if(mode=='title'): \n",
    "        documents = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.liteId AS lite_id, m.preprocessed AS preprocessed, m.keywords AS keywords, m.title AS title ORDER BY m.liteId ASC\").data()\n",
    "        total_percentage_glove = 0.0\n",
    "        sim_test_title = []\n",
    "        for doc in documents:\n",
    "            sum_embedding = np.zeros(shape=(embed_dim), dtype=float)\n",
    "            count_words = 0\n",
    "            total_words = 0\n",
    "            title = preprocess_text(doc['title'])\n",
    "            for word in doc['preprocessed']:\n",
    "                if(word in title):\n",
    "                    we = graph.run(\"MATCH (n:Word:_AI {name: $word}) RETURN n.embedding AS embedding\", parameters={'word':word}).data()\n",
    "                    total_words += 1\n",
    "                    if we:\n",
    "                        sum_embedding += we[0]['embedding']\n",
    "                        count_words += 1\n",
    "            if(count_words == 0): doc_embed = sum_embedding*count_words\n",
    "            else:\n",
    "                doc_embed = sum_embedding/count_words\n",
    "                total_percentage_glove += count_words/total_words\n",
    "            sim_test_title.append(doc_embed)\n",
    "        serialization(documents_embeddings=sim_test_title, file_name_cosine_similarity=index_WE_model_cs_title, file_name_euclidean_distance=index_WE_model_eu_title)\n",
    "        print(\"Percentage of words in GloVe: \" + str(total_percentage_glove/num_articles))\n",
    "        \n",
    "    \n",
    "    if(mode=='berglove'):\n",
    "        glove_embedding = WordEmbeddings('en-glove')\n",
    "        bert_embedding = BertEmbeddings('bert-large-uncased')\n",
    "        #multi_forward = FlairEmbeddings('multi-forward')\n",
    "        #multi_backward = FlairEmbeddings('multi-backward')\n",
    "        stacked_embeddings = StackedEmbeddings([\n",
    "                                                glove_embedding,\n",
    "                                                bert_embedding,\n",
    "                                                #FlairEmbeddings('news-forward'), \n",
    "                                                #FlairEmbeddings('news-backward'),\n",
    "                                                #multi_forward, \n",
    "                                                #multi_backward,\n",
    "                                               ])\n",
    "\n",
    "        # Embedding dimension\n",
    "        se = Sentence('grass')\n",
    "        stacked_embeddings.embed(se)\n",
    "        embed_dim = len(se[0].embedding)\n",
    "        documents = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.liteId AS lite_id, m.preprocessed AS preprocessed ORDER BY m.liteId ASC\").data()\n",
    "        num_doc = 1\n",
    "        documents_embeddings = [] #this is the WE-based Doc2Vec for the documents in the database\n",
    "        for doc in documents:\n",
    "            sentences = []\n",
    "            sum_embedding = np.zeros(shape=(embed_dim), dtype=float)\n",
    "            print(\"Computing Doc WE embedding: \" + str(num_doc) + \"/\" + str(num_articles), end=\"\\r\")\n",
    "            count_words = 0\n",
    "            sen = \"\"\n",
    "            for word in doc['preprocessed']:\n",
    "                temp = sen + word + \" \"\n",
    "                if(len(temp)>512):\n",
    "                    sentences.append(sen)\n",
    "                    sen = word + \" \"\n",
    "                else: sen = temp\n",
    "            sentences.append(sen)\n",
    "            for sen in sentences:\n",
    "                sentence = Sentence(sen)\n",
    "                stacked_embeddings.embed(sentence)\n",
    "                for token in sentence:\n",
    "                    sum_embedding += np.array(token.embedding)\n",
    "                    count_words += 1\n",
    "            if(count_words > 0):\n",
    "                doc_embed = sum_embedding/count_words\n",
    "            else: doc_embed = sum_embedding*0\n",
    "            documents_embeddings.append(doc_embed)\n",
    "            num_doc += 1\n",
    "        print(\"Computing Doc WE embedding: \" + str(num_doc) + \"/\" + str(num_articles), end=\"\\n\")\n",
    "        serialization(documents_embeddings=documents_embeddings,file_name_cosine_similarity=index_WE_model_cs_name_bert_glove, file_name_euclidean_distance=index_WE_model_eu_name_bert_glove)\n",
    "    \n",
    "    if(mode=='paper'):\n",
    "        #Paper Document Distances\n",
    "        time_ini = time.time()\n",
    "        word_mover_distance = WMD()\n",
    "        document_distances_paper = np.random.rand(num_articles,num_articles)*10000\n",
    "        counter = 0\n",
    "        total_to_count = int(num_articles**2)\n",
    "        doclist1 = graph.run(\"MATCH (m:Article:_AI) RETURN m.liteId AS lite_id, m.preprocessed AS preprocessed ORDER BY m.liteId ASC\").data()\n",
    "        for idd1,doc1 in enumerate(doclist1):\n",
    "            doclist2 = graph.run(\"MATCH (m:Article:_AI) RETURN m.liteId AS lite_id, m.preprocessed AS preprocessed ORDER BY m.liteId ASC\").data()\n",
    "            for idd2,doc2 in enumerate(doclist2):\n",
    "                percentage_proc = counter/total_to_count*100\n",
    "                print(\"Calculating document distance \" + str(idd1+1) + \" --> \" + str(idd2+1) + \"\\t Total num. of articles: \" + str(num_articles) + \" ({0:.1f}%)\".format(percentage_proc), end='\\r')\n",
    "                word_mover_distance.load_docs(doc1['preprocessed'],doc2['preprocessed'])\n",
    "                sol = word_mover_distance.calculate_distance()\n",
    "                document_distances_paper[idd1,idd2] = sol.fun\n",
    "                f = open(\"paper_distances.txt\", \"a\")\n",
    "                f.write(str(idd1) + \" \" + str(idd2) + \" \" + str(sol.fun) + \"\\n\")\n",
    "                f.close()\n",
    "                counter += 1\n",
    "        print(\"Calculating document distance \" + str(idd1+1) + \" --> \" + str(idd2+1) + \"\\t Total num. of articles: \" + str(num_articles) + \" ({0:3d}%)\\n\".format(100))\n",
    "        print(\"Serializing...\")\n",
    "        document_distances_paper.dump(index_WE_model_eu_name_paper)\n",
    "        time_elapsed = time.time() - time_ini\n",
    "        elapsed_hours = int(time_elapsed/3600)\n",
    "        elapsed_minutes = int(int(time_elapsed%3600)/60)\n",
    "        elapsed_seconds = int(int(time_elapsed%3600)%60)\n",
    "        print(\"WMD Document distances serialized.\")\n",
    "        print(\"Elapsed time Document Distances Paper: \" + str(elapsed_hours) + \" h \" + str(elapsed_minutes) + \" min \" + str(elapsed_seconds) + \" sec\\n\")\n",
    "\n",
    "def find_similardocs_lsi_training(lsi_model, lsi_index, num=11, doc=-1, return_results=True):\n",
    "    \"\"\"\n",
    "    Purpose: Find\n",
    "    Input:   \n",
    "    Output:  \n",
    "    \"\"\"\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "\n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    if(stemmed_tfidf): sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) AND m.liteId = $query_liteid RETURN m.preprocessed_stemmed AS preprocessed\"\n",
    "    else: sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) AND m.liteId = $query_liteid RETURN m.preprocessed AS preprocessed\"\n",
    "    lsiquery = graph.run(sim_query, parameters={'query_liteid': doc}).data()\n",
    "    if(stemmed_tfidf): doc_bow = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in lsiquery]\n",
    "    else: doc_bow = [corpus_memory_friendly_NST.dictionary.doc2bow(doc['preprocessed']) for doc in lsiquery]\n",
    "    doc_lsi = lsi_model[doc_bow]\n",
    "    docs_similar = lsi_index[doc_lsi]\n",
    "    sort_docs_similar = [sorted(enumerate(val), key=lambda item: -item[1])[:num] for it,val in enumerate(docs_similar)][0]\n",
    "    recommend_docs_idd = []\n",
    "    for idd,simil_score in sort_docs_similar[1:]:\n",
    "        recommend_docs_idd.append(idd)\n",
    "        #show no similar articles if there are not articles above a certain threshold (OBS: Right now we only show the best 3)\n",
    "    if return_results: return recommend_docs_idd\n",
    "    \n",
    "def find_similardocs_lda_training(lda_model, lda_index, num=11, doc=-1, return_results=True):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Input:\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "\n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    if(stemmed_tfidf): sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) AND m.liteId = $query_liteid RETURN m.preprocessed_stemmed AS preprocessed\"\n",
    "    else: sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) AND m.liteId = $query_liteid RETURN m.preprocessed AS preprocessed\"\n",
    "    ldaquery = graph.run(sim_query, parameters={'query_liteid': doc}).data()\n",
    "    if(stemmed_tfidf): doc_bow = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in ldaquery]\n",
    "    else: doc_bow = [corpus_memory_friendly_NST.dictionary.doc2bow(doc['preprocessed']) for doc in ldaquery]\n",
    "    doc_lda = lda_model[doc_bow]\n",
    "    docs_similar = lda_index[doc_lda]\n",
    "    sort_docs_similar = [sorted(enumerate(val), key=lambda item: -item[1])[:num] for it,val in enumerate(docs_similar)][0]\n",
    "    recommend_docs_idd = []\n",
    "    for idd,simil_score in sort_docs_similar[1:]:\n",
    "        recommend_docs_idd.append(idd)\n",
    "        #show no similar articles if there are not articles above a certain threshold (OBS: Right now we only show the best 3)\n",
    "    if return_results: return recommend_docs_idd\n",
    "\n",
    "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3, algorithm='lsa'):\n",
    "    \"\"\"\n",
    "    Input   : dictionary : Gensim dictionary\n",
    "              corpus : Feedly Teams corpus\n",
    "              texts : List of articles\n",
    "              stop : Max num of topics\n",
    "    purpose : Compute c_v coherence for various number of topics\n",
    "    Output  : model_list : List of LSA topic models\n",
    "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    print(\"Computing coherence analysis for \" + algorithm + \" ...\\n\")\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    maximum_coherence = -1\n",
    "    optimum_ntopics = 0\n",
    "    for num_topics in range(start, stop, step):\n",
    "        if((algorithm == 'lsa') or (algorithm == 'lsi')):\n",
    "            # generate LSA model\n",
    "            model = models.LsiModel(doc_term_matrix, num_topics=num_topics, id2word = dictionary)  # train model\n",
    "        elif(algorithm == 'lda'):\n",
    "            # generate LSA model\n",
    "            model = models.LdaModel(doc_term_matrix, num_topics=num_topics, id2word = dictionary)  # train model\n",
    "        else:\n",
    "            model = models.LsiModel(doc_term_matrix, num_topics=num_topics, id2word = dictionary)  # train model\n",
    "        coherencemodel = models.CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "        model_list.append(model)\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        if(coherencemodel.get_coherence() > maximum_coherence):\n",
    "            maximum_coherence = coherencemodel.get_coherence()\n",
    "            optimum_ntopics = num_topics\n",
    "        print(\"Coherence analysis: \" + str(num_topics) + \"/\" + str(stop), end=\"\\r\")\n",
    "    return model_list, coherence_values, maximum_coherence, optimum_ntopics\n",
    "\n",
    "def coherence_model(algorithm):\n",
    "    start = 2\n",
    "    stop = 15\n",
    "    step = 1\n",
    "    if(stemmed_tfidf): data = graph.run(\"MATCH (n:Article:_AI) WHERE EXISTS(n.preprocessed_stemmed) RETURN n.preprocessed_stemmed AS preproc ORDER BY n.liteId ASC\").data()\n",
    "    else: data = graph.run(\"MATCH (n:Article:_AI) WHERE EXISTS(n.preprocessed) RETURN n.preprocessed AS preproc ORDER BY n.liteId ASC\").data()\n",
    "    new_data = [item['preproc'] for item in data] #Careful: you are loading all the articles here!\n",
    "    if(stemmed_tfidf): model_list, coherence_values, maximum_coherence, optimum_ntopics = compute_coherence_values(dictionary=corpus_memory_friendly.dictionary, doc_term_matrix=corpus_memory_friendly, doc_clean=new_data, start=start, stop=stop, step=step, algorithm=algorithm)\n",
    "    else: model_list, coherence_values, maximum_coherence, optimum_ntopics = compute_coherence_values(dictionary=corpus_memory_friendly_NST.dictionary, doc_term_matrix=corpus_memory_friendly_NST, doc_clean=new_data, start=start, stop=stop, step=step, algorithm=algorithm)\n",
    "    # Show graph\n",
    "    x = range(start, stop, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(\"Coherence\", loc='best')\n",
    "    plt.show()\n",
    "    print(\"Optimum number of topics: \" + str(optimum_ntopics))\n",
    "    print(\"Max. coherence: \" + str(maximum_coherence))\n",
    "    \n",
    "def maximum_recall_score(algorithm):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Input:\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    start = 2\n",
    "    stop = 30\n",
    "    step = 1\n",
    "    if(stemmed_tfidf):\n",
    "        print(\"Computing for STEMMED WORDS\")\n",
    "        compare_docs = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) RETURN m.preprocessed_stemmed AS preprocessed ORDER BY m.liteId ASC\").data()\n",
    "        compare_docs_bow = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in compare_docs]\n",
    "    else:\n",
    "        print(\"Computing for NON-STEMMED WORDS\")\n",
    "        compare_docs = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.preprocessed AS preprocessed ORDER BY m.liteId ASC\").data()\n",
    "        compare_docs_bow = [corpus_memory_friendly_NST.dictionary.doc2bow(doc['preprocessed']) for doc in compare_docs]\n",
    "    print(\"Computing maximum recall score analysis for \" + algorithm + \" ...\\n\")\n",
    "    recall_scores = []\n",
    "    RBP_scores = []\n",
    "    RBPacc_scores = []\n",
    "    model_list = []\n",
    "    index_list = []\n",
    "    maximum_recall = -1\n",
    "    maximum_RBP = -1\n",
    "    maximum_RBPacc = -1\n",
    "    optimum_ntopics_recall = 0\n",
    "    optimum_ntopics_RBP = 0\n",
    "    optimum_ntopics_RBPacc = 0\n",
    "    for num_topics in range(start, stop+1, step):\n",
    "        if((algorithm == 'lsa') or (algorithm == 'lsi')):\n",
    "            # generate LSA model\n",
    "            if(stemmed_tfidf):\n",
    "                model = models.LsiModel(corpus_memory_friendly, id2word=corpus_memory_friendly.dictionary, num_topics=num_topics)\n",
    "                comp_lsi = model[compare_docs_bow]\n",
    "                ind_lsi = similarities.Similarity(output_prefix=\"sim_lsi_idx\", corpus=comp_lsi, num_features=len(corpus_memory_friendly.dictionary))\n",
    "            else:\n",
    "                model = models.LsiModel(corpus_memory_friendly_NST, id2word=corpus_memory_friendly_NST.dictionary, num_topics=num_topics)\n",
    "                comp_lsi = model[compare_docs_bow]\n",
    "                ind_lsi = similarities.Similarity(output_prefix=\"sim_lsi_nst_idx\", corpus=comp_lsi, num_features=len(corpus_memory_friendly_NST.dictionary))\n",
    "            mod = [model,ind_lsi]\n",
    "            recall = evaluate_method(mod)\n",
    "        elif(algorithm == 'lda'):\n",
    "            # generate LDA model\n",
    "            if(stemmed_tfidf):\n",
    "                model = models.LdaModel(corpus_memory_friendly, id2word=corpus_memory_friendly.dictionary, num_topics=num_topics, passes=15, alpha='auto', eval_every=5)\n",
    "                comp_lda = model[compare_docs_bow]\n",
    "                ind_lda = similarities.Similarity(output_prefix=\"sim_lda_idx\", corpus=comp_lda, num_features=len(corpus_memory_friendly.dictionary))\n",
    "            else:\n",
    "                model = models.LdaModel(corpus_memory_friendly_NST, id2word=corpus_memory_friendly_NST.dictionary, num_topics=num_topics, passes=15, alpha='auto', eval_every=5)\n",
    "                comp_lda = model[compare_docs_bow]\n",
    "                ind_lda = similarities.Similarity(output_prefix=\"sim_lda_nst_idx\", corpus=comp_lda, num_features=len(corpus_memory_friendly_NST.dictionary))\n",
    "            mod = [model,ind_lda]\n",
    "            recall = evaluate_method(mod)\n",
    "        model_list.append(model)\n",
    "        index_list.append(ind_lda)\n",
    "        recall_scores.append(recall[0])\n",
    "        RBP_scores.append(recall[1])\n",
    "        RBPacc_scores.append(recall[2])\n",
    "        if(recall[0] > maximum_recall):\n",
    "            maximum_recall = recall[0]\n",
    "            optimum_ntopics_recall = num_topics\n",
    "        if(recall[1] > maximum_RBP):\n",
    "            maximum_RBP = recall[1]\n",
    "            optimum_ntopics_RBP = num_topics\n",
    "        if(recall[2] > maximum_RBPacc):\n",
    "            maximum_RBPacc = recall[2]\n",
    "            optimum_ntopics_RBPacc = num_topics\n",
    "        print(\"Maximum recall analysis: \" + str(num_topics) + \"/\" + str(stop), end=\"\\r\")\n",
    "    model_list[optimum_ntopics_RBPacc-1].save(os.path.join(dir_name, \"LDA_MAX.model\"))\n",
    "    index_list[optimum_ntopics_RBPacc-1].save(os.path.join(dir_name, \"LDA_MAX_INDEX.index\"))\n",
    "    \n",
    "    return [recall_scores, RBP_scores, RBPacc_scores, start, step, stop]\n",
    "\n",
    "#Some callbacks for GUI events\n",
    "def keyentertitlecallback(event):\n",
    "    search_documents()\n",
    "\n",
    "def keyenterdocsamountcallback(event):\n",
    "    find_similardocuments()\n",
    "\n",
    "# Feature: Give me some insight! (click in icon at the center of the GUI)\n",
    "def create_input_wordcloud(input_item):\n",
    "    string_wordcloud = \"\"\n",
    "    if isinstance(input_item, str): #community\n",
    "        query = \"MATCH (n:Article:_AI) WHERE n.community_louvain_filtered_1 = $community_query RETURN n.keywords_nst AS result\" #we can do it with content, keywords...\n",
    "        data = graph.run(query, parameters={'community_query': input_item}).data()\n",
    "        for article in data: #by content also?\n",
    "            for word in article['result']:\n",
    "                string_wordcloud += word + \" \"\n",
    "    elif isinstance(input_item, int): #lite_id document\n",
    "        query = \"MATCH (n:Article:_AI {liteId: $lite_id}) RETURN n.preprocessed AS result\" #we can do it with content, keywords...\n",
    "        data = graph.run(query, parameters={'lite_id': input_item}).data()\n",
    "        for word in data[0]['result']:\n",
    "            string_wordcloud += word + \" \"\n",
    "    return string_wordcloud\n",
    "\n",
    "def build_wordcloud_ideas(tipology):\n",
    "    if(doc_insight.get() == 0): #query document\n",
    "        if(doc_var_idx.get() != no_article):\n",
    "            if(tipology==\"community\"):\n",
    "                community = graph.run(\"MATCH (n:Article:_AI {liteId: $lite_id}) RETURN n.community_louvain_filtered_1 AS community LIMIT 1\", parameters={'lite_id': doc_var_idx.get()}).data()[0]['community']\n",
    "                item_ext = community\n",
    "            elif(tipology==\"document\"):\n",
    "                item_ext = doc_var_idx.get()\n",
    "        else:\n",
    "            output_screen['text'] = \"You must select a document first.\"\n",
    "            \n",
    "    else: #recommended document\n",
    "        if(doc_sim_idx.get() != no_article):\n",
    "            if(tipology==\"community\"):\n",
    "                community = graph.run(\"MATCH (n:Article:_AI {liteId: $lite_id}) RETURN n.community_louvain_filtered_1 AS community LIMIT 1\", parameters={'lite_id': doc_sim_idx.get()}).data()[0]['community']\n",
    "                item_ext = community\n",
    "            elif(tipology==\"document\"):\n",
    "                item_ext = doc_sim_idx.get()\n",
    "        else:\n",
    "            output_screen['text'] = \"You must select a document first.\"\n",
    "            \n",
    "    directory_wordcloud = os.path.join(dir_name, 'wordclouds/')\n",
    "    file_wordcloud = os.path.join(directory_wordcloud, tipology + \"_\" + str(item_ext) + \".png\") \n",
    "    \n",
    "    if not os.path.isfile(file_wordcloud):\n",
    "        mask = np.array(Image.open(os.path.join(directory_wordcloud, 'cloud.png')))\n",
    "        wc = WordCloud(background_color=\"white\", mask=mask, max_words=200, stopwords=stopwords.words('english'))\n",
    "        text = create_input_wordcloud(item_ext)\n",
    "        wc.generate(text)\n",
    "        wc.to_file(file_wordcloud)\n",
    "    window_wordcloud = tk.Toplevel(root, height=HEIGHT, width=WIDTH*2)\n",
    "    wordcloud_image = ImageTk.PhotoImage(Image.open(file_wordcloud), master=window_wordcloud)\n",
    "    wordcloud_label = tk.Label(window_wordcloud, image=wordcloud_image)\n",
    "    wordcloud_label.place(anchor='n', relx=0.5, rely=0, relwidth=1, relheight=1)\n",
    "    window_wordcloud.mainloop()\n",
    "    \n",
    "def find_relations():\n",
    "    if(doc_var_idx.get() != no_article):\n",
    "        if(doc_sim_idx.get() != no_article):\n",
    "            first_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.keywords_nst) AND m.liteId = $query_liteid RETURN m.keywords_nst AS keywords\"\n",
    "            query = graph.run(first_query, parameters={'query_liteid': doc_var_idx.get()}).data()\n",
    "            if query:\n",
    "                sec_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.keywords) AND m.liteId = $query_liteid RETURN m.keywords AS keywords\"\n",
    "                second_query = graph.run(sec_query, parameters={'query_liteid': doc_sim_idx.get()}).data()\n",
    "                if second_query:\n",
    "                    stemmer = PorterStemmer()\n",
    "                    list_common_themes = []\n",
    "                    list_common_stems = []\n",
    "                    for keyword in query[0]['keywords']:\n",
    "                        stword = stemmer.stem(keyword)\n",
    "                        if stword in second_query[0]['keywords'] and stword not in list_common_stems: #\n",
    "                            list_common_themes.append(keyword)\n",
    "                            list_common_stems.append(stword)\n",
    "                            \n",
    "                    if not list_common_themes:\n",
    "                        tk.messagebox.showinfo(\"Info\", \"There were no words in common or the documents are not actually related.\")\n",
    "                    else:\n",
    "                        text_insight = \"The documents share the next concepts:\\n\"\n",
    "                        for word in list_common_themes[:-1]:\n",
    "                            text_insight += word + \", \"\n",
    "                        text_insight += list_common_themes[-1]\n",
    "                        output_screen['text'] = \"Showing insight of two documents.\"\n",
    "                        tk.messagebox.showinfo(\"Info\", text_insight)\n",
    "                else:\n",
    "                    tk.messagebox.showwarning(\"Info\", \"The application could not find the second document.\")\n",
    "            else:\n",
    "                tk.messagebox.showwarning(\"Info\", \"The application could not find the queried document.\")\n",
    "        else:\n",
    "            tk.messagebox.showinfo(\"Info\", \"You must select a second document.\")\n",
    "    else:\n",
    "        tk.messagebox.showinfo(\"Info\", \"You must select a first document.\")\n",
    "\n",
    "def insightinfocallback(event):\n",
    "    global doc_insight\n",
    "    window_insights = tk.Toplevel(root, height=HEIGHT/2-10, width=WIDTH/2-10)\n",
    "    title_insights = tk.Label(window_insights, text=\"Select an insight\")\n",
    "    title_insights.config(font=(\"Volvo Broad Pro\", 13))\n",
    "    title_insights.place(anchor='n', relx=0.5, rely=0.05, relwidth=0.6, relheight=0.08)\n",
    "    frame_select_document = tk.Frame(window_insights)\n",
    "    frame_select_document.place(relx=0.05, rely=0.15, anchor='nw', relwidth=0.7, relheight=0.15)\n",
    "    r1 = tk.Radiobutton(frame_select_document, text=\"For the query document\", selectcolor='#e6f2ff', wraplength=255, variable=doc_insight, value=0)\n",
    "    r1.config(font=(FONT_ARTICLES, 8))\n",
    "    r1.pack(anchor = 'w')\n",
    "    r2 = tk.Radiobutton(frame_select_document, text=\"For the recommended document\", selectcolor='#e6f2ff', wraplength=255, variable=doc_insight, value=1)\n",
    "    r2.config(font=(FONT_ARTICLES, 8))\n",
    "    r2.pack(anchor = 'w')\n",
    "    button_source_material = tk.Button(window_insights, text=\"Read source material\")\n",
    "    button_source_material.config(font=(TEXT_FONT, 10))\n",
    "    button_source_material.config(command=lambda: read_source_material())\n",
    "    button_source_material.place(anchor='n', relx=0.5, rely=0.32, relwidth=0.5, relheight=0.1)\n",
    "    button_wordcloud_document = tk.Button(window_insights, text=\"Idea of the document\")\n",
    "    button_wordcloud_document.config(font=(TEXT_FONT, 10))\n",
    "    button_wordcloud_document.config(command=lambda: build_wordcloud_ideas('document'))\n",
    "    button_wordcloud_document.place(anchor='n', relx=0.5, rely=0.43, relwidth=0.5, relheight=0.1)\n",
    "    button_wordcloud_community = tk.Button(window_insights, text=\"What else is in the topic?\")\n",
    "    button_wordcloud_community.config(font=(TEXT_FONT, 10))\n",
    "    button_wordcloud_community.config(command=lambda: build_wordcloud_ideas('community'))\n",
    "    button_wordcloud_community.place(anchor='n', relx=0.5, rely=0.54, relwidth=0.62, relheight=0.1)\n",
    "    button_why_related = tk.Button(window_insights, text=\"Why are they related?\")\n",
    "    button_why_related.config(font=(TEXT_FONT, 10))\n",
    "    button_why_related.config(command=lambda: find_relations())\n",
    "    button_why_related.place(anchor='n', relx=0.5, rely=0.65, relwidth=0.62, relheight=0.1)\n",
    "    \"\"\"\n",
    "    if(doc_var_idx.get() != no_article):\n",
    "        if(doc_sim_idx.get() != no_article):\n",
    "            first_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.keywords) AND m.liteId = $query_liteid RETURN m.keywords_nst AS keywords\"\n",
    "            query = graph.run(first_query, parameters={'query_liteid': doc_var_idx.get()}).data()\n",
    "            if query:\n",
    "                sec_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.keywords) AND m.liteId = $query_liteid RETURN m.keywords AS keywords\"\n",
    "                second_query = graph.run(sec_query, parameters={'query_liteid': doc_sim_idx.get()}).data()\n",
    "                if second_query:\n",
    "                    stemmer = PorterStemmer()\n",
    "                    list_common_themes = []\n",
    "                    list_common_stems = []\n",
    "                    for keyword in query[0]['keywords']:\n",
    "                        stword = stemmer.stem(keyword)\n",
    "                        if stword in second_query[0]['keywords'] and stword not in list_common_stems: #\n",
    "                            list_common_themes.append(keyword)\n",
    "                            list_common_stems.append(stword)\n",
    "                            \n",
    "                    if not list_common_themes:\n",
    "                        tk.messagebox.showinfo(\"Info\", \"There were no words in common or the documents are not actually related.\")\n",
    "                    else:\n",
    "                        text_insight = \"The documents share the next concepts:\\n\"\n",
    "                        for word in list_common_themes[:-1]:\n",
    "                            text_insight += word + \", \"\n",
    "                        text_insight += list_common_themes[-1]\n",
    "                        tk.messagebox.showinfo(\"Info\", text_insight)\n",
    "                else:\n",
    "                    tk.messagebox.showwarning(\"Info\", \"We couldn't find the second document.\\nCheck with Ivan.\")\n",
    "            else:\n",
    "                tk.messagebox.showwarning(\"Info\", \"We couldn't find the document.\\nCheck with Ivan.\")\n",
    "            #tk.messagebox.showinfo(\"Info\", \"This is meant to show insight.\")\n",
    "        else:\n",
    "            tk.messagebox.showinfo(\"Info\", \"You must select a second document.\")\n",
    "    else:\n",
    "        tk.messagebox.showinfo(\"Info\", \"You must select a document.\")\n",
    "    \"\"\"\n",
    "    window_insights.mainloop()\n",
    "\n",
    "def insightlabcallback(event):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Input:\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    global LST_features, ocult_train, stemmed_tfidf, tfidf, index_tfidf\n",
    "    if(ocult_train):\n",
    "        answerst = tk.messagebox.askyesno(\"Want to use the Non-stemmed version?\",\"Do you want to change to the NST version of the TF-IDF?\\nIf not, the standard stemmed version will be loaded.\")\n",
    "        stemmed_tfidf = not answerst\n",
    "        train_button = tk.Button(footer_frame, text=\"Train Algorithm\")\n",
    "        train_button.config(font=(TEXT_FONT, 10))\n",
    "        train_button.config(command=lambda: train_algorithm())\n",
    "        train_button.place(anchor='nw', relx=0.505, rely=0, relwidth=0.2, relheight=0.47)\n",
    "        evaluate_button = tk.Button(footer_frame, text=\"Evaluate\")\n",
    "        evaluate_button.config(font=(TEXT_FONT, 10))\n",
    "        evaluate_button.config(command=lambda: evaluate_method())\n",
    "        evaluate_button.place(anchor='sw', relx=0.505, rely=1, relwidth=0.2, relheight=0.53)\n",
    "        LST_features.append(train_button)\n",
    "        LST_features.append(evaluate_button)\n",
    "        output_screen['text'] = \"Wow, you discovered a new feature!\\n(Only for developers)\" #change icon to developer?\n",
    "        ocult_train = not ocult_train\n",
    "    else:\n",
    "        for i in LST_features:\n",
    "            i.destroy()\n",
    "        ocult_train = not ocult_train\n",
    "        output_screen['text'] = \"\" #change icon to developer?\n",
    "\n",
    "#Functions: Scrolling with the mouse\n",
    "def _bound_to_mousewheel(event):\n",
    "    left_dlistcanvas.bind_all(\"<MouseWheel>\", scroll_documentscallback)\n",
    "\n",
    "def _unbound_to_mousewheel(event):\n",
    "    left_dlistcanvas.unbind_all(\"<MouseWheel>\")\n",
    "\n",
    "def _bound_to_mousewheelsim(event):\n",
    "    right_dlistcanvas.bind_all(\"<MouseWheel>\", scroll_documentscallbacksim)\n",
    "\n",
    "def _unbound_to_mousewheelsim(event):\n",
    "    right_dlistcanvas.unbind_all(\"<MouseWheel>\")\n",
    "\n",
    "def scroll_documentscallback(event):\n",
    "    if(left_dlistframe.winfo_height() > left_dlistcanvas.winfo_height()):\n",
    "        left_dlistcanvas.yview_scroll(-1*int((event.delta/120)), \"units\")\n",
    "        \n",
    "def scroll_documentscallbacksim(event):\n",
    "    if(right_dlistframe.winfo_height() > right_dlistcanvas.winfo_height()):\n",
    "        right_dlistcanvas.yview_scroll(-1*int((event.delta/120)), \"units\")\n",
    "\n",
    "##################################################\n",
    "#  Functions: Give me related/similar documents  #\n",
    "#------------------------------------------------#\n",
    "##################################################\n",
    "\n",
    "def find_similardocuments():\n",
    "    \"\"\"\n",
    "    Purpose: Using the algorithms to find the closest or most similar documents.\n",
    "    Input:   None. The function will use the models trained in disk and the selected article from the GUI.\n",
    "    Output:  None. The representation of related documents will be shown in the GUI.\n",
    "    \"\"\"\n",
    "    global doc_sim_idx\n",
    "    pred_amount_doc = 10\n",
    "    valid_number = True\n",
    "    if(doc_var_idx.get() != no_article):\n",
    "        for label in LIST_NOT_FOUND_LABELS:\n",
    "            label.destroy()\n",
    "    \n",
    "        #Check if the user has defined an amount#\n",
    "        if entry_docs.get():\n",
    "            try:\n",
    "                amount = int(entry_docs.get(),10)\n",
    "            except:\n",
    "                valid_number = False\n",
    "                amount = pred_amount_doc\n",
    "            else:\n",
    "                if(amount <= 0): valid_number=False\n",
    "                else: annex = \"\\nShowing the \" + str(amount) + \" most similar article/s.\"\n",
    "        else:\n",
    "            annex = \"\\nPredifined: showing the \" + str(pred_amount_doc) + \" most similar articles.\"\n",
    "            amount = pred_amount_doc\n",
    "        if valid_number:\n",
    "            amount += 1\n",
    "            if(algorithmvariable.get()==\"Word Embeddings\"):\n",
    "                showing_recommendations(algorithmvariable.get()+annex)\n",
    "                find_similardocs_WE(amount)\n",
    "            elif(algorithmvariable.get()==\"TF-IDF\"):\n",
    "                showing_recommendations(algorithmvariable.get()+annex)\n",
    "                if(stemmed_tfidf): find_similardocs_tfidf(amount)\n",
    "                else: find_similardocs_tfidf_nst(amount)\n",
    "            elif(algorithmvariable.get()==\"Doc2vec\"):\n",
    "                not_implemented_message()\n",
    "            elif(algorithmvariable.get()==\"LSA\"):\n",
    "                showing_recommendations(\"Latent Semantic Analysis\"+annex)\n",
    "                if(stemmed_tfidf): find_similardocs_lsi(amount)\n",
    "                else: find_similardocs_lsi_nst(amount)\n",
    "            elif(algorithmvariable.get()==\"LDA\"):\n",
    "                showing_recommendations(\"Latent Dirichlet Allocation\"+annex)\n",
    "                find_similardocs_lda(amount)\n",
    "            elif(algorithmvariable.get()==\"Ensemble Method\"):\n",
    "                amount -= 1\n",
    "                find_similardocs_ensemble(amount)\n",
    "            elif(algorithmvariable.get()==\"Community Finding\"):\n",
    "                amount -= 1\n",
    "                find_similardocs_community(amount)\n",
    "            else:\n",
    "                algorithm_not_found_message()\n",
    "        else:\n",
    "            not_valid_message()\n",
    "        \n",
    "    else: article_not_selected_message()\n",
    "    update_idle()\n",
    "        \n",
    "\n",
    "def find_similardocs_tfidf(num=11, doc=-1, return_results=False):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "    \n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) AND m.liteId = $query_liteid RETURN m.preprocessed_stemmed AS preprocessed\"\n",
    "    query = graph.run(sim_query, parameters={'query_liteid': doc}).data()\n",
    "    doc_bow = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in query]\n",
    "    doc_tfidf = tfidf[doc_bow]\n",
    "    docs_similar = index_tfidf[doc_tfidf]\n",
    "    sort_docs_similar = [sorted(enumerate(val), key=lambda item: -item[1])[:num] for it,val in enumerate(docs_similar)][0]\n",
    "    recommend_docs_idd = show_results(sort_docs_similar)\n",
    "    if return_results: return recommend_docs_idd\n",
    "\n",
    "def find_similardocs_tfidf_nst(num=11, doc=-1, return_results=False):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "    \n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) AND m.liteId = $query_liteid RETURN m.preprocessed AS preprocessed\"\n",
    "    query = graph.run(sim_query, parameters={'query_liteid': doc}).data()\n",
    "    doc_bow = [corpus_memory_friendly_NST.dictionary.doc2bow(doc['preprocessed']) for doc in query]\n",
    "    doc_tfidf = tfidf_nst[doc_bow]\n",
    "    docs_similar = index_tfidf_nst[doc_tfidf]\n",
    "    sort_docs_similar = [sorted(enumerate(val), key=lambda item: -item[1])[:num] for it,val in enumerate(docs_similar)][0]\n",
    "    recommend_docs_idd = show_results(sort_docs_similar)\n",
    "    if return_results: return recommend_docs_idd\n",
    "    \n",
    "def find_similardocs_lsi(num=11, doc=-1, return_results=False):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "\n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) AND m.liteId = $query_liteid RETURN m.preprocessed_stemmed AS preprocessed\"\n",
    "    lsiquery = graph.run(sim_query, parameters={'query_liteid': doc}).data()\n",
    "    doc_bow = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in lsiquery]\n",
    "    doc_lsi = lsi[doc_bow]\n",
    "    docs_similar = index_lsi[doc_lsi]\n",
    "    sort_docs_similar = [sorted(enumerate(val), key=lambda item: -item[1])[:num] for it,val in enumerate(docs_similar)][0]\n",
    "    recommend_docs_idd = show_results(sort_docs_similar)\n",
    "    if return_results: return recommend_docs_idd\n",
    "\n",
    "def find_similardocs_lsi_nst(num=11, doc=-1, return_results=False):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "\n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) AND m.liteId = $query_liteid RETURN m.preprocessed AS preprocessed\"\n",
    "    lsiquery = graph.run(sim_query, parameters={'query_liteid': doc}).data()\n",
    "    doc_bow = [corpus_memory_friendly_NST.dictionary.doc2bow(doc['preprocessed']) for doc in lsiquery]\n",
    "    doc_lsi = lsi_nst[doc_bow]\n",
    "    docs_similar = index_lsi_nst[doc_lsi]\n",
    "    sort_docs_similar = [sorted(enumerate(val), key=lambda item: -item[1])[:num] for it,val in enumerate(docs_similar)][0]\n",
    "    recommend_docs_idd = show_results(sort_docs_similar)\n",
    "    if return_results: return recommend_docs_idd\n",
    "\n",
    "def find_similardocs_lda(num=11, doc=-1, return_results=False):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "\n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) AND m.liteId = $query_liteid RETURN m.preprocessed_stemmed AS preprocessed\"\n",
    "    ldaquery = graph.run(sim_query, parameters={'query_liteid': doc}).data()\n",
    "    doc_bow = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in ldaquery]\n",
    "    doc_lda = lda[doc_bow]\n",
    "    docs_similar = index_lda[doc_lda]\n",
    "    sort_docs_similar = [sorted(enumerate(val), key=lambda item: -item[1])[:num] for it,val in enumerate(docs_similar)][0]\n",
    "    recommend_docs_idd = show_results(sort_docs_similar)\n",
    "    if return_results: return recommend_docs_idd\n",
    "    \n",
    "def find_similardocs_lda_nst(num=11, doc=-1, return_results=False):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "\n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) AND m.liteId = $query_liteid RETURN m.preprocessed AS preprocessed\"\n",
    "    ldaquery = graph.run(sim_query, parameters={'query_liteid': doc}).data()\n",
    "    doc_bow = [corpus_memory_friendly_NST.dictionary.doc2bow(doc['preprocessed']) for doc in ldaquery]\n",
    "    doc_lda = lda_nst[doc_bow]\n",
    "    docs_similar = index_lda_nst[doc_lda]\n",
    "    sort_docs_similar = [sorted(enumerate(val), key=lambda item: -item[1])[:num] for it,val in enumerate(docs_similar)][0]\n",
    "    recommend_docs_idd = show_results(sort_docs_similar)\n",
    "    if return_results: return recommend_docs_idd\n",
    "    \n",
    "def find_similardocs_WE(num=11, doc=-1, return_results=False):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "    tech = 'pp'\n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    if (tech == 'cs'):\n",
    "        sim_matrix = np.load(index_WE_model_cs_name_bert_glove)[doc]\n",
    "        sort_docs_similar = [val for it,val in enumerate(sim_matrix)]\n",
    "        similar_docs = sorted(enumerate(sort_docs_similar), key=lambda item: -item[1])[:num]\n",
    "    elif (tech == 'eu'):\n",
    "        dist_matrix = np.load(index_WE_model_eu_name_bert_glove)[doc]\n",
    "        sort_docs_distances = [val for it,val in enumerate(dist_matrix)]\n",
    "        similar_docs = sorted(enumerate(sort_docs_distances), key=lambda item: item[1])[:num]\n",
    "    elif (tech == 'pp'):\n",
    "        dist_matrix = np.load(index_WE_model_eu_name_paper)[doc]\n",
    "        sort_docs_distances = [val for it,val in enumerate(dist_matrix)]\n",
    "        similar_docs = sorted(enumerate(sort_docs_distances), key=lambda item: item[1])[:num]\n",
    "    recommend_docs_idd = show_results(similar_docs)\n",
    "    if return_results: return recommend_docs_idd\n",
    "    \n",
    "def find_similardocs_ensemble(num=10, doc=-1, return_results=False):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "\n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    similarity_query = \"MATCH (m:Article:_AI {liteId: $query_liteid})-[r:RELATES_TO]->(a2:Article:_AI) RETURN a2.title AS title, a2.liteId AS lite_id, r.weight AS weight ORDER BY r.weight DESC LIMIT $num_docs\"\n",
    "    community_query = graph.run(similarity_query, parameters={'query_liteid': doc, 'num_docs': num}).data()\n",
    "    recommend_docs_idd = []\n",
    "    for article in community_query:\n",
    "        label = article['title'] + \" (\" + str(article['weight']) + \")\"\n",
    "        r = tk.Radiobutton(right_dlistframe, text=label, selectcolor='#e6f2ff', wraplength=255, bg='white', relief='ridge', overrelief='ridge', indicatoron=False, variable=doc_sim_idx, value=article['lite_id'])\n",
    "        r.config(font=(FONT_ARTICLES, 9))\n",
    "        r.pack(anchor = 'w', fill='x')\n",
    "        LIST_SIM_DOCS.append(r)\n",
    "        recommend_docs_idd.append(article['lite_id'])\n",
    "    if return_results: return recommend_docs_idd\n",
    "\n",
    "def find_similardocs_community(num=10, doc=-1, return_results=False):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "\n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    similarity_query = \"MATCH (a1:Article:_AI {liteId: $query_liteid})-[r:RELATES_TO]->(a2:Article:_AI) WHERE a1.community_louvain_filtered_1 = a2.community_louvain_filtered_1 RETURN a2.title AS title, a2.liteId AS lite_id, r.weight AS weight ORDER BY r.weight DESC LIMIT $num_docs\"\n",
    "    community_query = graph.run(similarity_query, parameters={'query_liteid': doc, 'num_docs': num}).data()\n",
    "    recommend_docs_idd = []\n",
    "    for article in community_query:\n",
    "        label = article['title'] + \" (\" + str(article['weight']) + \")\"\n",
    "        r = tk.Radiobutton(right_dlistframe, text=label, selectcolor='#e6f2ff', wraplength=255, bg='white', relief='ridge', overrelief='ridge', indicatoron=False, variable=doc_sim_idx, value=article['lite_id'])\n",
    "        r.config(font=(FONT_ARTICLES, 9))\n",
    "        r.pack(anchor = 'w', fill='x')\n",
    "        LIST_SIM_DOCS.append(r)\n",
    "        recommend_docs_idd.append(article['lite_id'])\n",
    "    if return_results: return recommend_docs_idd\n",
    "\n",
    "def show_results(docs):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "    recommend_docs_idd = []\n",
    "    for idd,simil_score in docs[1:]:\n",
    "        query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.content) AND m.liteId = $lite_id RETURN m.title AS title, m.content AS content, m.liteId AS lite_id\"\n",
    "        retrieve_document = graph.run(query, parameters={'lite_id': idd}).data()\n",
    "        label = retrieve_document[0]['title'] + \" (\" + str(simil_score) + \")\"\n",
    "        r = tk.Radiobutton(right_dlistframe, text=label, selectcolor='#e6f2ff', wraplength=255, bg='white', relief='ridge', overrelief='ridge', indicatoron=False, variable=doc_sim_idx, value=retrieve_document[0]['lite_id'])\n",
    "        r.config(font=(FONT_ARTICLES, 9))\n",
    "        r.pack(anchor = 'w', fill='x')\n",
    "        LIST_SIM_DOCS.append(r)\n",
    "        recommend_docs_idd.append(idd)\n",
    "    return recommend_docs_idd\n",
    "\n",
    "#Function: update the GUI, so new content can be showed!\n",
    "def update_idle():\n",
    "    right_dlistcanvas.update_idletasks()\n",
    "    doc_sim_idx.set(no_article)\n",
    "    right_dlistcanvas.configure(scrollregion=right_dlistcanvas.bbox('all'))\n",
    "\n",
    "#Secret feature: Evaluate accuracy with the test set!\n",
    "def evaluate_method(models=[]):\n",
    "    \"\"\"\n",
    "    Purpose: Have a metric to objectively compare different models, also different versions of the same algorithm.\n",
    "    Input:   Normally it is not used. Only in the version when training a model to maximize the evaluation score\n",
    "             will be necessary (LSI/LDA for max recall).\n",
    "             models - [lsi_model, lsi_index] list of the model and index for similarity evaluation.\n",
    "    Output:  None. The result will be shown in the GUI.\n",
    "             Provides:\n",
    "                 Recall: Accuracy metric over what documents of the same cluster appear in a direct recommendation.\n",
    "                 RBP:    Score that values the rank of the recommendations. The smaller the rank, the higher relevance.\n",
    "                 RBPacc: A multiplication of the two previous metrics. This is in order to value not only accuracy\n",
    "                         but also the order in which the recommendations appear in the GUI.\n",
    "                 \n",
    "                 For more information, read the paper \"Feeling Lucky? Multi-armed Bandits for Ordering Judgements in\n",
    "                    Pooling-based Evaluation\" by David E. Losada et al. (2016) OR the project documentation.\n",
    "    \"\"\"\n",
    "    evaluation_matrix = [[37, 104, 145, 144],\n",
    "                         [175, 113, 44, 11],\n",
    "                         [160, 153, 23, 25, 135, 222],\n",
    "                         [329, 212, 152],\n",
    "                         [179, 190, 29, 122, 277],\n",
    "                         [57, 2, 42, 81, 39],\n",
    "                         [58, 104, 116],\n",
    "                         [147, 18, 224, 347],\n",
    "                         [10, 225],\n",
    "                         [304, 303, 340],\n",
    "                         [3, 98],\n",
    "                         [98, 337, 196, 304],\n",
    "                         [3, 346],\n",
    "                         [61, 196],\n",
    "                         [1, 209, 328, 267, 287, 281],\n",
    "                         [8, 94]] #test set evaluation with known clusters manually evaluated\n",
    "    total_recommendations = 0\n",
    "    total_expected_recommendations = 0\n",
    "    decay = 0.85 #decay coefficient\n",
    "    RBP = 0 #rank-biased precision\n",
    "    RBPacc = 0 #rank-biased precision x recall accuracy measurement\n",
    "    if(algorithmvariable.get()==\"Word Embeddings\"):\n",
    "        for cluster in evaluation_matrix:\n",
    "            n_doc_cluster = len(cluster)\n",
    "            for itarticle in cluster:\n",
    "                recom = find_similardocs_WE(doc=itarticle, return_results=True)\n",
    "                for i,rec in enumerate(recom):\n",
    "                    if (rec in cluster):\n",
    "                        total_recommendations += 1\n",
    "                        RBP += decay**i\n",
    "                total_recommendations += 1 #to include itself in the cluster recommendation\n",
    "                total_expected_recommendations += n_doc_cluster\n",
    "    elif(algorithmvariable.get()==\"TF-IDF\"):\n",
    "         for cluster in evaluation_matrix:\n",
    "            n_doc_cluster = len(cluster)\n",
    "            for itarticle in cluster:\n",
    "                if(stemmed_tfidf): recom = find_similardocs_tfidf(doc=itarticle, return_results=True)\n",
    "                else: recom = find_similardocs_tfidf_nst(doc=itarticle, return_results=True)\n",
    "                for i,rec in enumerate(recom):\n",
    "                    if (rec in cluster):\n",
    "                        total_recommendations += 1\n",
    "                        RBP += decay**i\n",
    "                total_recommendations += 1 #to include itself in the cluster recommendation\n",
    "                total_expected_recommendations += n_doc_cluster\n",
    "    elif(algorithmvariable.get()==\"Doc2vec\"):\n",
    "        not_implemented_message()\n",
    "    elif(algorithmvariable.get()==\"LSA\"):\n",
    "        for cluster in evaluation_matrix:\n",
    "            n_doc_cluster = len(cluster)\n",
    "            for itarticle in cluster:\n",
    "                if(algorithm_training):\n",
    "                    recom = find_similardocs_lsi_training(doc=itarticle, lsi_model=models[0], lsi_index=models[1], return_results=True)\n",
    "                else:\n",
    "                    if(stemmed_tfidf): recom = find_similardocs_lsi(doc=itarticle, return_results=True)\n",
    "                    else: recom = find_similardocs_lsi_nst(doc=itarticle, return_results=True)\n",
    "                for i,rec in enumerate(recom):\n",
    "                    if (rec in cluster):\n",
    "                        total_recommendations += 1\n",
    "                        RBP += decay**i\n",
    "                total_recommendations += 1 #to include itself in the cluster recommendation\n",
    "                total_expected_recommendations += n_doc_cluster\n",
    "    elif(algorithmvariable.get()==\"LDA\"):\n",
    "        for cluster in evaluation_matrix:\n",
    "            n_doc_cluster = len(cluster)\n",
    "            for itarticle in cluster:\n",
    "                if(algorithm_training):\n",
    "                    recom = find_similardocs_lda_training(doc=itarticle, lda_model=models[0], lda_index=models[1], return_results=True)\n",
    "                else:\n",
    "                    if(stemmed_tfidf): recom = find_similardocs_lda(doc=itarticle, return_results=True)\n",
    "                    else: recom = find_similardocs_lda_nst(doc=itarticle, return_results=True)\n",
    "                for i,rec in enumerate(recom):\n",
    "                    if (rec in cluster):\n",
    "                        total_recommendations += 1\n",
    "                        RBP += decay**i\n",
    "                total_recommendations += 1 #to include itself in the cluster recommendation\n",
    "                total_expected_recommendations += n_doc_cluster\n",
    "    elif(algorithmvariable.get()==\"Ensemble Method\"):\n",
    "        for cluster in evaluation_matrix:\n",
    "            n_doc_cluster = len(cluster)\n",
    "            for itarticle in cluster:\n",
    "                recom = find_similardocs_ensemble(doc=itarticle, return_results=True)\n",
    "                for i,rec in enumerate(recom):\n",
    "                    if (rec in cluster):\n",
    "                        total_recommendations += 1\n",
    "                        RBP += decay**i\n",
    "                total_recommendations += 1 #to include itself in the cluster recommendation\n",
    "                total_expected_recommendations += n_doc_cluster\n",
    "    elif(algorithmvariable.get()==\"Community Finding\"):\n",
    "        for cluster in evaluation_matrix:\n",
    "            n_doc_cluster = len(cluster)\n",
    "            dictionary_evaluation = dict()\n",
    "            for itarticle in cluster:\n",
    "                community_found = graph.run(\"MATCH (n:Article:_AI {liteId: $lite_id}) RETURN DISTINCT n.community_louvain_filtered_1 AS community LIMIT 1\", parameters={'lite_id': itarticle}).data()\n",
    "                dictionary_evaluation[community_found[0]['community']] = dictionary_evaluation.get(community_found[0]['community'], 0) + 1\n",
    "                recom = find_similardocs_community(doc=itarticle, return_results=True)\n",
    "                for i,rec in enumerate(recom):\n",
    "                    if (rec in cluster):\n",
    "                        RBP += decay**i\n",
    "            dictionary_evaluation = sorted(dictionary_evaluation.items(), key= lambda item: -item[1])\n",
    "            total_recommendations += dictionary_evaluation[0][1]\n",
    "            total_expected_recommendations += n_doc_cluster\n",
    "    else:\n",
    "        algorithm_not_found_message()\n",
    "        \n",
    "    if(total_expected_recommendations > 0):\n",
    "        recomendation_evaluation = total_recommendations/total_expected_recommendations\n",
    "        RBPacc = RBP*recomendation_evaluation\n",
    "        output_screen['text'] = \"Recall score: %.2f\\nThe RBP score: %.2f\\nThe RBP-accuracy is: %.2f\" % (recomendation_evaluation,RBP,RBPacc)\n",
    "        if(algorithm_training): return [recomendation_evaluation,RBP,RBPacc]\n",
    "    update_idle() #update the GUI to represent the new things!\n",
    "\n",
    "#Feature: Searching and visualizing an article in the web browser\n",
    "def read_source_material():\n",
    "    \"\"\"\n",
    "    Purpose: Open a new tab in the web browser with the url to the selected article in the GUI.\n",
    "    Input: None. Selection from the GUI (global variable).\n",
    "    Output: Tab in the preferred web browser.\n",
    "    \"\"\"\n",
    "    query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.url) AND m.liteId = $lite_id RETURN m.url AS url\"\n",
    "    if doc_insight.get() == 0:\n",
    "        if(doc_var_idx.get() != no_article):\n",
    "            url = graph.run(query, parameters={'lite_id': doc_var_idx.get()}).data()\n",
    "        else:\n",
    "            output_screen['text'] = \"You must select a document first.\"\n",
    "    else:\n",
    "        if(doc_sim_idx.get() != no_article):\n",
    "            url = graph.run(query, parameters={'lite_id': doc_sim_idx.get()}).data()\n",
    "        else:\n",
    "            output_screen['text'] = \"You must select a document first.\"\n",
    "    if url:\n",
    "        output_screen['text'] = \"\"\n",
    "        webbrowser.open_new_tab(url[0]['url'][0])\n",
    "    else:\n",
    "        output_screen['text'] = \"You must select a document first.\"\n",
    "    \n",
    "#Feature: Using the search bar for articles\n",
    "def search_documents():\n",
    "    \"\"\"\n",
    "    Purpose: Queries the articles in the database that match the words in the search bar.\n",
    "    Input: None. The input search is done in the GUI (accessible variable).\n",
    "    Output: List of articles returned by the search, in the left column of the GUI.\n",
    "    Note: The matching words are only applied by the title.\n",
    "          The query varies depending on the words and filters used in the GUI.\n",
    "    \"\"\"\n",
    "    global LIST_DOCUMENTS, LIST_SIM_DOCS, LIST_NOT_FOUND_LABELS, LIST_OF_VIEWERS, VIEW_DOCUMENT, entry_title, doc_var_idx, doc_sim_idx\n",
    "    \n",
    "    for radiobutton in LIST_DOCUMENTS:\n",
    "        radiobutton.destroy()\n",
    "        \n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    \n",
    "    for label in LIST_NOT_FOUND_LABELS:\n",
    "        label.destroy()\n",
    "    \n",
    "    LIST_DOCUMENTS = []\n",
    "    LIST_SIM_DOCS = []\n",
    "    LIST_NOT_FOUND_LABELS = []\n",
    "    title_search = str(entry_title.get()).replace('\"', '')\n",
    "    title_search = title_search.replace(\"'\",\"\")\n",
    "    words = title_search.split()\n",
    "    body_query = \"\"\n",
    "    for word in words:\n",
    "        body_query += \" AND m.title =~ '(?i).*\" + word.lower() + \".*'\"\n",
    "    footer_query = \" RETURN m.title AS title, m.liteId AS LID ORDER BY m.liteId ASC\"\n",
    "    \n",
    "    if(typevar.get() == TYPES[0]):\n",
    "        header_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.content)\"\n",
    "    else:\n",
    "        missmatch = False\n",
    "        if(typevar.get() == TYPES[1]):\n",
    "            header_query = \"MATCH (m:Article:_AI)-[]-(n:HorizonScanningArea:_AI) WHERE EXISTS(m.content)\"\n",
    "        elif(typevar.get() == TYPES[2]):\n",
    "            header_query = \"MATCH (m:Article:_AI)-[]-(n:LtsFocusArea:_AI) WHERE EXISTS(m.content)\"\n",
    "        elif(typevar.get() == TYPES[3]):\n",
    "            header_query = \"MATCH (m:Article:_AI)-[]-(n:Megatrend:_AI) WHERE EXISTS(m.content)\"\n",
    "        else:\n",
    "            tk.messagebox.showerror(\"Error: Name Missmatch\", \"The names appearing do not correspond to the type of nodes in the database.\\nPlease check the names in code.\\nNo filters will be applied in this case.\")\n",
    "            header_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.content)\"\n",
    "            missmatch = True\n",
    "        if(not missmatch):\n",
    "            body_query += \" AND n.name =~ '(?i)\" + catvar.get()+ \"'\"\n",
    "    search_query = graph.run(header_query + body_query + footer_query).data()\n",
    "    entry_title.delete(0,'end')\n",
    "    if(not search_query):\n",
    "        blank_article = tk.Label(left_dlistframe, text=\"(Article not found)\", bg='white')\n",
    "        blank_article.config(font=(FONT_NOT_FOUND, 9), width=310)\n",
    "        blank_article.pack(anchor='nw', fill='x')\n",
    "        LIST_NOT_FOUND_LABELS.append(blank_article)\n",
    "    else:\n",
    "        for idd,doc in enumerate(search_query):\n",
    "            r = tk.Radiobutton(left_dlistframe, text=doc['title'], selectcolor='#e6f2ff', wraplength=255, bg='white', relief='ridge', overrelief='ridge', indicatoron=False, variable=doc_var_idx, value=doc['LID'])\n",
    "            r.config(font=(FONT_ARTICLES, 9))\n",
    "            r.pack(anchor = 'w', fill='x')\n",
    "            LIST_DOCUMENTS.append(r)\n",
    "            \n",
    "    left_dlistcanvas.update_idletasks()\n",
    "    doc_var_idx.set(no_article)\n",
    "    left_dlistcanvas.configure(scrollregion=left_dlistcanvas.bbox('all'))\n",
    "    \n",
    "    blank_simarticle = tk.Label(right_dlistframe, text=\"\", bg='white')\n",
    "    blank_simarticle.config(font=(FONT_NOT_FOUND, 9), width=310)\n",
    "    blank_simarticle.pack(anchor='nw', fill='x')\n",
    "    LIST_NOT_FOUND_LABELS.append(blank_simarticle)\n",
    "    \n",
    "    right_dlistcanvas.update_idletasks()\n",
    "    doc_sim_idx.set(no_article)\n",
    "    right_dlistcanvas.configure(scrollregion=right_dlistcanvas.bbox('all'))\n",
    "    \n",
    "    if VIEW_DOCUMENT:\n",
    "        change_view_button['text'] = 'Preview'\n",
    "        for element in LIST_OF_VIEWERS:\n",
    "            element.destroy()\n",
    "        LIST_OF_VIEWERS = []\n",
    "        output_screen['text'] = \"\"\n",
    "        VIEW_DOCUMENT = False\n",
    "\n",
    "def change_view():\n",
    "    \"\"\"\n",
    "    Purpose: Provide a pre-visualization for articles in the GUI. Highlights the predicted key-words for the article.\n",
    "    Input: None. The article is selected in the GUI, and the LiteID accessible from this function.\n",
    "    Output: None. A visualization window is opened in the GUI. Close it by clicking again in the button in the GUI.\n",
    "    \"\"\"\n",
    "    global LIST_OF_VIEWERS, VIEW_DOCUMENT\n",
    "    if(doc_var_idx.get() != no_article): #Makes sure that an article has been selected\n",
    "        if(VIEW_DOCUMENT == False):\n",
    "            if search_query:\n",
    "                #if document selected\n",
    "                change_view_button['text'] = 'Close Preview'\n",
    "                vquery = \"MATCH (m:Article:_AI) WHERE EXISTS(m.content) AND m.liteId = $lite_id RETURN m.title AS title, m.content AS content, m.keywords_viewer_nst AS keywordsview\"\n",
    "                graph_query = graph.run(vquery, parameters={'lite_id': doc_var_idx.get()}).data()\n",
    "                nliteid_found = len(graph_query)\n",
    "                if (nliteid_found > 1):\n",
    "                    print(\"Wait, something is wrong with your LiteIDs. There should be a unique one per document.\")\n",
    "                    print(\"A have found %d documents with the LiteId: %d\" % (nliteid_found, doc_var_idx.get()))\n",
    "                title = 'Title: ' + graph_query[0]['title'] + '\\n'\n",
    "                text = clean_spacelines(extract_markups(html.unescape(graph_query[0]['content'])))\n",
    "                text = cleanClipText(text)\n",
    "                keywords = graph_query[0]['keywordsview']\n",
    "                viewer = tk.Text(inner_dwn_frame_b, bg='white', wrap='word', padx=20)\n",
    "                viewer.insert(tk.INSERT, title)\n",
    "                viewer.insert(tk.INSERT,text)\n",
    "                search_list = []\n",
    "                for keyword in keywords:\n",
    "                    if(len(keyword) > 2): search_list.append(keyword)\n",
    "                    search_list.append(keyword.capitalize()) #look for capitalized version of the words\n",
    "                    search_list.append(keyword.upper()) #look for upper-cased version of the words\n",
    "                for keyword in search_list:\n",
    "                    start = 1.0\n",
    "                    long = len(keyword)\n",
    "                    while True:\n",
    "                        pos = viewer.search(keyword, start, stopindex='end') #this is CASE-SENSITIVE\n",
    "                        if pos == \"\": break\n",
    "                        viewer.tag_add(\"here\", pos, pos+\"+%dc\" % (long))\n",
    "                        start = pos+\"+%dc\" % (long)\n",
    "                viewer.tag_config(\"here\", background=\"yellow\", foreground=\"blue\")\n",
    "                viewer.place(anchor='nw', relx=0, rely=0, relwidth=1, relheight=1)\n",
    "                LIST_OF_VIEWERS.append(viewer)\n",
    "                output_screen['text'] = \"You're previewing the text.\\n Highlighted you will find its keywords.\"\n",
    "        else:\n",
    "            change_view_button['text'] = 'Preview'\n",
    "            for element in LIST_OF_VIEWERS:\n",
    "                element.destroy()\n",
    "            LIST_OF_VIEWERS = []\n",
    "            output_screen['text'] = \"\"\n",
    "        VIEW_DOCUMENT = not VIEW_DOCUMENT\n",
    "    else:\n",
    "        output_screen['text'] = 'Select a document to view it.'\n",
    "    \n",
    "def sort_categories():\n",
    "    #Changes menu options in the GUI depending on the filter chosen (LtsFocusAreas, Megatrends, HScanning, None)\n",
    "    if(typevar.get() == TYPES[0]):\n",
    "        CATEGORIES = [\n",
    "        \"None\"\n",
    "        ]\n",
    "    elif(typevar.get() == TYPES[1]):\n",
    "        CATEGORIES = [\n",
    "        \"Business And Economy\",\n",
    "        \"Environment And Resources\",\n",
    "        \"Politics And Law\",\n",
    "        \"Society And Individuals\",\n",
    "        \"Technologies And Innovation\",\n",
    "        \"Miscellaneous\"\n",
    "        ]\n",
    "    elif(typevar.get() == TYPES[2]):\n",
    "        CATEGORIES = [\n",
    "        \"Autonomous Drive\",\n",
    "        \"Collaboration\",\n",
    "        \"Continuous Learning\",\n",
    "        \"Cyber Security\",\n",
    "        \"Data And Intelligence\",\n",
    "        \"Fleet Operators\",\n",
    "        \"Handling Complexity\",\n",
    "        \"Mobility Infrastructure\",\n",
    "        \"Playable Platform\",\n",
    "        \"Services\",\n",
    "        \"Urban Mobility\",\n",
    "        \"UX And Interactions\"\n",
    "        ]\n",
    "    elif(typevar.get() == TYPES[3]):\n",
    "        CATEGORIES = [\n",
    "        \"Demographic Changes\",\n",
    "        \"Diffusion of Power\",\n",
    "        \"Economic Growth\",\n",
    "        \"Globalization\",\n",
    "        \"Health And Well-being\",\n",
    "        \"Immaterialization\",\n",
    "        \"Individualization\",\n",
    "        \"Knowledge Society\",\n",
    "        \"Sustainability\",\n",
    "        \"Technology Development\"\n",
    "        ]\n",
    "    catvar.set(CATEGORIES[0])\n",
    "    \n",
    "    #Clean the categories menu\n",
    "    localmenu = catmenu[\"menu\"]\n",
    "    localmenu.delete(0,\"end\")\n",
    "    \n",
    "    for cat in CATEGORIES:\n",
    "        localmenu.add_command(label=cat, command=lambda value=cat: catvar.set(value))\n",
    "\n",
    "###########################\n",
    "##  HERE STARTS THE GUI  ##\n",
    "## --------------------  ##\n",
    "###########################\n",
    "\n",
    "canvas = tk.Canvas(root, height= HEIGHT, width= WIDTH)\n",
    "\n",
    "#Header\n",
    "header_frame = tk.Frame(root, bd=5, bg='#d9d9d9')\n",
    "header_frame.place(relx=0.5, rely=0, anchor='n', relwidth=1, relheight=0.1)\n",
    "title_h = tk.Label(header_frame, text=\"AI Applied to Knowledge Graphs\") #Title\n",
    "title_h.config(font=(\"Volvo Broad Pro\", 19))\n",
    "title_h.place(anchor='n', relx=0.5, rely=0, relwidth=0.6, relheight=0.5)\n",
    "title_s = tk.Label(header_frame, text=\"Horizon Scanning AI\") #Sub-title\n",
    "title_s.config(font=(\"Volvo Broad Pro\", 13))\n",
    "title_s.place(anchor='n', relx=0.5, rely=0.5, relwidth=0.6, relheight=0.5)\n",
    "insightlab_image = ImageTk.PhotoImage(Image.open('./img/insightlab_logo.png').resize((72,69)), master=root)\n",
    "insightlab_label = tk.Label(header_frame, image=insightlab_image) #Insight Lab logo\n",
    "insightlab_label.bind(\"<Triple-Button-3>\", insightlabcallback)\n",
    "insightlab_label.place(anchor='nw', relx=0.8, rely=0, relwidth=0.2, relheight=1)\n",
    "volvo_image = ImageTk.PhotoImage(Image.open('./img/volvo_logo2.png').resize((75,75)), master=root)\n",
    "volvo_label = tk.Label(header_frame, image=volvo_image) #Volvo Cars logo\n",
    "volvo_label.place(anchor='nw', relx=0, rely=0, relwidth=0.2, relheight=1)\n",
    "\n",
    "#Body\n",
    "body_frame = tk.Frame(root)\n",
    "body_frame.place(relx=0.5, rely=0.1, anchor='n', relwidth=1, relheight=0.77)\n",
    "\n",
    "# Body-header\n",
    "inner_upp_frame_b = tk.Frame(body_frame, bd=3)\n",
    "inner_upp_frame_b.place(anchor='nw', relx=0, rely=0, relwidth=1, relheight=0.1)\n",
    "inner_upp_left_frame = tk.Frame(inner_upp_frame_b)\n",
    "inner_upp_left_frame.place(anchor='nw', relx=0, rely=0, relwidth=0.2, relheight=1)\n",
    "inner_upp_center_frame = tk.Frame(inner_upp_frame_b)\n",
    "inner_upp_center_frame.place(anchor='nw', relx=0.2, rely=0, relwidth=0.50, relheight=1)\n",
    "inner_upp_right_frame = tk.Frame(inner_upp_frame_b)\n",
    "inner_upp_right_frame.place(anchor='nw', relx=0.7, rely=0, relwidth=0.3, relheight=1)\n",
    "\n",
    "lb_search = tk.Label(inner_upp_left_frame, text=\"Search by:\")\n",
    "lb_search.config(font=(TEXT_FONT, 10))\n",
    "lb_search.place(anchor='n', relx=0.5, rely=0.04, relwidth=0.92, relheight=0.44)\n",
    "\n",
    "lb_title = tk.Label(inner_upp_center_frame, text=\"Title\")\n",
    "lb_title.config(font=(TEXT_FONT, 9))\n",
    "lb_title.place(anchor='nw', relx=0.03, rely=0.04, relwidth=0.31, relheight=0.44)\n",
    "\n",
    "entry_title = tk.Entry(inner_upp_center_frame, justify='center') #Search bar\n",
    "entry_title.config(font=(TEXT_FONT, 9))\n",
    "entry_title.place(anchor='ne', relx=0.97, rely=0.04, relwidth=0.60, relheight=0.44)\n",
    "\n",
    "lb_type = tk.Label(inner_upp_right_frame, text=\"Type\")\n",
    "lb_type.config(font=(TEXT_FONT, 9))\n",
    "lb_type.place(anchor='nw', relx=0.04, rely=0.04, relwidth=0.3, relheight=0.44)\n",
    "\n",
    "TYPES = [\n",
    "\"None\",\n",
    "\"H. Scanning\",\n",
    "\"Lts Focus Area\",\n",
    "\"Megatrend\"\n",
    "]\n",
    "\n",
    "typevar = tk.StringVar(inner_upp_right_frame)\n",
    "typevar.set(TYPES[0]) # default value\n",
    "\n",
    "typemenu = tk.OptionMenu(inner_upp_right_frame, typevar, *TYPES, command=lambda e: sort_categories()) #Filter (Focus Areas, Megatrend, etc.)\n",
    "typemenu.config(font=(TEXT_FONT,9))\n",
    "typemenu.place(anchor='ne', relx=0.96, rely=0.04, relwidth=0.6, relheight=0.44)\n",
    "\n",
    "search_button = tk.Button(inner_upp_left_frame, text=\"Search\")\n",
    "search_button.config(font=(TEXT_FONT, 10))\n",
    "search_button.place(anchor='n', relx=0.5, rely=0.56, relwidth=0.6, relheight=0.40)\n",
    "\n",
    "change_view_button = tk.Button(inner_upp_right_frame, text=\"Preview\", command=lambda: change_view()) #Pre-visualization button\n",
    "change_view_button.config(font=(TEXT_FONT, 10))\n",
    "change_view_button.place(anchor='ne', relx=0.96, rely=0.56, relwidth=0.6, relheight=0.40)\n",
    "\n",
    "lb_cat = tk.Label(inner_upp_center_frame, text=\"Categories\")\n",
    "lb_cat.config(font=(TEXT_FONT, 9))\n",
    "lb_cat.place(anchor='nw', relx=0.03, rely=0.52, relwidth=0.31, relheight=0.44)\n",
    "\n",
    "catvar = tk.StringVar(inner_upp_center_frame)\n",
    "catvar.set(\"None\") # default value\n",
    "\n",
    "catmenu = tk.OptionMenu(inner_upp_center_frame, catvar, \"None\") #Categories existing under the filter\n",
    "catmenu.config(font=(TEXT_FONT,9))\n",
    "catmenu.place(anchor='ne', relx=0.97, rely=0.52, relwidth=0.6, relheight=0.44)\n",
    "\n",
    "# Body-main\n",
    "inner_dwn_frame_b = tk.Frame(body_frame)\n",
    "inner_dwn_frame_b.place(anchor='nw', relx=0, rely=0.1, relwidth=1, relheight=0.9)\n",
    "\n",
    "infoinsight_image = ImageTk.PhotoImage(Image.open('./img/info_insight.png').resize((69,40)), master=root)\n",
    "infoinsight_label = tk.Label(inner_dwn_frame_b, image=infoinsight_image) #Logo button: \"Give me some insight!\"\n",
    "infoinsight_label.bind(\"<Button-1>\", insightinfocallback)\n",
    "infoinsight_label.place(anchor='n', relx=0.5, rely=0.90, relwidth=0.25, relheight=0.1)\n",
    "\n",
    "\n",
    "#Footer\n",
    "footer_frame = tk.Frame(root, bd=8)\n",
    "footer_frame.place(relx=0.5, rely=0.87, anchor='n', relwidth=1, relheight=0.11)\n",
    "output_screen = tk.Label(footer_frame, justify='left', bg=\"white\", text=\"Welcome to the application.\") # GUI screen for messages and information\n",
    "output_screen.place(relx=0.025, rely=0.5, anchor='w', relheight=1, relwidth=0.48)\n",
    "inner_frame_f = tk.Frame(footer_frame)\n",
    "inner_frame_f.place(relx=0.975, rely=1, anchor='se', relwidth=0.27, relheight=0.8)\n",
    "fcc_button = tk.Button(inner_frame_f, text=\"Find Closest\") #Button for executing the search for closest documents\n",
    "fcc_button.config(font=(TEXT_FONT, 10))\n",
    "fcc_button.place(anchor='nw', relx=0, rely=0, relwidth=0.55, relheight=0.4)\n",
    "entry_docs = tk.Entry(inner_frame_f, justify='right') #Defines the amount of documents to be returned\n",
    "entry_docs.place(anchor='nw', relx=0.6, rely=0, relwidth=0.4, relheight=0.4)\n",
    "\n",
    "LST_features = []\n",
    "\n",
    "#Dropdown menu\n",
    "ALGORITHMS = [\n",
    "\"Word Embeddings\",\n",
    "\"TF-IDF\",\n",
    "#\"Doc2vec\",\n",
    "\"LSA\",\n",
    "\"LDA\",\n",
    "\"Ensemble Method\",\n",
    "\"Community Finding\"\n",
    "]\n",
    "\n",
    "algorithmvariable = tk.StringVar(inner_frame_f)\n",
    "algorithmvariable.set(ALGORITHMS[0]) # default value\n",
    "\n",
    "algorithmenu = tk.OptionMenu(inner_frame_f, algorithmvariable, *ALGORITHMS) #Selection of the algorithm to use!\n",
    "algorithmenu.config(font=(\"Volvo Broad Pro\",11))\n",
    "algorithmenu.place(anchor=\"se\", relx=1, rely=1, relwidth=1, relheight=0.5)\n",
    "\n",
    "canvas.pack()\n",
    "\n",
    "#Initialization of variables, load models, initialize the py2neo v4 Neo4j backend, checks the status of the database\n",
    "try:\n",
    "    #Connect to Neo4j Database\n",
    "    graph = Graph(auth=('user','password'), host=\"gotsvl1706.got.volvocars.net\", port=7687, secure=True)\n",
    "    exists_database = graph.run(\"MATCH (n:Article:_AI) RETURN n LIMIT 1\").data() #Checks if there is something in the database or is empty\n",
    "except Exception as e:\n",
    "    #This happens if there is no connection with the database, normally\n",
    "    tk.messagebox.showwarning(\"Warning: No Connection\", \"Warning: No connection with Neo4j. Please, make sure that Neo4j is running and you have entered the correct password.\")\n",
    "    root.destroy() #This destroys the application (close it)\n",
    "else:\n",
    "    if not exists_database: #If the connection has been made, but the database is empty (it will create one)\n",
    "        tk.messagebox.showinfo(\"Info\",\"Info: The Knowledge Graph database is empty.\\nWe will need to prepare it for you.\\nThis may take several minutes.\")\n",
    "        # Load Neo4j JSON\n",
    "        try:\n",
    "            load_articlesNeo4j() # Load JSON file from the import folder\n",
    "            preprocess_articlesNeo4j() # Pre-filter the documents imported\n",
    "            process_documentsNeo4j() # Process the content and extract significant words\n",
    "            clean_empty_processed_docs() #Post-filter the documents that have been processed\n",
    "            create_LiteId_documents() #This has to be done only once\n",
    "        except Exception as e:\n",
    "            print(\"Something went wrong loading the articles into Neo4j.\\nCheck that the JSON file is in the import folder and try again.\")\n",
    "            print(\"If the problem persists, check the code functions.\")\n",
    "        try:\n",
    "            process_wordembeddingsNeo4j() #OBS!: Only do this if you don't have GloVe in Neo4j and you want to.\n",
    "        except Exception as e:\n",
    "            print(\"Something went wrong loading GloVe embeddings into Neo4j.\\nCheck that the file exists in the import folder and try again.\")\n",
    "            print(\"If the problem persists, check the code function, the format and dimensions of the CSV file.\")\n",
    "    check_coherence = check_documents()\n",
    "    if check_coherence:\n",
    "        # Load/Create the corpus\n",
    "        # Note: All this strings should maybe appear in the GUI\n",
    "        corpus_memory_friendly = MyCorpusDashNeo()\n",
    "        corpus_memory_friendly_NST = MyCorpusNeoNST()\n",
    "        #(Careful, these are right now generators, or they will yield those)\n",
    "        #Some functions might be deprecated for generators in the future.\n",
    "        \n",
    "        #TF-IDF Load\n",
    "        if not os.path.isfile(index_tfidf_name):\n",
    "            # Create the TF-IDF model\n",
    "            tfidf = models.TfidfModel((bow for bow in corpus_memory_friendly), normalize=True)\n",
    "            compare_docs_query = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) RETURN m.preprocessed_stemmed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "            # Note: Tests can be done in in-memory batches for large datasets\n",
    "            compare_docs_bow = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in compare_docs_query]\n",
    "            compare_tfidf = tfidf[compare_docs_bow]\n",
    "            index_tfidf = similarities.Similarity(output_prefix=\"sim_tfidf_idx\", corpus=compare_tfidf, num_features=len(corpus_memory_friendly.dictionary))\n",
    "            tfidf.save(index_tfidf_model_name)\n",
    "            index_tfidf.save(index_tfidf_name)\n",
    "        \n",
    "        #TF-IDF Load (NST-Version) (Non-stemmed)\n",
    "        if not os.path.isfile(index_tfidf_name_nst):\n",
    "            # (This will be Feedly dashboards)\n",
    "            tfidf_nst = models.TfidfModel((bow for bow in corpus_memory_friendly_NST), normalize=True)\n",
    "            compare_docs_query_nst = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.preprocessed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "            # Note: Tests can be done in in-memory batches for large datasets !\n",
    "            compare_docs_bow_nst = [corpus_memory_friendly_NST.dictionary.doc2bow(doc['preprocessed']) for doc in compare_docs_query_nst]\n",
    "            compare_tfidf_nst = tfidf_nst[compare_docs_bow_nst]\n",
    "            index_tfidf_nst = similarities.Similarity(output_prefix=\"sim_tfidf_idx_nst\", corpus=compare_tfidf_nst, num_features=len(corpus_memory_friendly_NST.dictionary))\n",
    "            # will the length of features be same if we include the entire Feedly? (Nope)\n",
    "            tfidf_nst.save(index_tfidf_model_name_nst)\n",
    "            index_tfidf_nst.save(index_tfidf_name_nst)\n",
    "        stemmed_tfidf = True #This variable defines whether to use the stemmed version or the nst-version in the application\n",
    "        try:\n",
    "            tfidf = models.TfidfModel.load(index_tfidf_model_name)\n",
    "            index_tfidf = similarities.Similarity.load(index_tfidf_name)\n",
    "            tfidf_nst = models.TfidfModel.load(index_tfidf_model_name_nst)\n",
    "            index_tfidf_nst = similarities.Similarity.load(index_tfidf_name_nst)\n",
    "        except RuntimeError:\n",
    "            print(\"Something went wrong. Please check that the TF-IDF index exists.\")\n",
    "        evaluate_keywordsNeo4j()    #predict and extract the key-words from the articles (stem version)\n",
    "        evaluate_keywordsNeo4jNST() #predict and extract the key-words from the articles (nst-version)\n",
    "        \n",
    "        #Latent Semantic Analysis (LSA) Load\n",
    "        if not os.path.isfile(index_lsi_name):\n",
    "            # (This will be Feedly dashboards)\n",
    "            lsi = models.LsiModel(corpus_memory_friendly, id2word=corpus_memory_friendly.dictionary, num_topics=37) #to tune (49 prev)\n",
    "            compare_docs_query = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) RETURN m.preprocessed_stemmed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "            # Note: Tests can be done in in-memory batches for large datasets !\n",
    "            compare_docs_bow = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in compare_docs_query]\n",
    "            compare_lsi = lsi[compare_docs_bow]\n",
    "            index_lsi = similarities.Similarity(output_prefix=\"sim_lsi_idx\", corpus=compare_lsi, num_features=len(corpus_memory_friendly.dictionary))\n",
    "            lsi.save(index_lsi_model_name)\n",
    "            index_lsi.save(index_lsi_name)\n",
    "        \n",
    "        #Latent Semantic Analysis (LSA) Load (NST-Version)\n",
    "        if not os.path.isfile(index_lsi_name_nst):\n",
    "            lsi_nst = models.LsiModel(corpus_memory_friendly_NST, id2word=corpus_memory_friendly_NST.dictionary, num_topics=40) #to tune (49 prev)\n",
    "            compare_docs_query_nst = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.preprocessed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "            # Note: Tests can be done in in-memory batches for large datasets !\n",
    "            compare_docs_bow_nst = [corpus_memory_friendly_NST.dictionary.doc2bow(doc['preprocessed']) for doc in compare_docs_query_nst]\n",
    "            compare_lsi_nst = lsi_nst[compare_docs_bow_nst]\n",
    "            index_lsi_nst = similarities.Similarity(output_prefix=\"sim_lsi_nst_idx\", corpus=compare_lsi_nst, num_features=len(corpus_memory_friendly_NST.dictionary))\n",
    "            lsi_nst.save(index_lsi_model_name_nst)\n",
    "            index_lsi_nst.save(index_lsi_name_nst)\n",
    "        try:\n",
    "            lsi = models.LsiModel.load(index_lsi_model_name)\n",
    "            index_lsi = similarities.Similarity.load(index_lsi_name)\n",
    "            lsi_nst = models.LsiModel.load(index_lsi_model_name_nst)\n",
    "            index_lsi_nst = similarities.Similarity.load(index_lsi_name_nst)\n",
    "        except RuntimeError:\n",
    "            print(\"Something went wrong. Please check that the LSI index exists.\")\n",
    "        \n",
    "        #Latent Dirichlet Allocation (LDA) Load\n",
    "        if not os.path.isfile(index_lda_name):\n",
    "            # (This will be Feedly dashboards)\n",
    "            lda = models.LdaModel(corpus_memory_friendly, id2word=corpus_memory_friendly.dictionary, num_topics=12, passes=12, alpha='auto') #to tune, eval_every=5\n",
    "            ldacompare_docs_query = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) RETURN m.preprocessed_stemmed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "            # Note: Training can be done online for large datasets !\n",
    "            compare_docs_bow = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in ldacompare_docs_query]\n",
    "            compare_lda = lda[compare_docs_bow]\n",
    "            index_lda = similarities.Similarity(output_prefix=\"sim_lda_idx\", corpus=compare_lda, num_features=len(corpus_memory_friendly.dictionary))\n",
    "            # will the length of features be same if we include the entire Feedly?\n",
    "            lda.save(index_lda_model_name)\n",
    "            index_lda.save(index_lda_name)\n",
    "        \n",
    "        #Latent Dirichlet Allocation (LDA) Load (NST-Version)\n",
    "        if not os.path.isfile(index_lda_name_nst):\n",
    "            # (This will be Feedly dashboards)\n",
    "            lda_nst = models.LdaModel(corpus_memory_friendly_NST, id2word=corpus_memory_friendly_NST.dictionary, num_topics=12, passes=10, alpha='auto') #to tune, eval_every=5\n",
    "            ldacompare_docs_query = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.preprocessed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "            # Note: Training can be done online for large datasets !\n",
    "            compare_docs_bow = [corpus_memory_friendly_NST.dictionary.doc2bow(doc['preprocessed']) for doc in ldacompare_docs_query]\n",
    "            compare_lda = lda_nst[compare_docs_bow]\n",
    "            index_lda_nst = similarities.Similarity(output_prefix=\"sim_lda_idx\", corpus=compare_lda, num_features=len(corpus_memory_friendly_NST.dictionary))\n",
    "            # will the length of features be same if we include the entire Feedly?\n",
    "            lda_nst.save(index_lda_model_name_nst)\n",
    "            index_lda_nst.save(index_lda_name_nst)\n",
    "        try:\n",
    "            #lda = models.LdaModel.load(os.path.join(dir_name, \"LDA_MAX.model\"))\n",
    "            #index_lda = similarities.Similarity.load(os.path.join(dir_name, \"LDA_MAX_INDEX.index\"))\n",
    "            lda = models.LdaModel.load(index_lda_model_name)\n",
    "            index_lda = similarities.Similarity.load(index_lda_name)\n",
    "            lda_nst = models.LdaModel.load(index_lda_model_name_nst)\n",
    "            index_lda_nst = similarities.Similarity.load(index_lda_name_nst)\n",
    "        except RuntimeError:\n",
    "            print(\"Something went wrong. Please check that the LSI index exists.\")\n",
    "            \n",
    "        # List-of-documents for the first visualization and Welcome to the application's GUI\n",
    "        left_dlist = tk.Frame(inner_dwn_frame_b, bg='white')\n",
    "        left_dlist.place(anchor='nw', relx=0.05, rely=0.05, relwidth=0.43, relheight=0.85)\n",
    "        left_dlistcanvas = tk.Canvas(left_dlist, bg='white')\n",
    "        left_scrollbar = tk.Scrollbar(left_dlist, orient='vertical')\n",
    "        left_dlistframe = tk.Frame(left_dlistcanvas)\n",
    "        window = left_dlistcanvas.create_window(0, 0, anchor='nw', window=left_dlistframe, width=314)\n",
    "        left_scrollbar.pack(fill='y', side='right')\n",
    "        LIST_OF_VIEWERS = []\n",
    "        doc_insight = tk.IntVar() #for the Insight! functionality\n",
    "        doc_insight.set(0) #set query document as pre-defined\n",
    "        doc_var_idx = tk.IntVar() #to track the selected document in the left column\n",
    "        LIST_DOCUMENTS = []\n",
    "        LIST_NOT_FOUND_LABELS = []\n",
    "        header_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.content) RETURN m.title AS title, m.liteId AS LID ORDER BY m.liteId ASC\"\n",
    "        search_query = graph.run(header_query).data()\n",
    "        for idd,doc in enumerate(search_query):\n",
    "            r = tk.Radiobutton(left_dlistframe, text=doc['title'], selectcolor='#e6f2ff', wraplength=255, bg='white', relief='ridge', overrelief='ridge', indicatoron=False, variable=doc_var_idx, value=doc['LID'])\n",
    "            r.config(font=(FONT_ARTICLES, 9))\n",
    "            r.pack(anchor = 'w', fill='x')\n",
    "            LIST_DOCUMENTS.append(r)\n",
    "        left_dlistcanvas.update_idletasks()\n",
    "        doc_var_idx.set(no_article)\n",
    "        left_scrollbar.config(command=left_dlistcanvas.yview)\n",
    "        left_dlistcanvas.configure(scrollregion=left_dlistcanvas.bbox('all'), yscrollcommand=left_scrollbar.set)\n",
    "        left_dlistcanvas.pack(fill='both', side='left')\n",
    "        left_dlistcanvas.bind('<Enter>', _bound_to_mousewheel)\n",
    "        left_dlistcanvas.bind('<Leave>', _unbound_to_mousewheel)\n",
    "\n",
    "        doc_sim_idx = tk.IntVar()\n",
    "        LIST_SIM_DOCS = []\n",
    "\n",
    "        # List of similar documents\n",
    "        right_dlist = tk.Frame(inner_dwn_frame_b, bg='white')\n",
    "        right_dlist.place(anchor='ne', relx=0.95, rely=0.05, relwidth=0.43, relheight=0.85)\n",
    "        right_dlistcanvas = tk.Canvas(right_dlist, bg='white')\n",
    "        right_scrollbar = tk.Scrollbar(right_dlist, orient='vertical')\n",
    "        right_dlistframe = tk.Frame(right_dlistcanvas)\n",
    "        rwindow = right_dlistcanvas.create_window(0, 0, anchor='nw', window=right_dlistframe, width=314)\n",
    "        right_scrollbar.pack(fill='y', side='right')\n",
    "        doc_sim_idx.set(no_article)\n",
    "        right_scrollbar.config(command=right_dlistcanvas.yview)\n",
    "        right_dlistcanvas.configure(scrollregion=right_dlistcanvas.bbox('all'), yscrollcommand=right_scrollbar.set)\n",
    "        right_dlistcanvas.pack(fill='both', side='left')\n",
    "        right_dlistcanvas.bind('<Enter>', _bound_to_mousewheelsim)\n",
    "        right_dlistcanvas.bind('<Leave>', _unbound_to_mousewheelsim)\n",
    "        \n",
    "        search_button.config(command=lambda : search_documents())\n",
    "        fcc_button.config(command=lambda: find_similardocuments())\n",
    "        entry_title.bind(\"<Return>\", keyentertitlecallback)\n",
    "        entry_docs.bind(\"<Return>\", keyenterdocsamountcallback)\n",
    "    else:\n",
    "        tk.messagebox.showerror(\"Error\", \"Error: The Graph database is not coherent. The LiteID do not match with the articles. Please, check Neo4j.\")\n",
    "        delete_database = tk.messagebox.askyesno(\"\",\"Do you want to delete the database before exiting?\\nNext time you open, it will be created again.\")\n",
    "        if delete_database:\n",
    "            try:\n",
    "                graph.run(\"MATCH (n:_AI) WHERE NOT '_GlobalConfigurationControl' IN labels(n) DETACH DELETE n\")\n",
    "                for the_file in os.listdir(dir_name):\n",
    "                    file_path = os.path.join(dir_name, the_file)\n",
    "                    if os.path.isfile(file_path):\n",
    "                        os.unlink(file_path)\n",
    "            except Exception as e:\n",
    "                tk.messagebox.showinfo(\"\", \"Couldn't delete the database. The error is:\\n\" + e)\n",
    "            else:\n",
    "                tk.messagebox.showinfo(\"\", \"Database deleted.\")\n",
    "        root.destroy()\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUTURE UPDATE FUNCTION\n",
    "def update_database_Neo4j():\n",
    "    #TODO: API Request JSON, do the middle step conversion, load merge new documents, clean the ones without content, preprocess them, update the tf-idf model and the dictionary, assign keywords, assign LiteId to them \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check in the dictionary for weird or composed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food-delivery\n",
      "real-time\n",
      "ride-sharing\n",
      "web-based\n",
      "24-hour\n",
      "non-profit\n",
      "all-electric\n",
      "car-sharing\n",
      "electric-vehicle\n",
      "on-demand\n",
      "ride-hailing\n",
      "self-driving\n",
      "would-be\n",
      "double-digit\n",
      "human-driven\n",
      "pedal-assist\n",
      "second-class\n",
      "autonomous-driving\n",
      "e-hailing\n",
      "e-scooter\n",
      "e-scooters\n",
      "first-\n",
      "last-mile\n",
      "real-world\n",
      "start-ups\n",
      "bike-share\n",
      "car-free\n",
      "brick-and-mortar\n",
      "e-commerce\n",
      "custom-made\n",
      "high-quality\n",
      "dark-skinned\n",
      "data-driven\n",
      "low-income\n",
      "machine-learning\n",
      "end-to-end\n",
      "all-time\n",
      "blockchain-based\n",
      "cross-border\n",
      "peer-to-peer\n",
      "state-backed\n",
      "third-party\n",
      "two-day\n",
      "well-known\n",
      "x-ray\n",
      "re-identification\n",
      "time-consuming\n",
      "wi-fi\n",
      "electric-powered\n",
      "knowledge-based\n",
      "open-source\n",
      "--\n",
      "one-quarter\n",
      "socio-economic\n",
      "uk-based\n",
      "us-based\n",
      "well-being\n",
      "i-pace\n",
      "small-scale\n",
      "e-bikes\n",
      "five-year\n",
      "mid-2000s\n",
      "e-mail\n",
      "e-mailed\n",
      "e-mails\n",
      "gene-edited\n",
      "gene-editing\n",
      "high-profile\n",
      "at-home\n",
      "off-the-shelf\n",
      "29-year-old\n",
      "english-language\n",
      "far-fetched\n",
      "gene-therapy\n",
      "know-how\n",
      "in-app\n",
      "one-stop\n",
      "cedars-sinai\n",
      "co-founder\n",
      "consumer-facing\n",
      "early-stage\n",
      "record-keeping\n",
      "seattle-based\n",
      "ai-assisted\n",
      "two-thirds\n",
      "multi-year\n",
      "ai-powered\n",
      "deep-learning\n",
      "start-up\n",
      "facial-recognition\n",
      "horse-drawn\n",
      "mercedes-benz\n",
      "sci-fi\n",
      "business-as-usual\n",
      "cost-effective\n",
      "economy-wide\n",
      "low-carbon\n",
      "on-site\n",
      "per-capita\n",
      "public-sector\n",
      "on-road\n",
      "real-life\n",
      "cross-functional\n",
      "decision-making\n",
      "easy-to-use\n",
      "high-risk\n",
      "private-sector\n",
      "public-health\n",
      "trade-off\n",
      "trade-offs\n",
      "use-case\n",
      "far-reaching\n",
      "fast-charging\n",
      "long-distance\n",
      "mobility-as-a-service\n",
      "in-depth\n",
      "light-skinned\n",
      "ai-generated\n",
      "day-to-day\n",
      "three-year\n",
      "zero-emissions\n",
      "10-year\n",
      "first-ever\n",
      "long-range\n",
      "long-term\n",
      "three-month\n",
      "fixed-route\n",
      "greenhouse-gas\n",
      "large-scale\n",
      "low-\n",
      "low-cost\n",
      "off-peak\n",
      "point-to-point\n",
      "public-\n",
      "robo-taxi\n",
      "robo-taxis\n",
      "stop-and-go\n",
      "vehicle-to-infrastructure\n",
      "wake-up\n",
      "last-minute\n",
      "opt-in\n",
      "3d-printed\n",
      "decision-makers\n",
      "wheelchair-accessible\n",
      "pre-tax\n",
      "flat-footed\n",
      "battery-powered\n",
      "beijing-based\n",
      "francisco-based\n",
      "get-go\n",
      "long-held\n",
      "single-passenger\n",
      "full-time\n",
      "three-quarters\n",
      "high-end\n",
      "netflix-like\n",
      "business-to-business\n",
      "customer-facing\n",
      "hewlett-packard\n",
      "plug-in\n",
      "all-important\n",
      "kick-start\n",
      "must-have\n",
      "lithium-ion\n",
      "ai-based\n",
      "artificial-intelligence\n",
      "in-car\n",
      "in-vehicle\n",
      "short-term\n",
      "bike-\n",
      "one-off\n",
      "ride-hail\n",
      "autonomous-vehicle\n",
      "2-\n",
      "sub-saharan\n",
      "augmented-reality\n",
      "three-dimensional\n",
      "air-taxi\n",
      "full-size\n",
      "vehicle-to-vehicle\n",
      "e-bike\n",
      "t-mobile\n",
      "ultra-low\n",
      "head-on\n",
      "next-generation\n",
      "on-street\n",
      "street-level\n",
      "cutting-edge\n",
      "on-board\n",
      "car-related\n",
      "co-operation\n",
      "so-called\n",
      "-0\n",
      "dollar-denominated\n",
      "pre-emptive\n",
      "south-east\n",
      "tie-up\n",
      "built-in\n",
      "data-sharing\n",
      "scooter-sharing\n",
      "brussels-capital\n",
      "fast-food\n",
      "pop-up\n",
      "rolls-royce\n",
      "internet-connected\n",
      "self-fulfilling\n",
      "well-intentioned\n",
      "high-speed\n",
      "mass-transit\n",
      "single-family\n",
      "asia-pacific\n",
      "5g-based\n",
      "catch-up\n",
      "high-frequency\n",
      "white-collar\n",
      "african-americans\n",
      "high-stakes\n",
      "pro-gun\n",
      "self-service\n",
      "world-first\n",
      "co-director\n",
      "on-campus\n",
      "face-scanning\n",
      "ai-related\n",
      "human-level\n",
      "city-wide\n",
      "semi-autonomous\n",
      "sign-up\n",
      "high-growth\n",
      "high-tech\n",
      "highly-skilled\n",
      "mid-1990s\n",
      "co-authors\n",
      "false-positive\n",
      "-5\n",
      "urbana-champaign\n",
      "one-third\n",
      "post-crisis\n",
      "-saar\n",
      "back-end\n",
      "bike-powered\n",
      "billion-dollar\n",
      "capital-efficient\n",
      "capital-light\n",
      "de-risking\n",
      "debt-funded\n",
      "door-to-door\n",
      "federal-aid\n",
      "incentive-based\n",
      "market-specific\n",
      "meet-ups\n",
      "multi-level\n",
      "new-driver\n",
      "non-farm\n",
      "product-market\n",
      "stay-at-home\n",
      "under-utilized\n",
      "value-add\n",
      "venture-backed\n",
      "over-the-air\n",
      "record-breaking\n",
      "lock-in\n",
      "germ-line\n",
      "hiv-positive\n",
      "off-target\n",
      "self-regulation\n",
      "stem-cell\n",
      "evidence-based\n",
      "check-in\n",
      "ai-driven\n",
      "cross-referenced\n",
      "full-screen\n",
      "kai-fu\n",
      "mass-market\n",
      "ever-changing\n",
      "pre-industrial\n",
      "ai-enabled\n",
      "computer-vision\n",
      "data-science\n",
      "high-level\n",
      "human-made\n",
      "natural-language\n",
      "one-year\n",
      "social-media\n",
      "stand-alone\n",
      "step-by-step\n",
      "3-d\n",
      "side-by-side\n",
      "two-year\n",
      "vice-president\n",
      "year-over-year\n",
      "roof-mounted\n",
      "s-class\n",
      "short-lived\n",
      "20-minute\n",
      "500-person\n",
      "tech-heavy\n",
      "world-class\n",
      "ai-infused\n",
      "bottom-up\n",
      "in-house\n",
      "long-standing\n",
      "venture-capital\n",
      "hands-off\n",
      "self-contained\n",
      "use-cases\n",
      "high-visibility\n",
      "middle-income\n",
      "service-based\n",
      "cloud-based\n",
      "custom-built\n",
      "york-based\n",
      "40-year-old\n",
      "france-presse\n",
      "3d-printing\n",
      "inner-city\n",
      "e-tron\n",
      "rear-seat\n",
      "high-altitude\n",
      "pre-emptively\n",
      "well-trodden\n",
      "co-founders\n",
      "state-run\n",
      "ever-increasing\n",
      "zero-sum\n",
      "one-time\n",
      "radio-frequency\n",
      "still-evolving\n",
      "well-funded\n",
      "high-performance\n",
      "paid-for\n",
      "public-private\n",
      "ground-based\n",
      "-10\n",
      "one-way\n",
      "three-day\n",
      "20-year\n",
      "ever-growing\n",
      "for-hire\n",
      "leading-edge\n",
      "cross-domain\n",
      "non-proprietary\n",
      "re-use\n",
      "s-eye-view\n",
      "000-square-foot\n",
      "half-dozen\n",
      "three-story\n",
      "well-positioned\n",
      "all-in-one\n",
      "health-care\n",
      "old-fashioned\n",
      "six-month\n",
      "7-eleven\n",
      "vehicle-to-grid\n",
      "city-state\n",
      "state-of-the-art\n",
      "zero-emission\n",
      "top-down\n",
      "third-largest\n",
      "plug-and-play\n",
      "machine-readable\n",
      "rear-view\n",
      "up-to-date\n",
      "hands-free\n",
      "renault-nissan-mitsubishi\n",
      "multi-modal\n",
      "well-established\n",
      "bike-sharing\n",
      "multibillion-dollar\n",
      "carbon-free\n",
      "energy-related\n",
      "singapore-based\n",
      "100-mile\n",
      "emerging-market\n",
      "risk-taking\n",
      "second-largest\n",
      "anti-immigrant\n",
      "right-wing\n",
      "battery-electric\n",
      "large-format\n",
      "ill-advised\n",
      "well-suited\n",
      "hydrogen-powered\n"
     ]
    }
   ],
   "source": [
    "for idx,word in iteritems(corpus_memory_friendly_NST.dictionary):\n",
    "    for punt in punctuation_marks_extended:\n",
    "        if punt in word:\n",
    "            print(word)\n",
    "    if '-' in word:\n",
    "        print(word)\n",
    "    if(len(word) < 2): print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculator for different scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unmodified Score is: 0.5185185185185185\n",
      "The modified Score: 0.5925925925925926\n",
      "The Mean Reciprocal Rank is: 0.2833333333333333\n",
      "The rank exponent score is: 0.5652473154277343\n"
     ]
    }
   ],
   "source": [
    "Example_to_calculate = [4,2,'x'] # Ranks: [TF-IDF, LSA, Word_Embeddings]\n",
    "#This is just a few ways to combine the results form the algorithms into one single weight. ('x' means that there is no recommendation from that algorithm)\n",
    "MRR = 0\n",
    "RBP = 0\n",
    "p = 0.85\n",
    "maxrank = 10\n",
    "semscore = 0\n",
    "occurrences = 0\n",
    "length = len(Example_to_calculate)\n",
    "for index, rank in enumerate(Example_to_calculate):\n",
    "    if not isinstance(rank,int): Example_to_calculate[index] = maxrank\n",
    "    else: occurrences += 1\n",
    "\n",
    "for rank in Example_to_calculate:\n",
    "    MRR += 1/rank\n",
    "    semscore += (maxrank - rank)\n",
    "    RBP += p**rank\n",
    "\n",
    "#Normal Score used here\n",
    "score = semscore/((maxrank - 1)*length)\n",
    "print(\"The unmodified Score is: \" + str(score))\n",
    "\n",
    "#Modified Score with penalization for unmatching algorithms\n",
    "final_score = ((occurrences-1)+semscore/((maxrank-1)*occurrences))/length\n",
    "print(\"The modified Score: \" + str(final_score))\n",
    "\n",
    "#Mean reciprocal rank (MRR)\n",
    "MRR /= length\n",
    "print(\"The Mean Reciprocal Rank is: \" + str(MRR))\n",
    "\n",
    "#RBP score normalized\n",
    "RBP /= p*length\n",
    "print(\"The rank exponent score is: \" + str(RBP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the results from the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = False\n",
    "if merge:\n",
    "    store_recommendations_Neo4j()\n",
    "    merge_recommendations_Neo4j()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing results in Matlab format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import scipy.io\n",
    "\n",
    "export_MATLAB = False #Choose if you want to export some models to MATLAB format\n",
    "\n",
    "if(export_MATLAB):\n",
    "    \n",
    "    #TF-IDF\n",
    "    tf_idfmatrix = similarities.Similarity.load(index_tfidf_name)\n",
    "    corpus_test = MyCorpusDashNeo()\n",
    "    tfidf_test = models.TfidfModel.load(index_tfidf_model_name)\n",
    "    index_tfidf_test = similarities.Similarity.load(index_tfidf_name)\n",
    "    corpus_tfidf = tfidf_test[corpus_test]\n",
    "    sim = index_tfidf_test[corpus_tfidf]\n",
    "    scipy.io.savemat('matrix_tf_idf.mat', dict(x=sim))\n",
    "\n",
    "    #LSA\n",
    "    docs = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) RETURN m.preprocessed_stemmed AS preprocessed ORDER BY m.liteId ASC\").data()\n",
    "    bow_test = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in docs]\n",
    "    lsi = models.LsiModel.load(index_lsi_model_name)\n",
    "    index = similarities.Similarity.load(index_lsi_name)\n",
    "    docs_lsi = lsi[bow_test]\n",
    "    simlsa = index[docs_lsi]\n",
    "    scipy.io.savemat('matrix_lsa.mat', dict(x=simlsa))\n",
    "    \n",
    "    #Word Embeddings\n",
    "    x = np.load(index_WE_model_eu_name_paper)\n",
    "    scipy.io.savemat('matrix_euclidean_dist_paper.mat', dict(x=x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lpa = False\n",
    "if run_lpa:\n",
    "    for i in range(1000):\n",
    "        query = \"\"\"\n",
    "        MATCH (e1:Article:`_AI` {cluster:\"cluster_9.495788516814935\"})<-[d1:RELATES_TO]-(e2:Article:`_AI` {cluster:\"cluster_9.495788516814935\"})\n",
    "        WHERE exists(e2.cluster_sp1)\n",
    "        WITH e1, count(e2.cluster_sp1) AS cluster_count, e2.cluster_sp1 AS cluster_prop\n",
    "        SET e1.cluster_sp1 = CASE WHEN cluster_count >= e1.clusterCount THEN cluster_prop ELSE e1.cluster_sp1 END,\n",
    "        e1.clusterCount = CASE WHEN cluster_count >= e1.clusterCount THEN cluster_count ELSE e1.clusterCount END\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        MATCH (e1:Article:`_AI`)<-[d1:RELATES_TO]-(e2:Article:`_AI`)\n",
    "        WHERE exists(e2.cluster)\n",
    "        WITH e1, count(e2.cluster) AS cluster_count, e2.cluster AS cluster_prop\n",
    "        SET e1.cluster = CASE WHEN cluster_count >= e1.clusterCount THEN cluster_prop ELSE e1.cluster END,\n",
    "        e1.clusterCount = CASE WHEN cluster_count >= e1.clusterCount THEN cluster_count ELSE e1.clusterCount END\n",
    "        \"\"\"\n",
    "        graph.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_wordcloud = True\n",
    "def create_input_wordcloud(input_item):\n",
    "    if isinstance(input_item, str): #community\n",
    "        pass\n",
    "    elif isinstance(input_item, int): #lite_id document\n",
    "        query = \"MATCH (n:Article:_AI {liteId: $lite_id}) RETURN n.preprocessed_stemmed AS result\" #we can do it with content, keywords...\n",
    "        data = graph.run(query, parameters={'lite_id': input_item}).data()\n",
    "        string_wordcloud = \"\"\n",
    "        for word in data[0]['result']:\n",
    "            string_wordcloud += word + \" \"\n",
    "    return string_wordcloud\n",
    "        \n",
    "    \n",
    "if run_wordcloud:\n",
    "    mask = np.array(Image.open(os.path.join(dir_name, 'wordclouds/cloud.png')))\n",
    "    wc = WordCloud(background_color=\"white\", mask=mask, max_words=200, stopwords=stopwords.words('english'))\n",
    "    text = create_input_wordcloud(135)\n",
    "    wc.generate(text)\n",
    "    wc.to_file(os.path.join(dir_name, 'wordclouds/cloud_135_stem.png'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original content:\n",
      "<div><img title=\"‘Stupid Smart Cities’ With Molly Sauter\" src=\"https://cdn-images-1.medium.com/fit/t/1638/2048/1*Ng3VxdA_KnxaB7KMJyKIKg.jpeg\"><br><div><div>\n",
      "<p>Medium</p>\n",
      "<p>‘Stupid Smart Cities’ With Molly Sauter</p>\n",
      "<p>Is your city the next VC guinea pig in the technocratic experiment to grow cities and extract their value?</p>\n",
      "</div>\n",
      "</div></div>\n",
      "\n",
      "Pre-processed content:\n",
      "['medium', 'stupid', 'smart', 'citi', 'molli', 'sauter', 'citi', 'next', 'vc', 'guinea', 'pig', 'technocrat', 'experi', 'grow', 'citi', 'extract', 'valu']\n",
      "\n",
      "Bag of words:\n",
      "[(159, 1), (296, 3), (297, 1), (298, 1), (299, 1), (300, 1), (301, 1), (302, 1), (303, 1), (304, 1), (305, 1), (306, 1), (307, 1)]\n"
     ]
    }
   ],
   "source": [
    "data = graph.run(\"MATCH (n:Article) WHERE n.title=~ '(?i).*Molly Sauter.*' RETURN n.content AS content, n.preprocessed_stemmed AS prep LIMIT 1\").data()\n",
    "bowdata = corpus_memory_friendly.dictionary.doc2bow(data[0]['prep'])\n",
    "print(\"Original content:\")\n",
    "print(data[0]['content'], end=\"\\n\\n\")\n",
    "print(\"Pre-processed content:\")\n",
    "print(data[0]['prep'], end=\"\\n\\n\")\n",
    "print(\"Bag of words:\")\n",
    "print(bowdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of topics:\n",
      "------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.533*\"amountofmoney\" + 0.301*\"usa\" + 0.228*\"citi\" + 0.221*\"data\" + 0.159*\"use\" + 0.124*\"compani\"'),\n",
       " (1,\n",
       "  '0.528*\"amountofmoney\" + 0.341*\"usa\" + -0.226*\"data\" + -0.189*\"use\" + -0.135*\"ai\" + -0.126*\"compani\"'),\n",
       " (2,\n",
       "  '0.478*\"citi\" + 0.343*\"00\" + -0.271*\"emiss\" + -0.247*\"use\" + -0.227*\"data\" + -0.152*\"co2\"'),\n",
       " (3,\n",
       "  '0.434*\"data\" + -0.411*\"emiss\" + 0.248*\"ai\" + -0.218*\"co2\" + -0.200*\"electr\" + -0.174*\"citi\"'),\n",
       " (4,\n",
       "  '0.294*\"data\" + -0.281*\"compani\" + 0.261*\"00\" + 0.232*\"ai\" + 0.218*\"citi\" + 0.212*\"emiss\"'),\n",
       " (5,\n",
       "  '-0.419*\"debt\" + -0.344*\"trillion\" + -0.256*\"80\" + -0.250*\"60\" + -0.223*\"70\" + -0.211*\"50\"'),\n",
       " (6,\n",
       "  '-0.499*\"data\" + 0.442*\"ai\" + 0.199*\"use\" + 0.177*\"car\" + -0.164*\"compani\" + 0.148*\"case\"'),\n",
       " (7,\n",
       "  '0.366*\"citi\" + -0.221*\"say\" + -0.203*\"said\" + 0.182*\"scooter\" + 0.174*\"vehicl\" + 0.161*\"urban\"'),\n",
       " (8,\n",
       "  '-0.265*\"compani\" + -0.254*\"franchis\" + -0.231*\"00\" + -0.214*\"busi\" + -0.207*\"ai\" + 0.196*\"said\"'),\n",
       " (9,\n",
       "  '0.412*\"car\" + -0.261*\"citi\" + 0.217*\"data\" + 0.201*\"vehicl\" + -0.165*\"compani\" + -0.144*\"said\"'),\n",
       " (10,\n",
       "  '0.240*\"market\" + 0.237*\"growth\" + -0.226*\"said\" + -0.219*\"compani\" + 0.214*\"europ\" + -0.203*\"use\"'),\n",
       " (11,\n",
       "  '0.210*\"franchis\" + -0.209*\"scooter\" + -0.186*\"00\" + -0.178*\"said\" + -0.177*\"growth\" + 0.175*\"urban\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Example of topics:\")\n",
    "print(\"------------------\")\n",
    "lsi.show_topics(num_topics=12, num_words=6, formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\illopis\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\similarities\\docsim.py:528: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  result = numpy.hstack(shard_results)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Similarity scores from Gensim:similarities.docsim.Similarity\n",
      "------------------------------------------------------------\n",
      "[0.30212247 0.04956286 0.6154264  0.2597915  0.39088592 0.3842931\n",
      " 0.3778536  0.3104151  0.22955361 0.4611849  0.51046    0.43769276\n",
      " 0.5052805  0.5277489  0.22411619 0.33471656 0.26910535 0.41786805\n",
      " 0.5239456  0.72261894 0.39387247 0.35863447 0.31230327 0.873045\n",
      " 0.5778198  0.84581834 0.2927637  0.41167647 0.47986642 0.31448\n",
      " 0.44029564 0.37297463 0.315411   0.5078711  0.53187954 0.5668112\n",
      " 0.4851275  0.11895483 0.35654843 0.3187701  0.2224991  0.3268307\n",
      " 0.42465708 0.46880358 0.22830614 0.15693648 0.3731786  0.76959544\n",
      " 0.3806922  0.3386366  0.33852616 0.1208545  0.24358585 0.34388173\n",
      " 0.5129665  0.5555518  0.59257776 0.55975026 0.4332778  0.22296406\n",
      " 0.35245377 0.26832294 0.45965555 0.38949436 0.1939505  0.29078034\n",
      " 0.10090712 0.18194221 0.33557725 0.4194428  0.4425024  0.44103342\n",
      " 0.4148211  0.315926   0.40060905 0.28857833 0.1891473  0.07481961\n",
      " 0.40074986 0.5296494  0.36550125 0.2290579  0.45681047 0.2863459\n",
      " 0.38066414 0.26103473 0.19154835 0.2531057  0.22979969 0.408218\n",
      " 0.404515   0.46807137 0.53021514 0.23063836 0.23598453 0.21922903\n",
      " 0.30891743 0.4019559  0.22254385 0.30737802 0.28839627 0.33505073\n",
      " 0.22332324 0.29114324 0.17836791 0.49033546 0.42375898 0.2315677\n",
      " 0.3757202  0.42936093 0.41004628 0.46981135 0.3229408  0.452226\n",
      " 0.48285815 0.17294799 0.45866475 0.40586278 0.28890038 0.4707533\n",
      " 0.56253374 0.5212153  0.32397085 0.19932163 0.5104682  0.20764747\n",
      " 0.35794213 0.08095387 0.21624874 0.19444862 0.54671454 0.22984421\n",
      " 0.5524624  0.30188093 0.18256904 0.9671922  0.4591978  0.59532857\n",
      " 0.4489585  0.27179307 0.17986558 0.2763938  0.38468397 0.3947229\n",
      " 0.34475434 0.42191863 0.4696375  0.29157582 0.25254852 0.2989976\n",
      " 0.25588804 0.1086375  0.26087427 0.89706355 0.5238248  0.37741578\n",
      " 0.47794247 0.46084785 0.57264733 0.55588967 0.9184112  0.4611052\n",
      " 0.19437195 0.33707204 0.53894925 0.55143976 0.14949271 0.39630288\n",
      " 0.55364317 0.25874698 0.32015574 0.5501151  0.16461374 0.5019049\n",
      " 0.6483642  0.43986937 0.02905374 0.6415137  0.36255908 0.31949845\n",
      " 0.33024707 0.5457959  0.59001946 0.5029423  0.3205219  0.3473449\n",
      " 0.42796874 0.56008595 0.37229943 0.26797822 0.36235815 0.27992287\n",
      " 0.02612254 0.29774413 0.1043069  0.44577816 0.32276598 0.25539047\n",
      " 0.26106036 0.23745593 0.08709686 0.44846585 0.24832717 0.6029524\n",
      " 0.11689074 0.2304764  0.03725662 0.20036085 0.3018998  0.14289254\n",
      " 0.47068998 0.38699007 0.27205437 0.25525182 0.6385385  0.40155682\n",
      " 0.3042592  0.30145773 0.04529756 0.4130998  0.5642311  0.61309546\n",
      " 0.79351836 0.28806078 0.45044142 0.50153893 0.3882159  0.31126854\n",
      " 0.3027845  0.23035936 0.2699107  0.5153298  0.13766417 0.37463886\n",
      " 0.25844344 0.27162808 0.35547316 0.36407015 0.25983948 0.3745894\n",
      " 0.20931588 0.15662253 0.2206831  0.4711114  0.20331399 0.29960173\n",
      " 0.49534258 0.5215555  0.5162198  0.5578718  0.25097933 0.39806116\n",
      " 0.35158646 0.30451062 0.09013995 0.5391157  0.2281846  0.25709814\n",
      " 0.18771395 0.26106453 0.4692659  0.36385596 0.1591178  0.3801796\n",
      " 0.5890306  0.3221194  0.29600462 0.23528333 0.3966177  0.41106784\n",
      " 0.4601993  0.39614737 0.62153244 0.38491678 0.5420231  0.4680389\n",
      " 0.37904513 0.45715773 0.28894916 0.6365841  0.3031385  0.29449266\n",
      " 0.32720956 0.25829074 0.2630712  0.34539402 0.2114387  0.07700981\n",
      " 0.24630722 0.13292994 0.4018182  0.04884265 0.2654517  0.21166518\n",
      " 0.46472916 0.4949854  0.4014342  0.3622646  0.40412897 0.47389787\n",
      " 0.6149576  0.25417092 0.38826147 0.21593034 0.24540274 0.3605216\n",
      " 0.14233324 0.47716376 0.42384177 0.24487077 0.34193805 0.33032164\n",
      " 0.39460644 0.412969   0.45596352 0.43083417 0.29028574 0.4972484\n",
      " 0.3153841  0.5413576  0.18737103 0.4087622  0.43547562 0.28435054\n",
      " 0.44977182 0.5615035  0.37766853 0.29700384 0.31933337 0.29244375\n",
      " 0.3359664  0.18500505 0.4376282  0.6420331  0.36867195 0.3736442\n",
      " 0.6189468  0.30866247 0.4876361  0.30912432 0.2209471  0.2502874\n",
      " 0.37033045 0.40921244 0.31660518 0.6473656  0.14559235 0.19927363\n",
      " 0.47674176 0.55175024 0.48428145 0.5982803  0.17671953 0.52077574\n",
      " 0.40054217 0.27249134 0.33343965 0.36415648 0.30237105 0.23231809\n",
      " 0.09691457 0.5923064  0.41514933 0.4745604  0.03542143 0.4444798\n",
      " 0.6082805  0.34695452 0.3037941  0.18271044 0.28371757 0.2870738\n",
      " 0.26752156 0.29573715]\n",
      "\n",
      "\n",
      "Similarity scores from Gensim:similarities.docsim.Similarity (enumerated)\n",
      "-------------------------------------------------------------------------\n",
      "[(0, 0.30212247), (1, 0.049562864), (2, 0.6154264), (3, 0.2597915), (4, 0.39088592), (5, 0.3842931), (6, 0.3778536), (7, 0.3104151), (8, 0.22955361), (9, 0.4611849), (10, 0.51046), (11, 0.43769276), (12, 0.5052805), (13, 0.5277489), (14, 0.22411619), (15, 0.33471656), (16, 0.26910535), (17, 0.41786805), (18, 0.5239456), (19, 0.72261894), (20, 0.39387247), (21, 0.35863447), (22, 0.31230327), (23, 0.873045), (24, 0.5778198), (25, 0.84581834), (26, 0.2927637), (27, 0.41167647), (28, 0.47986642), (29, 0.31448), (30, 0.44029564), (31, 0.37297463), (32, 0.315411), (33, 0.5078711), (34, 0.53187954), (35, 0.5668112), (36, 0.4851275), (37, 0.11895483), (38, 0.35654843), (39, 0.3187701), (40, 0.2224991), (41, 0.3268307), (42, 0.42465708), (43, 0.46880358), (44, 0.22830614), (45, 0.15693648), (46, 0.3731786), (47, 0.76959544), (48, 0.3806922), (49, 0.3386366), (50, 0.33852616), (51, 0.1208545), (52, 0.24358585), (53, 0.34388173), (54, 0.5129665), (55, 0.5555518), (56, 0.59257776), (57, 0.55975026), (58, 0.4332778), (59, 0.22296406), (60, 0.35245377), (61, 0.26832294), (62, 0.45965555), (63, 0.38949436), (64, 0.1939505), (65, 0.29078034), (66, 0.100907125), (67, 0.18194221), (68, 0.33557725), (69, 0.4194428), (70, 0.4425024), (71, 0.44103342), (72, 0.4148211), (73, 0.315926), (74, 0.40060905), (75, 0.28857833), (76, 0.1891473), (77, 0.07481961), (78, 0.40074986), (79, 0.5296494), (80, 0.36550125), (81, 0.2290579), (82, 0.45681047), (83, 0.2863459), (84, 0.38066414), (85, 0.26103473), (86, 0.19154835), (87, 0.2531057), (88, 0.22979969), (89, 0.408218), (90, 0.404515), (91, 0.46807137), (92, 0.53021514), (93, 0.23063836), (94, 0.23598453), (95, 0.21922903), (96, 0.30891743), (97, 0.4019559), (98, 0.22254385), (99, 0.30737802), (100, 0.28839627), (101, 0.33505073), (102, 0.22332324), (103, 0.29114324), (104, 0.17836791), (105, 0.49033546), (106, 0.42375898), (107, 0.2315677), (108, 0.3757202), (109, 0.42936093), (110, 0.41004628), (111, 0.46981135), (112, 0.3229408), (113, 0.452226), (114, 0.48285815), (115, 0.17294799), (116, 0.45866475), (117, 0.40586278), (118, 0.28890038), (119, 0.4707533), (120, 0.56253374), (121, 0.5212153), (122, 0.32397085), (123, 0.19932163), (124, 0.5104682), (125, 0.20764747), (126, 0.35794213), (127, 0.08095387), (128, 0.21624874), (129, 0.19444862), (130, 0.54671454), (131, 0.22984421), (132, 0.5524624), (133, 0.30188093), (134, 0.18256904), (135, 0.9671922), (136, 0.4591978), (137, 0.59532857), (138, 0.4489585), (139, 0.27179307), (140, 0.17986558), (141, 0.2763938), (142, 0.38468397), (143, 0.3947229), (144, 0.34475434), (145, 0.42191863), (146, 0.4696375), (147, 0.29157582), (148, 0.25254852), (149, 0.2989976), (150, 0.25588804), (151, 0.108637504), (152, 0.26087427), (153, 0.89706355), (154, 0.5238248), (155, 0.37741578), (156, 0.47794247), (157, 0.46084785), (158, 0.57264733), (159, 0.55588967), (160, 0.9184112), (161, 0.4611052), (162, 0.19437195), (163, 0.33707204), (164, 0.53894925), (165, 0.55143976), (166, 0.14949271), (167, 0.39630288), (168, 0.55364317), (169, 0.25874698), (170, 0.32015574), (171, 0.5501151), (172, 0.16461374), (173, 0.5019049), (174, 0.6483642), (175, 0.43986937), (176, 0.029053744), (177, 0.6415137), (178, 0.36255908), (179, 0.31949845), (180, 0.33024707), (181, 0.5457959), (182, 0.59001946), (183, 0.5029423), (184, 0.3205219), (185, 0.3473449), (186, 0.42796874), (187, 0.56008595), (188, 0.37229943), (189, 0.26797822), (190, 0.36235815), (191, 0.27992287), (192, 0.026122538), (193, 0.29774413), (194, 0.1043069), (195, 0.44577816), (196, 0.32276598), (197, 0.25539047), (198, 0.26106036), (199, 0.23745593), (200, 0.087096855), (201, 0.44846585), (202, 0.24832717), (203, 0.6029524), (204, 0.116890736), (205, 0.2304764), (206, 0.037256617), (207, 0.20036085), (208, 0.3018998), (209, 0.14289254), (210, 0.47068998), (211, 0.38699007), (212, 0.27205437), (213, 0.25525182), (214, 0.6385385), (215, 0.40155682), (216, 0.3042592), (217, 0.30145773), (218, 0.04529756), (219, 0.4130998), (220, 0.5642311), (221, 0.61309546), (222, 0.79351836), (223, 0.28806078), (224, 0.45044142), (225, 0.50153893), (226, 0.3882159), (227, 0.31126854), (228, 0.3027845), (229, 0.23035936), (230, 0.2699107), (231, 0.5153298), (232, 0.13766417), (233, 0.37463886), (234, 0.25844344), (235, 0.27162808), (236, 0.35547316), (237, 0.36407015), (238, 0.25983948), (239, 0.3745894), (240, 0.20931588), (241, 0.15662253), (242, 0.2206831), (243, 0.4711114), (244, 0.20331399), (245, 0.29960173), (246, 0.49534258), (247, 0.5215555), (248, 0.5162198), (249, 0.5578718), (250, 0.25097933), (251, 0.39806116), (252, 0.35158646), (253, 0.30451062), (254, 0.09013995), (255, 0.5391157), (256, 0.2281846), (257, 0.25709814), (258, 0.18771395), (259, 0.26106453), (260, 0.4692659), (261, 0.36385596), (262, 0.1591178), (263, 0.3801796), (264, 0.5890306), (265, 0.3221194), (266, 0.29600462), (267, 0.23528333), (268, 0.3966177), (269, 0.41106784), (270, 0.4601993), (271, 0.39614737), (272, 0.62153244), (273, 0.38491678), (274, 0.5420231), (275, 0.4680389), (276, 0.37904513), (277, 0.45715773), (278, 0.28894916), (279, 0.6365841), (280, 0.3031385), (281, 0.29449266), (282, 0.32720956), (283, 0.25829074), (284, 0.2630712), (285, 0.34539402), (286, 0.2114387), (287, 0.07700981), (288, 0.24630722), (289, 0.13292994), (290, 0.4018182), (291, 0.048842654), (292, 0.2654517), (293, 0.21166518), (294, 0.46472916), (295, 0.4949854), (296, 0.4014342), (297, 0.3622646), (298, 0.40412897), (299, 0.47389787), (300, 0.6149576), (301, 0.25417092), (302, 0.38826147), (303, 0.21593034), (304, 0.24540274), (305, 0.3605216), (306, 0.14233324), (307, 0.47716376), (308, 0.42384177), (309, 0.24487077), (310, 0.34193805), (311, 0.33032164), (312, 0.39460644), (313, 0.412969), (314, 0.45596352), (315, 0.43083417), (316, 0.29028574), (317, 0.4972484), (318, 0.3153841), (319, 0.5413576), (320, 0.18737103), (321, 0.4087622), (322, 0.43547562), (323, 0.28435054), (324, 0.44977182), (325, 0.5615035), (326, 0.37766853), (327, 0.29700384), (328, 0.31933337), (329, 0.29244375), (330, 0.3359664), (331, 0.18500505), (332, 0.4376282), (333, 0.6420331), (334, 0.36867195), (335, 0.3736442), (336, 0.6189468), (337, 0.30866247), (338, 0.4876361), (339, 0.30912432), (340, 0.2209471), (341, 0.2502874), (342, 0.37033045), (343, 0.40921244), (344, 0.31660518), (345, 0.6473656), (346, 0.14559235), (347, 0.19927363), (348, 0.47674176), (349, 0.55175024), (350, 0.48428145), (351, 0.5982803), (352, 0.17671953), (353, 0.52077574), (354, 0.40054217), (355, 0.27249134), (356, 0.33343965), (357, 0.36415648), (358, 0.30237105), (359, 0.23231809), (360, 0.096914575), (361, 0.5923064), (362, 0.41514933), (363, 0.4745604), (364, 0.035421427), (365, 0.4444798), (366, 0.6082805), (367, 0.34695452), (368, 0.3037941), (369, 0.18271044), (370, 0.28371757), (371, 0.2870738), (372, 0.26752156), (373, 0.29573715)]\n",
      "\n",
      "\n",
      "Similarity scores from Gensim:similarities.docsim.Similarity (enumerated and ordered)\n",
      "-------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(135, 0.9671922), (160, 0.9184112), (153, 0.89706355), (23, 0.873045), (25, 0.84581834), (222, 0.79351836), (47, 0.76959544), (19, 0.72261894), (174, 0.6483642), (345, 0.6473656), (333, 0.6420331), (177, 0.6415137), (214, 0.6385385), (279, 0.6365841), (272, 0.62153244), (336, 0.6189468), (2, 0.6154264), (300, 0.6149576), (221, 0.61309546), (366, 0.6082805), (203, 0.6029524), (351, 0.5982803), (137, 0.59532857), (56, 0.59257776), (361, 0.5923064), (182, 0.59001946), (264, 0.5890306), (24, 0.5778198), (158, 0.57264733), (35, 0.5668112), (220, 0.5642311), (120, 0.56253374), (325, 0.5615035), (187, 0.56008595), (57, 0.55975026), (249, 0.5578718), (159, 0.55588967), (55, 0.5555518), (168, 0.55364317), (132, 0.5524624), (349, 0.55175024), (165, 0.55143976), (171, 0.5501151), (130, 0.54671454), (181, 0.5457959), (274, 0.5420231), (319, 0.5413576), (255, 0.5391157), (164, 0.53894925), (34, 0.53187954), (92, 0.53021514), (79, 0.5296494), (13, 0.5277489), (18, 0.5239456), (154, 0.5238248), (247, 0.5215555), (121, 0.5212153), (353, 0.52077574), (248, 0.5162198), (231, 0.5153298), (54, 0.5129665), (124, 0.5104682), (10, 0.51046), (33, 0.5078711), (12, 0.5052805), (183, 0.5029423), (173, 0.5019049), (225, 0.50153893), (317, 0.4972484), (246, 0.49534258), (295, 0.4949854), (105, 0.49033546), (338, 0.4876361), (36, 0.4851275), (350, 0.48428145), (114, 0.48285815), (28, 0.47986642), (156, 0.47794247), (307, 0.47716376), (348, 0.47674176), (363, 0.4745604), (299, 0.47389787), (243, 0.4711114), (119, 0.4707533), (210, 0.47068998), (111, 0.46981135), (146, 0.4696375), (260, 0.4692659), (43, 0.46880358), (91, 0.46807137), (275, 0.4680389), (294, 0.46472916), (9, 0.4611849), (161, 0.4611052), (157, 0.46084785), (270, 0.4601993), (62, 0.45965555), (136, 0.4591978), (116, 0.45866475), (277, 0.45715773), (82, 0.45681047), (314, 0.45596352), (113, 0.452226), (224, 0.45044142), (324, 0.44977182), (138, 0.4489585), (201, 0.44846585), (195, 0.44577816), (365, 0.4444798), (70, 0.4425024), (71, 0.44103342), (30, 0.44029564), (175, 0.43986937), (11, 0.43769276), (332, 0.4376282), (322, 0.43547562), (58, 0.4332778), (315, 0.43083417), (109, 0.42936093), (186, 0.42796874), (42, 0.42465708), (308, 0.42384177), (106, 0.42375898), (145, 0.42191863), (69, 0.4194428), (17, 0.41786805), (362, 0.41514933), (72, 0.4148211), (219, 0.4130998), (313, 0.412969), (27, 0.41167647), (269, 0.41106784), (110, 0.41004628), (343, 0.40921244), (321, 0.4087622), (89, 0.408218), (117, 0.40586278), (90, 0.404515), (298, 0.40412897), (97, 0.4019559), (290, 0.4018182), (215, 0.40155682), (296, 0.4014342), (78, 0.40074986), (74, 0.40060905), (354, 0.40054217), (251, 0.39806116), (268, 0.3966177), (167, 0.39630288), (271, 0.39614737), (143, 0.3947229), (312, 0.39460644), (20, 0.39387247), (4, 0.39088592), (63, 0.38949436), (302, 0.38826147), (226, 0.3882159), (211, 0.38699007), (273, 0.38491678), (142, 0.38468397), (5, 0.3842931), (48, 0.3806922), (84, 0.38066414), (263, 0.3801796), (276, 0.37904513), (6, 0.3778536), (326, 0.37766853), (155, 0.37741578), (108, 0.3757202), (233, 0.37463886), (239, 0.3745894), (335, 0.3736442), (46, 0.3731786), (31, 0.37297463), (188, 0.37229943), (342, 0.37033045), (334, 0.36867195), (80, 0.36550125), (357, 0.36415648), (237, 0.36407015), (261, 0.36385596), (178, 0.36255908), (190, 0.36235815), (297, 0.3622646), (305, 0.3605216), (21, 0.35863447), (126, 0.35794213), (38, 0.35654843), (236, 0.35547316), (60, 0.35245377), (252, 0.35158646), (185, 0.3473449), (367, 0.34695452), (285, 0.34539402), (144, 0.34475434), (53, 0.34388173), (310, 0.34193805), (49, 0.3386366), (50, 0.33852616), (163, 0.33707204), (330, 0.3359664), (68, 0.33557725), (101, 0.33505073), (15, 0.33471656), (356, 0.33343965), (311, 0.33032164), (180, 0.33024707), (282, 0.32720956), (41, 0.3268307), (122, 0.32397085), (112, 0.3229408), (196, 0.32276598), (265, 0.3221194), (184, 0.3205219), (170, 0.32015574), (179, 0.31949845), (328, 0.31933337), (39, 0.3187701), (344, 0.31660518), (73, 0.315926), (32, 0.315411), (318, 0.3153841), (29, 0.31448), (22, 0.31230327), (227, 0.31126854), (7, 0.3104151), (339, 0.30912432), (96, 0.30891743), (337, 0.30866247), (99, 0.30737802), (253, 0.30451062), (216, 0.3042592), (368, 0.3037941), (280, 0.3031385), (228, 0.3027845), (358, 0.30237105), (0, 0.30212247), (208, 0.3018998), (133, 0.30188093), (217, 0.30145773), (245, 0.29960173), (149, 0.2989976), (193, 0.29774413), (327, 0.29700384), (266, 0.29600462), (373, 0.29573715), (281, 0.29449266), (26, 0.2927637), (329, 0.29244375), (147, 0.29157582), (103, 0.29114324), (65, 0.29078034), (316, 0.29028574), (278, 0.28894916), (118, 0.28890038), (75, 0.28857833), (100, 0.28839627), (223, 0.28806078), (371, 0.2870738), (83, 0.2863459), (323, 0.28435054), (370, 0.28371757), (191, 0.27992287), (141, 0.2763938), (355, 0.27249134), (212, 0.27205437), (139, 0.27179307), (235, 0.27162808), (230, 0.2699107), (16, 0.26910535), (61, 0.26832294), (189, 0.26797822), (372, 0.26752156), (292, 0.2654517), (284, 0.2630712), (259, 0.26106453), (198, 0.26106036), (85, 0.26103473), (152, 0.26087427), (238, 0.25983948), (3, 0.2597915), (169, 0.25874698), (234, 0.25844344), (283, 0.25829074), (257, 0.25709814), (150, 0.25588804), (197, 0.25539047), (213, 0.25525182), (301, 0.25417092), (87, 0.2531057), (148, 0.25254852), (250, 0.25097933), (341, 0.2502874), (202, 0.24832717), (288, 0.24630722), (304, 0.24540274), (309, 0.24487077), (52, 0.24358585), (199, 0.23745593), (94, 0.23598453), (267, 0.23528333), (359, 0.23231809), (107, 0.2315677), (93, 0.23063836), (205, 0.2304764), (229, 0.23035936), (131, 0.22984421), (88, 0.22979969), (8, 0.22955361), (81, 0.2290579), (44, 0.22830614), (256, 0.2281846), (14, 0.22411619), (102, 0.22332324), (59, 0.22296406), (98, 0.22254385), (40, 0.2224991), (340, 0.2209471), (242, 0.2206831), (95, 0.21922903), (128, 0.21624874), (303, 0.21593034), (293, 0.21166518), (286, 0.2114387), (240, 0.20931588), (125, 0.20764747), (244, 0.20331399), (207, 0.20036085), (123, 0.19932163), (347, 0.19927363), (129, 0.19444862), (162, 0.19437195), (64, 0.1939505), (86, 0.19154835), (76, 0.1891473), (258, 0.18771395), (320, 0.18737103), (331, 0.18500505), (369, 0.18271044), (134, 0.18256904), (67, 0.18194221), (140, 0.17986558), (104, 0.17836791), (352, 0.17671953), (115, 0.17294799), (172, 0.16461374), (262, 0.1591178), (45, 0.15693648), (241, 0.15662253), (166, 0.14949271), (346, 0.14559235), (209, 0.14289254), (306, 0.14233324), (232, 0.13766417), (289, 0.13292994), (51, 0.1208545), (37, 0.11895483), (204, 0.116890736), (151, 0.108637504), (194, 0.1043069), (66, 0.100907125), (360, 0.096914575), (254, 0.09013995), (200, 0.087096855), (127, 0.08095387), (287, 0.07700981), (77, 0.07481961), (1, 0.049562864), (291, 0.048842654), (218, 0.04529756), (206, 0.037256617), (364, 0.035421427), (176, 0.029053744), (192, 0.026122538)]\n"
     ]
    }
   ],
   "source": [
    "a = graph.run(\"MATCH (n:Article) WHERE n.liteId = 135 RETURN DISTINCT n.preprocessed AS prep LIMIT 1\").data()\n",
    "test = corpus_memory_friendly.dictionary.doc2bow(a[0]['prep'])\n",
    "test = lsi[test]\n",
    "ind = index_lsi[test]\n",
    "print(\"\\n\\nSimilarity scores from Gensim:similarities.docsim.Similarity\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(ind)\n",
    "print(\"\\n\\nSimilarity scores from Gensim:similarities.docsim.Similarity (enumerated)\")\n",
    "print(\"-------------------------------------------------------------------------\")\n",
    "print([val for val in enumerate(ind)])\n",
    "#docs_similar = index_tfidf[doc_tfidf]\n",
    "print(\"\\n\\nSimilarity scores from Gensim:similarities.docsim.Similarity (enumerated and ordered)\")\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "sort_ind = sorted(enumerate(ind), key=lambda item: -item[1])\n",
    "print(sort_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
