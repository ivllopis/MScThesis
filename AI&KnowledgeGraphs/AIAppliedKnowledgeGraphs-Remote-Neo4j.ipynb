{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-23 09:02:50,382 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2019-08-23 09:02:54,130 : INFO : Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import logging\n",
    "import traceback\n",
    "import html\n",
    "import matplotlib.pyplot as plt\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from collections import defaultdict\n",
    "from six import iteritems\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings, BertEmbeddings #import only what is needed everyt time?\n",
    "from flair.data import Sentence\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Parameters\n",
    "TRAINING = False\n",
    "UPDATE_EMBEDDINGS = False\n",
    "\n",
    "#Naming the directories for the models\n",
    "dir_name = './models/'\n",
    "corpus_name = os.path.join(dir_name, 'corpus_feedly.mm') #Stemmed Version\n",
    "dict_name = os.path.join(dir_name, 'dictionary_feedly.dict')\n",
    "index_tfidf_name = os.path.join(dir_name, 'fd_index_tfidf.index')\n",
    "index_tfidf_model_name = os.path.join(dir_name, 'fd_model_tfidf.model')\n",
    "index_lsi_name = os.path.join(dir_name, 'fd_index_lsi.index')\n",
    "index_lsi_model_name = os.path.join(dir_name, 'fd_model_lsi.model')\n",
    "index_lda_name = os.path.join(dir_name, 'fd_index_lda.index')\n",
    "index_lda_model_name = os.path.join(dir_name, 'fd_model_lda.model')\n",
    "corpus_name_nst = os.path.join(dir_name, 'corpus_feedly_nst.mm') #Not-Stemmed Version\n",
    "dict_name_nst = os.path.join(dir_name, 'dictionary_feedly_nst.dict')\n",
    "index_tfidf_name_nst = os.path.join(dir_name, 'fd_index_tfidf_nst.index')\n",
    "index_tfidf_model_name_nst = os.path.join(dir_name, 'fd_model_tfidf_nst.model')\n",
    "index_lsi_name_nst = os.path.join(dir_name, 'fd_index_lsi_nst.index')\n",
    "index_lsi_model_name_nst = os.path.join(dir_name, 'fd_model_lsi_nst.model')\n",
    "index_lda_name_nst = os.path.join(dir_name, 'fd_index_lda_nst.index')\n",
    "index_lda_model_name_nst = os.path.join(dir_name, 'fd_model_lda_nst.model')\n",
    "index_WE_model_cs_name_glove = os.path.join(dir_name, 'fd_model_we_cosine_similarity_glove.dat')\n",
    "index_WE_model_eu_name_glove = os.path.join(dir_name, 'fd_model_we_euclidean_distance_glove.dat')\n",
    "index_WE_model_eu_name_paper = os.path.join(dir_name, 'fd_model_we_euclidean_distance_paper_method.dat')\n",
    "index_WE_model_cs_name_flair = os.path.join(dir_name, 'fd_model_we_cosine_similarity_flair.dat')\n",
    "index_WE_model_eu_name_flair = os.path.join(dir_name, 'fd_model_we_euclidean_distance_flair.dat')\n",
    "index_WE_model_cs_name_bert = os.path.join(dir_name, 'fd_model_we_cosine_similarity_bert.dat')\n",
    "index_WE_model_eu_name_bert = os.path.join(dir_name, 'fd_model_we_euclidean_distance_bert.dat')\n",
    "index_WE_model_cs_name_bert_glove = os.path.join(dir_name, 'fd_model_we_cosine_similarity_bert_glove.dat')\n",
    "index_WE_model_eu_name_bert_glove = os.path.join(dir_name, 'fd_model_we_euclidean_distance_bert_glove.dat')\n",
    "index_WE_model_cs_name_flair_glove_news = os.path.join(dir_name, 'fd_model_we_cosine_similarity_flair_glove_news.dat')\n",
    "index_WE_model_eu_name_flair_glove_news = os.path.join(dir_name, 'fd_model_we_euclidean_distance_flair_glove_news.dat')\n",
    "index_WE_model_cs_name_flair_glove_multi = os.path.join(dir_name, 'fd_model_we_cosine_similarity_flair_glove_multi.dat')\n",
    "index_WE_model_eu_name_flair_glove_multi = os.path.join(dir_name, 'fd_model_we_euclidean_distance_flair_glove_multi.dat')\n",
    "index_WE_model_cs_title = os.path.join(dir_name, 'fd_model_we_cs_title.dat')\n",
    "index_WE_model_eu_title = os.path.join(dir_name, 'fd_model_we_eu_title.dat')\n",
    "glovefilename = os.path.join(dir_name, 'glove.42B.300d.txt')\n",
    "\n",
    "if not os.path.isdir(dir_name):\n",
    "    os.mkdir(dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for preprocessing and cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_punctuations = '‘' + '’' + '‚' + '„' + '…' + '``' + '“' + '”' + '£' + '€' + '¥' + '¢' + '₹' + '₱' + '₩' + '฿' + '₫' + '₪' + '‰' + '†' + '‡' + '•' + '¤' + '§' + '©' + '®' + '™' + '℠' + '«' + '»' + '¸' + '·' + '¯' + '¦' + '—'\n",
    "punctuation_marks_extended = string.punctuation.replace('-','') + extended_punctuations\n",
    "def extract_punctuation(text):\n",
    "    \"\"\"\n",
    "    Purpose: Clean the text from any punctuation mark, currency and special symbol.\n",
    "    Input: <List>. List of tokens from a text.\n",
    "    Output: <List>. Cleaned list of tokens from a text.\n",
    "    \"\"\"\n",
    "    processed_punctuation = []\n",
    "    acronymregex = re.compile(r'([A-z]{1}\\.)([A-z]{1}\\.)+') #check for acronyms with punctuation: r'([A-z]{1}\\.)([A-z]{1}\\.)+\\Z'\n",
    "    for word in text:\n",
    "        processed = False\n",
    "        if (len(word) > 1):\n",
    "            if acronymregex.match(word):\n",
    "                word = word.replace('.', '')\n",
    "                processed_punctuation.append(word)\n",
    "            else:\n",
    "                for punctmark in punctuation_marks_extended:\n",
    "                    if word.startswith(punctmark):\n",
    "                        word = word.replace(punctmark,'')\n",
    "                    if word.endswith(punctmark):\n",
    "                        word = word.replace(punctmark,'')\n",
    "                    if '/' not in word:\n",
    "                        if len(word) > 1:\n",
    "                            subwords = word.split(punctmark)\n",
    "                        if(len(subwords) > 1):\n",
    "                            processed = True\n",
    "                            for subword in subwords:\n",
    "                                if (len(subword)> 1): processed_punctuation.append(subword)\n",
    "                    else:\n",
    "                        processed = True\n",
    "                        break\n",
    "                if(processed == False): processed_punctuation.append(word)\n",
    "        #Note: I'm not letting pass one-letter words: unlikely to have a meaning and likely to be a stopword or punctuation mark\n",
    "    return processed_punctuation\n",
    "\n",
    "def convert_numbers_to_specialkey(content):\n",
    "    \"\"\"\n",
    "    Purpose: Transform any sum of money to a common token, so they convey the same meaning.\n",
    "    Input: <String>. Plain text, prefiltered of markups in this case.\n",
    "    Output: <String>. Text where any mention to amounts of money is substituted for the token 'amountofmoney'.\n",
    "    \"\"\"\n",
    "    amount = re.compile(r'([\\$€¥£¢₹₱₩฿₫₪]{1}[0-9]+(,[0-9]+)?)') #modify any amount of money in the document\n",
    "    content = re.sub(amount,'amountofmoney',content)\n",
    "    #numbers = re.compile(r'([0-9]+(,[0-9]+)?([a-z]{1,2})?)') #modify any number quantity in the document\n",
    "    #content = re.sub(numbers,'number',content)\n",
    "    return content\n",
    "\n",
    "def extract_markups(text):\n",
    "    \"\"\"\n",
    "    Purpose: Clean a text from markup <tag> elements.\n",
    "    Input: <String>. Plain text.\n",
    "    Output: <String>. Text cleaned from markups.\n",
    "    Note: It is rather a simple one, as it doesn't distinguish <tag> from <tag>(Stuff)</tag>.\n",
    "    \"\"\"\n",
    "    markups = re.compile(r'(<.*?>)') #remove markups\n",
    "    cltext = re.sub(markups,'',text)\n",
    "    return cltext\n",
    "\n",
    "def extract_stopwords(text):\n",
    "    \"\"\"\n",
    "    Purpose: Remove those words that are general in meaning and do not convey any specific context or topic.\n",
    "    Input: <List>. List of tokens which include stop words.\n",
    "    Output: <List>. List of tokens which do not include stop words.\n",
    "    \"\"\"\n",
    "    return [word for word in text if word not in stopwords.words('english')]\n",
    "\n",
    "def stem_words(text):\n",
    "    \"\"\"\n",
    "    Purpose: Transform words to their lexemes.\n",
    "             This is used for unifying words like 'fast', 'faster' and 'fastest', for example, into one word.\n",
    "             So words with different forms that mean the same semmantic meaning are unified.\n",
    "    Input: <List>. List of tokens.\n",
    "    Output: <List>. List of stemmed tokens.\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = []\n",
    "    for word in text:\n",
    "        stemmed_words.append(stemmer.stem(word))\n",
    "    return stemmed_words\n",
    "\n",
    "def cleanClipText(cliptext):\n",
    "    \"\"\"\n",
    "    Purpose: Filtering the elements to UTF-16. Tkinter cannot represent items outside this range, and causes an error.\n",
    "    Input: <String>. A word that can contain UTF-32 characters.\n",
    "    Output: <String>. Word without the characters out of representable range in Tkinter.\n",
    "    \"\"\"\n",
    "    #Removing all characters > 65535 (that's the range for tcl)\n",
    "    cliptext = \"\".join([c for c in cliptext if ord(c) <= 65535])\n",
    "    return cliptext\n",
    "\n",
    "def final_clean(text_list):\n",
    "    \"\"\"\n",
    "    Purpose: Do a final sweep over the tokens in search for elements that could have passed the filters.\n",
    "    Input: <List>. List of preprocessed tokens from the texts.\n",
    "    Output: <List>. List of cleaned preprocessed tokens from the texts.\n",
    "    \"\"\"\n",
    "    clean_text = []\n",
    "    final_text = []\n",
    "    for word in text_list:\n",
    "        processed = False\n",
    "        if '/' not in word:\n",
    "            for punctmark in punctuation_marks_extended:\n",
    "                if word.startswith(punctmark):\n",
    "                    word = word.replace(punctmark,'')\n",
    "                if word.endswith(punctmark):\n",
    "                    word = word.replace(punctmark,'')\n",
    "                subwords = word.split(punctmark)\n",
    "                if(len(subwords) > 1):\n",
    "                    processed = True\n",
    "                    for subword in subwords:\n",
    "                        if (len(subword)> 1): clean_text.append(subword)\n",
    "            if(processed == False):\n",
    "                if(len(word) >= 2): clean_text.append(word) #clean remaining single letters and white-spaces\n",
    "    for word in clean_text:\n",
    "        clean_word = cleanClipText(word)\n",
    "        final_text.append(clean_word)\n",
    "    return final_text\n",
    "\n",
    "def preprocess_text(article):\n",
    "    \"\"\"\n",
    "    Purpose: Main function for preprocessing a text.\n",
    "    Input: <String>. Raw plain text coming from a source. In this case, HTML source code.\n",
    "    Output: <List>. List of clean representable tokens that convey meaning from the raw text.\n",
    "    \"\"\"\n",
    "    \n",
    "    content = html.unescape(article) #clean unwanted html hexadecimal entities\n",
    "    content = extract_markups(content) #\n",
    "    content = convert_numbers_to_specialkey(content)\n",
    "    content = word_tokenize(content.lower()) #tokenize words\n",
    "    content = extract_punctuation(content) #remove punctuation marks\n",
    "    content = final_clean(content)\n",
    "    content = extract_stopwords(content) #remove stopwords (english)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for loading information into Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_articlesNeo4j():\n",
    "    \"\"\"\n",
    "    Purpose: This will load a list of articles in JSON format into Neo4j from the import folder.\n",
    "    Input: JSON file coming from the Feedly.com API.\n",
    "    Output: The articles will be imported in the active database in Neo4j as nodes.\n",
    "    Note: The JSON file must follow the same structure that Feedly.com provides.\n",
    "    \"\"\"\n",
    "    \n",
    "    queryLoadFeedlyArticles = \"\"\"\n",
    "    CALL apoc.load.json('file:///all_complete_articles.json') YIELD value\n",
    "    UNWIND value.items AS item\n",
    "    MERGE (a:Article:_AI {id:item.id})\n",
    "    SET a.created = item.crawled,\n",
    "        a.image = item.visual.url,\n",
    "        a.title = trim(item.title),\n",
    "        a.author = trim(item.author),\n",
    "        a.content = coalesce(item.content.content,item.fullContent),    \n",
    "        a.summary = item.summary.content,\n",
    "        a.url = [],\n",
    "        a.url = a.url + coalesce(item.canonicalUrl,[]),\n",
    "        a.highlightedText = []\n",
    "    FOREACH (annotation IN item.annotations |\n",
    "        SET a.highlightedText = a.highlightedText + annotation.highlight.text\n",
    "    )\n",
    "    FOREACH (alt IN item.alternate |\n",
    "        SET a.url = a.url + alt.href\n",
    "    )\n",
    "\n",
    "    FOREACH (tag IN item.tags |\n",
    "        FOREACH(ignoreMe IN CASE WHEN left(tag.label,3) = \"FA.\" THEN [1] ELSE [] END |\n",
    "            MERGE (lfa:LtsFocusArea:_AI {name:trim(substring(tag.label,3))})\n",
    "            MERGE (a)-[r:RELATES_TO]->(lfa)\n",
    "        )\n",
    "    )\n",
    "    FOREACH (tag IN item.tags |\n",
    "        FOREACH(ignoreMe IN CASE WHEN left(tag.label,3) = \"HS.\" THEN [1] ELSE [] END |\n",
    "            MERGE (hs:HorizonScanningArea:_AI {name:trim(substring(tag.label,3))})\n",
    "            MERGE (a)-[r:IS_AN_INSTANCE_OF]->(hs)\n",
    "        )\n",
    "    )\n",
    "    FOREACH (tag IN item.tags |\n",
    "        FOREACH(ignoreMe IN CASE WHEN left(tag.label,3) = \"MT.\" THEN [1] ELSE [] END |\n",
    "            MERGE (mt:Megatrend:_AI {name:trim(substring(tag.label,3))})\n",
    "            MERGE (a)-[r:RELATES_TO]->(mt)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    WITH count(*) AS ignored\n",
    "\n",
    "    MATCH (a:Article:_AI)\n",
    "    WHERE a.highlightedText = []\n",
    "    SET a.highlightedText = NULL\n",
    "\n",
    "    WITH count(*) AS ignored\n",
    "\n",
    "    MATCH (a:Article:_AI)\n",
    "    WHERE a.url = []\n",
    "    SET a.url = NULL\n",
    "    \"\"\"\n",
    "    graph.run(queryLoadFeedlyArticles)\n",
    "\n",
    "def preprocess_articlesNeo4j():\n",
    "    \"\"\"\n",
    "    Purpose: Clean the database from bad examples. Relocate the content in articles where the content is in summary, etc.\n",
    "    Input: None.\n",
    "    Output: The database is updated.\n",
    "    Note: Some properties may be modified, and some articles deleted (empty articles, mostly).\n",
    "    \"\"\"\n",
    "    \n",
    "    graph.run(\"MATCH (m:Article:_AI) WHERE NOT EXISTS(m.summary) AND NOT EXISTS(m.content) DETACH DELETE m\")\n",
    "    graph.run(\"MATCH (m:Article:_AI) WHERE length(m.summary)>length(m.content) SET m.content = m.summary\")\n",
    "    graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.summary) AND NOT EXISTS(m.content) SET m.content = m.summary\")\n",
    "\n",
    "def process_documentsNeo4j():\n",
    "    \"\"\"\n",
    "    Purpose: Preprocess the content of the articles existing in the database.\n",
    "    Input: None.\n",
    "    Output: The database is updated. The processed text will be stored in the properties: 'preprocessed' and 'preprocessed_stemmed' of the nodes.\n",
    "    \"\"\"\n",
    "    preprocess_query = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.content) AND NOT EXISTS(m.preprocessed) RETURN m.content AS content, id(m) AS node_id\")\n",
    "    for item in preprocess_query:\n",
    "        processed_doc = preprocess_text(item['content'])\n",
    "        query = \"MATCH (m:Article:_AI) WHERE id(m) = $node_id SET m.preprocessed = $preproc\"\n",
    "        parameters = {'node_id': item['node_id'], 'preproc': processed_doc}\n",
    "        graph.run(query, parameters=parameters)\n",
    "        \n",
    "    preprocessstem_query = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) AND NOT EXISTS(m.preprocessed_stemmed) RETURN m.preprocessed AS preproc, id(m) AS node_id\")\n",
    "    for art in preprocessstem_query:\n",
    "        processed_stem_doc = stem_words(art['preproc'])\n",
    "        query = \"MATCH (m:Article:_AI) WHERE id(m) = $node_id SET m.preprocessed_stemmed = $preproc_stem\"\n",
    "        parameters = {'node_id': art['node_id'], 'preproc_stem': processed_stem_doc}\n",
    "        graph.run(query, parameters=parameters)\n",
    "    \n",
    "    #Clean the graph from empty articles\n",
    "    graph.run(\"MATCH (n:Article:_AI) WHERE EXISTS(n.content) AND n.preprocessed=[] DETACH DELETE n\")\n",
    "    graph.run(\"MATCH (n:Article:_AI) WHERE EXISTS(n.preprocessed) AND n.preprocessed_stemmed=[] DETACH DELETE n\")\n",
    "\n",
    "def clean_empty_processed_docs():\n",
    "    \"\"\"\n",
    "    Purpose: Clean the database from bad examples. In this case, empty content articles.\n",
    "    Input: None.\n",
    "    Output: The database in Neo4j is updated.\n",
    "    \"\"\"\n",
    "    graph.run(\"MATCH (n:Article:_AI) WHERE NOT EXISTS(n.content) DETACH DELETE n\")\n",
    "\n",
    "def evaluate_keywordsNeo4j():\n",
    "    \"\"\"\n",
    "    Purpose: Evaluate the key words defining the articles in the database.\n",
    "    Input: None.\n",
    "    Output: The database is updated in Neo4j. The results are stored in the node property: 'keywords'.\n",
    "    Note: This uses the TF-IDF model. Make sure it is up and running.\n",
    "    \"\"\"\n",
    "    \n",
    "    preprocess_query = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) AND NOT EXISTS(m.keywords) RETURN m.preprocessed_stemmed AS preproc_stem, id(m) AS node_id\")\n",
    "    for item in preprocess_query:\n",
    "        ptext_bow = corpus_memory_friendly.dictionary.doc2bow(item['preproc_stem'])\n",
    "        ptext_tfidf = tfidf[ptext_bow] #this is in local memory currently\n",
    "        #tfidf_doc = [item for item in ptext_tfidf]\n",
    "        keyw = sorted(ptext_tfidf, key=lambda item: -item[1])\n",
    "        idwords = []\n",
    "        idwordsview = []\n",
    "        for word in keyw:\n",
    "            (idw,tf) = word\n",
    "            if(tf >= 0.099):\n",
    "                idwords.append(idw)\n",
    "                if(tf >= 0.125):\n",
    "                    idwordsview.append(idw)\n",
    "        if not idwords:\n",
    "            (idw,tf) = keyw[0] #at least put one keyword\n",
    "            idwords.append(idw)\n",
    "            idwordsview.append(idw)\n",
    "\n",
    "        keywords = [corpus_memory_friendly.dictionary[idword] for idword in idwords]\n",
    "        keywords_viewer = [corpus_memory_friendly.dictionary[idwordv] for idwordv in idwordsview]\n",
    "        query = \"MATCH (m:Article:_AI) WHERE id(m) = $node_id SET m.keywords = $keywords, m.keywords_viewer = $keywords_view\"\n",
    "        parameters = {'node_id': item['node_id'], 'keywords': keywords, 'keywords_view': keywords_viewer}\n",
    "        graph.run(query, parameters=parameters)\n",
    "    \n",
    "def evaluate_keywordsNeo4jNST(): #NST stands for --> Non-STemmed\n",
    "    \"\"\"\n",
    "    Purpose: Evaluate the key words (Non-stemmed) defining the articles in the database.\n",
    "    Input: None.\n",
    "    Output: The database is updated in Neo4j. The results are stored in the node property: 'keywords_nst'.\n",
    "    Note: This uses the NST TF-IDF model. Make sure it is up and running.\n",
    "    \"\"\"\n",
    "    preprocess_query = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) AND NOT EXISTS(m.keywords_nst) RETURN m.preprocessed AS preproc, id(m) AS node_id\")\n",
    "    for item in preprocess_query:\n",
    "        ptext_bow = corpus_memory_friendly_NST.dictionary.doc2bow(item['preproc'])\n",
    "        ptext_tfidf = tfidf_nst[ptext_bow] #this is in local memory currently\n",
    "        keyw = sorted(ptext_tfidf, key=lambda item: -item[1])\n",
    "        idwords = []\n",
    "        idwordsview = []\n",
    "        for word in keyw:\n",
    "            (idw,tf) = word\n",
    "            if(tf >= 0.099):\n",
    "                idwords.append(idw)\n",
    "                if(tf >= 0.125):\n",
    "                    idwordsview.append(idw)\n",
    "        if not idwords:\n",
    "            (idw,tf) = keyw[0] #at least put one keyword\n",
    "            idwords.append(idw)\n",
    "            idwordsview.append(idw)\n",
    "\n",
    "        keywords = [corpus_memory_friendly_NST.dictionary[idword] for idword in idwords]\n",
    "        keywords_viewer = [corpus_memory_friendly_NST.dictionary[idwordv] for idwordv in idwordsview]\n",
    "        query = \"MATCH (m:Article:_AI) WHERE id(m) = $node_id SET m.keywords_nst = $keywords, m.keywords_viewer_nst = $keywords_view\"\n",
    "        parameters = {'node_id': item['node_id'], 'keywords': keywords, 'keywords_view': keywords_viewer}\n",
    "        graph.run(query, parameters=parameters)\n",
    "        \n",
    "def create_LiteId_documents():\n",
    "    \"\"\"\n",
    "    Purpose: Create a lite version of id's, only for the articles and that is sequential.\n",
    "    Input: None.\n",
    "    Output: The database in Neo4j is updated. The results are stored in the node property: 'liteId'.\n",
    "    Note: This serves to identify articles when using the models. It is very important that are sequential,\n",
    "          and that those id's coincide with the rows and columns of the similarity matrix for each article.\n",
    "          For more information, visit the gensim.similarities.docsim documentation:\n",
    "          https://radimrehurek.com/gensim/similarities/docsim.html at July 1st, 2019.\n",
    "    \"\"\"\n",
    "    \n",
    "    queryliteId = \"\"\"\n",
    "    MATCH (n:Article:_AI)\n",
    "    WITH range(coalesce(max(n.liteId)+1,0),count(n)-1,1) AS enum\n",
    "\n",
    "    MATCH (n:Article:_AI)\n",
    "    WHERE NOT EXISTS(n.liteId)\n",
    "    WITH enum, range(0,count(n)-1,1) AS index, collect(id(n)) AS id\n",
    "    UNWIND index AS indexes\n",
    "    WITH id[indexes] AS IDs, enum[indexes] AS ENUMs\n",
    "\n",
    "    MATCH (n:Article:_AI)\n",
    "    WHERE id(n)=IDs\n",
    "    SET n.liteId=ENUMs\n",
    "    \"\"\"\n",
    "    graph.run(queryliteId)\n",
    "    \n",
    "def check_documents():\n",
    "    \"\"\"\n",
    "    Purpose: Check the coherence of the Lite ID's.\n",
    "    Input: None.\n",
    "    Output:\n",
    "    Note: If the LiteID's are incoherent, it will raise a warning and the application will not let you continue.\n",
    "          This is for protecting the well-functioning. An incoherence here will provide bad recommendations, or even runtime errors, in some cases.\n",
    "          That is a very important link (as explained when creating the LiteID's).\n",
    "          Note that this is simple right now. It only checks that the amount of nodes with LiteID are the equal to the number of Article nodes\n",
    "          and that the largest LiteID in the database is the same as it should be for the amount of Article nodes in the database.\n",
    "          It does not check if the LiteID's are completely sequential or if there are duplicates. So in some missused cases, the coherence check\n",
    "          may come through, when the LiteID's are not sequential. This may cause Errors. Please, check this oftenly when modifying this property or creating a new graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    check = False\n",
    "    num_docs_liteid = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.liteId) RETURN count(m) AS count_liteid\").data()\n",
    "    last_num_liteid = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.liteId) RETURN m.liteId AS lite_id ORDER BY m.liteId DESC LIMIT 1\").data()\n",
    "    num_docs_total = graph.run(\"MATCH (m:Article:_AI) RETURN count(m) AS count_total\").data()\n",
    "    if(num_docs_liteid[0]['count_liteid'] == num_docs_total[0]['count_total']) and ((last_num_liteid[0]['lite_id']+1) == num_docs_total[0]['count_total']): check = True\n",
    "    return check\n",
    "\n",
    "def process_wordembeddingsNeo4j():\n",
    "    \"\"\"\n",
    "    Purpose: Store the pre-trained word embeddings from GloVe in the graph.\n",
    "    Input: CSV file downloaded from GloVe (https://nlp.stanford.edu/projects/glove/, at July 1st, 2019).\n",
    "    Output: The graph database is updated with a new type of node :Word. The word vectors are stored in the node property 'embedding'.\n",
    "    Note: The CSV file must contain a header called 'header'. This is for the GloVe vectors with 300 dimensions.\n",
    "          Please, modify the code if you are going to use a different pre-trained model.\n",
    "    \"\"\"\n",
    "    \n",
    "    query = \"\"\"\n",
    "    USING PERIODIC COMMIT 20000\n",
    "    LOAD CSV WITH HEADERS FROM 'file:///glove.42B.300d.csv' AS csvLine FIELDTERMINATOR \"↨\"\n",
    "    MERGE (w:Word:_AI {name:split(csvLine.header,\" \")[0]})\n",
    "    ON CREATE SET w.embedding = [x IN split(csvLine.header,' ')[1..301] | toFloat(x)]\n",
    "    \"\"\"\n",
    "    graph.run(\"CREATE CONSTRAINT ON (word:Word) ASSERT word.name IS UNIQUE\")\n",
    "    # Take into account that the character \" starting at the beginning of a line breaks the query\n",
    "    # replace every double quot at the beginning of a line for something else, like a single quot\n",
    "    graph.run(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Data and create a Dictionary of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpusDashNeo(object):\n",
    "    \"\"\"\n",
    "    Purpose: This is a class that represents both the pre-processed corpus (articles) and the dictionary of words.\n",
    "             Loads those from disk, if they exist. If not, the class creates and saves them in the computer.\n",
    "    Input:   None.\n",
    "    Output:  Object which is iterable and yields the pre-processed corpus. It can also be done tu use dictionary functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        if (os.path.isfile(dict_name) and not TRAINING):\n",
    "            self.process = False  \n",
    "        else:\n",
    "            self.process = True\n",
    "        \n",
    "        if self.process:\n",
    "            try:\n",
    "                print(\"Creating dictionary...\")\n",
    "                query_corpus = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) RETURN m.preprocessed_stemmed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "                self.dictionary = corpora.Dictionary(article['preprocessed'] for article in query_corpus)\n",
    "                once_ids = (tokenid for tokenid, docfreq in iteritems(self.dictionary.dfs) if docfreq == 1)\n",
    "                self.dictionary.filter_tokens(once_ids)\n",
    "                self.dictionary.compactify()\n",
    "                self.dictionary.save(dict_name)\n",
    "                print(\"Dictionary created.\")\n",
    "                print(self.dictionary)\n",
    "            except Exception as e:\n",
    "                print(\"Failed at creating the dictionary. Please check the dictionary generator.\")\n",
    "                print(\"Type of error: \" + str(e))\n",
    "                print(traceback.format_exc())\n",
    "            else:\n",
    "                try:\n",
    "                    corpora.MmCorpus.serialize(corpus_name, self)\n",
    "                except Exception as e:\n",
    "                    print(\"Error at serializing the corpus in memory. Please check the code snippet at the corpus serializer.\")\n",
    "                \n",
    "                try:\n",
    "                    self.__load_corpus()\n",
    "                except Exception as e:\n",
    "                    print(\"There was an error loading the corpus. Please, check the code.\")\n",
    "                else:\n",
    "                    self.process = False\n",
    "        \n",
    "        else:\n",
    "            try:\n",
    "                print(\"Loading dictionary...\")\n",
    "                self.dictionary = corpora.Dictionary.load(dict_name)\n",
    "                print(\"Dictionary loaded.\")\n",
    "                print(self.dictionary)\n",
    "            except Exception as e:\n",
    "                print(\"Failed at loading the dictionary. Please check the dictionary file.\")\n",
    "                print(\"Type of error: \" + str(e))\n",
    "                print(traceback.format_exc())\n",
    "            else:\n",
    "                self.__load_corpus()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.process:\n",
    "            try:\n",
    "                query_corpus = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) RETURN m.preprocessed_stemmed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "                for idc,art in enumerate(query_corpus):\n",
    "                    print(\"Building model: \" + str(idc+1) + \"/\" + str(self.dictionary.num_docs), end='\\r') # + \"\\r\"\n",
    "                    yield self.dictionary.doc2bow(art['preprocessed'])\n",
    "                print('\\n')\n",
    "            except Exception as e:\n",
    "                print(\"Failed at processing the corpus. Please check the transformation dict-->corpus and/or the Neo4j query.\")\n",
    "                print(\"Check also that Neo4j is open and running.\")\n",
    "                print(\"Type of error: \" + str(e))\n",
    "                print(traceback.format_exc())\n",
    "                print(\"Should I run an old file?\")\n",
    "                # Run an old file if it exists and fails?\n",
    "        else:\n",
    "            try:\n",
    "                for artitem in self.corpus:\n",
    "                    yield artitem\n",
    "            except Exception as e:\n",
    "                print(\"The generator has failed at yielding the corpus documents. Check the iterator of the corpus.\")\n",
    "                print(\"Type of error: \" + str(e))\n",
    "                print(traceback.format_exc())\n",
    "                \n",
    "    def __load_corpus(self):\n",
    "        try:\n",
    "            print(\"Loading corpus...\")\n",
    "            self.corpus = corpora.MmCorpus(corpus_name)\n",
    "            print(\"Corpus loaded.\")\n",
    "            print(self.corpus)\n",
    "        except Exception as e:\n",
    "            print(\"Failed at loading the corpus. Please check that the corpus file is correct.\")\n",
    "            print(\"Type of error: \" + str(e))\n",
    "            print(traceback.format_exc())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpusNeoNST(object):\n",
    "    \"\"\"\n",
    "    Purpose: (Non-Stemmed Version) This is a class that represents both the pre-processed corpus (articles) and the dictionary of words.\n",
    "             Loads those from disk, if they exist. If not, the class creates and saves them in the computer.\n",
    "    Input:   None.\n",
    "    Output:  Object which is iterable and yields the pre-processed corpus. It can also be done tu use dictionary functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        if (os.path.isfile(dict_name_nst) and not TRAINING):\n",
    "            self.process = False  \n",
    "        else:\n",
    "            self.process = True\n",
    "        \n",
    "        if self.process:\n",
    "            try:\n",
    "                print(\"Creating dictionary not stemmed...\")\n",
    "                query_corpus = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.preprocessed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "                self.dictionary = corpora.Dictionary(article['preprocessed'] for article in query_corpus)\n",
    "                once_ids = (tokenid for tokenid, docfreq in iteritems(self.dictionary.dfs) if docfreq == 1)\n",
    "                self.dictionary.filter_tokens(once_ids)\n",
    "                self.dictionary.compactify()\n",
    "                self.dictionary.save(dict_name_nst)\n",
    "                print(\"Dictionary created.\")\n",
    "                print(self.dictionary)\n",
    "            except Exception as e:\n",
    "                print(\"Failed at creating the dictionary. Please check the dictionary generator.\")\n",
    "                print(\"Type of error: \" + str(e))\n",
    "                print(traceback.format_exc())\n",
    "            else:\n",
    "                try:\n",
    "                    corpora.MmCorpus.serialize(corpus_name_nst, self)\n",
    "                except Exception as e:\n",
    "                    print(\"Error at serializing the corpus in memory. Please check the code snippet at the corpus serializer.\")\n",
    "                \n",
    "                try:\n",
    "                    self.__load_corpus()\n",
    "                except Exception as e:\n",
    "                    print(\"There was an error loading the corpus. Please, check the code.\")\n",
    "                else:\n",
    "                    self.process = False\n",
    "        \n",
    "        else:\n",
    "            try:\n",
    "                print(\"Loading dictionary...\")\n",
    "                self.dictionary = corpora.Dictionary.load(dict_name_nst)\n",
    "                print(\"Dictionary loaded.\")\n",
    "                print(self.dictionary)\n",
    "            except Exception as e:\n",
    "                print(\"Failed at loading the dictionary. Please check the dictionary file.\")\n",
    "                print(\"Type of error: \" + str(e))\n",
    "                print(traceback.format_exc())\n",
    "            else:\n",
    "                self.__load_corpus()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.process:\n",
    "            try:\n",
    "                query_corpus = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.preprocessed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "                for idc,art in enumerate(query_corpus):\n",
    "                    print(\"Building model: \" + str(idc+1) + \"/\" + str(self.dictionary.num_docs), end='\\r') # + \"\\r\"\n",
    "                    yield self.dictionary.doc2bow(art['preprocessed'])\n",
    "                print('\\n')\n",
    "            except Exception as e:\n",
    "                print(\"Failed at processing the corpus. Please check the transformation dict-->corpus and/or the Neo4j query.\")\n",
    "                print(\"Check also that Neo4j is open and running.\")\n",
    "                print(\"Type of error: \" + str(e))\n",
    "                print(traceback.format_exc())\n",
    "                print(\"Should I run an old file?\")\n",
    "                # Run an old file if it exists and fails?\n",
    "        else:\n",
    "            try:\n",
    "                for artitem in self.corpus:\n",
    "                    yield artitem\n",
    "            except Exception as e:\n",
    "                print(\"The generator has failed at yielding the corpus documents. Check the iterator of the corpus.\")\n",
    "                print(\"Type of error: \" + str(e))\n",
    "                print(traceback.format_exc())\n",
    "                \n",
    "    def __load_corpus(self):\n",
    "        try:\n",
    "            print(\"Loading corpus...\")\n",
    "            self.corpus = corpora.MmCorpus(corpus_name_nst)\n",
    "            print(\"Corpus loaded.\")\n",
    "            print(self.corpus)\n",
    "        except Exception as e:\n",
    "            print(\"Failed at loading the corpus. Please check that the corpus file is correct.\")\n",
    "            print(\"Type of error: \" + str(e))\n",
    "            print(traceback.format_exc())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "class WMD(object):\n",
    "    \"\"\"\n",
    "    Purpose: The class computes an optimization process called Word Moving Distance (WMD)\n",
    "    Input:   None.\n",
    "    Output:  It computes the minimum euclidean distance between two articles using the WMD method.\n",
    "    Note:    For more information, read the paper: \"From Word Embeddings To Document Distances\", by Matt J. Kusner et al. (2015)\n",
    "             The words should not be stemmed for this method, as GloVe do not contain embeddings for stemmed words.\n",
    "             Make sure that you are using the NST-version of the models to compute this one.\n",
    "             \n",
    "    Note2:   This is currently computing the Relaxed Word Moving Distance (RWMD) version, for a reduced computing time.\n",
    "             If you would like to use the standard version of the WMD, use the commented second constraint in the code.\n",
    "             Beware that the computation time will increase and the convergence might not happen during optimization.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaded = False\n",
    "    def load_docs(self, text1, text2):\n",
    "        #Parameters\n",
    "        MAX_WORDS = 20 # tunable: could be 20, 30, 50, 70... also using TF-IDF or not\n",
    "        USING_TFIDF = True\n",
    "        \n",
    "        if(USING_TFIDF):\n",
    "            bow_1 = corpus_memory_friendly_NST.dictionary.doc2bow(text1)\n",
    "            bow_1 = tfidf_nst[bow_1] # OBS! careful you have to make sure this is the non-stemmed one all the time!\n",
    "            bow_2 = corpus_memory_friendly_NST.dictionary.doc2bow(text2)\n",
    "            bow_2 = tfidf_nst[bow_2] # OBS! careful you have to make sure this is the non-stemmed one all the time!\n",
    "        else:\n",
    "            # dict_1 --> nbow_1 --> self.d_1\n",
    "            dict_1 = corpora.Dictionary([text1])\n",
    "            bow_1 = dict_1.doc2bow(text1)\n",
    "            # dict_2 --> nbow_2 --> self.d_2\n",
    "            dict_2 = corpora.Dictionary([text2])\n",
    "            bow_2 = dict_2.doc2bow(text2)\n",
    "            \n",
    "        bow_1 = sorted(bow_1, key=lambda x: -x[1])[:MAX_WORDS]\n",
    "        bow_2 = sorted(bow_2, key=lambda x: -x[1])[:MAX_WORDS]\n",
    "        \n",
    "        idw_1 = np.array([it for it,val in bow_1])\n",
    "        nbow_1 = np.array([val for it,val in bow_1])\n",
    "        self.d_1 = nbow_1/np.sum(nbow_1)\n",
    "        idw_2 = np.array([it for it,val in bow_2])\n",
    "        nbow_2 = np.array([val for it,val in bow_2])\n",
    "        self.d_2 = nbow_2/np.sum(nbow_2)\n",
    "        n = len(self.d_1) #length of Text 1\n",
    "        m = len(self.d_2) #length of Text 2\n",
    "        keep_list_1 = list(range(n))\n",
    "        keep_list_2 = list(range(m))\n",
    "        \n",
    "        c = np.random.rand(n,m)*10000\n",
    "        for pos1, idword1 in enumerate(idw_1):\n",
    "            if(USING_TFIDF): word1 = corpus_memory_friendly_NST.dictionary[idword1]\n",
    "            else: word1 = dict_1[idword1]\n",
    "            emb1 = graph.run(\"MATCH (m:Word:_AI {name: $word_1}) RETURN m.embedding AS embedding LIMIT 1\", parameters={'word_1': word1}).data()\n",
    "            if(emb1):\n",
    "                emb1 = emb1[0]['embedding']\n",
    "                for pos2, idword2 in enumerate(idw_2):\n",
    "                    if(USING_TFIDF): word2 = corpus_memory_friendly_NST.dictionary[idword2]\n",
    "                    else: word2 = dict_2[idword2]\n",
    "                    emb2 = graph.run(\"MATCH (m:Word:_AI {name: $word_2}) RETURN m.embedding AS embedding LIMIT 1\", parameters={'word_2': word2}).data()\n",
    "                    if(emb2):\n",
    "                        emb2 = emb2[0]['embedding']\n",
    "                        dist = euclidean_distances([emb1,emb2])[0,1]\n",
    "                        c[pos1,pos2] = dist\n",
    "                    else:\n",
    "                        if(pos2 in keep_list_2): keep_list_2.remove(pos2)\n",
    "            else: \n",
    "                if(pos1 in keep_list_1): keep_list_1.remove(pos1)\n",
    "        \n",
    "        c = c[keep_list_1,:]\n",
    "        c = c[:,keep_list_2]\n",
    "        \n",
    "        # custom dictionary for the two documents?\n",
    "        self.n,self.m = c.shape #length of Text1, Text2\n",
    "        \n",
    "        self.d_1 = np.ones(self.n) #this is only for single word texts: this needs to be bow\n",
    "        self.d_2 = np.ones(self.n) #this is only for single word texts: this needs to be bow\n",
    "\n",
    "        self.c = np.transpose(c.flatten())\n",
    "        self.loaded = True\n",
    "        \n",
    "    #####################\n",
    "    ## Paper Doc Dist. ##\n",
    "    #####################\n",
    "    #Objective\n",
    "    def __objective(self,T):\n",
    "        cost_function = np.dot(T,self.c)\n",
    "        return cost_function\n",
    "\n",
    "    #Constraints\n",
    "    def __constr1(self,x):\n",
    "        jc = x.reshape(self.n,self.m)\n",
    "        jc = -np.sum(jc, axis=1)\n",
    "        l = np.add(jc,self.d_1)\n",
    "        return l\n",
    "\n",
    "    def __constr2(self,x):\n",
    "        jc2 = x.reshape(self.n,self.m)\n",
    "        jc2 = -np.sum(jc2, axis=0)\n",
    "        l2 = np.add(jc2,self.d_2)\n",
    "        return l2\n",
    "    \n",
    "    #Calculate the document distances\n",
    "    def calculate_distance(self):\n",
    "        if(self.loaded):\n",
    "            cons = [{'type': 'eq', 'fun': self.__constr1}] # , {'type': 'eq', 'fun': constr2}\n",
    "            T0 = np.random.rand(self.n,self.m)\n",
    "            T0 = T0.flatten()\n",
    "            b = (0.0,None) # Bounds for the transformations\n",
    "            bnds = (b,)*self.n*self.m\n",
    "            sol = minimize(self.__objective, T0, method='SLSQP', bounds=bnds, constraints=cons, tol=1e-4)\n",
    "            return sol\n",
    "        else:\n",
    "            print(\"You need to load the documents you want to calculate the distance of first.\\nUse the function 'load_docs(doc1,doc2)' for that.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npapertest = WMD()\\ndoc_1 = graph.run(\"MATCH (m:Article:_AI)-[]-(n:LtsFocusArea:_AI {name:\\'Autonomous Drive\\'}) RETURN m.title AS title, m.preprocessed AS prep LIMIT 1\").data()[0]\\ndoc_2 = graph.run(\"MATCH (m:Article:_AI)-[]-(n:Megatrend:_AI {name:\\'Technology development\\'}) RETURN m.title AS title, m.preprocessed AS prep LIMIT 1\").data()[0]\\nprint(doc_1[\\'title\\'], doc_2[\\'title\\'])\\npapertest.load_docs(doc_2[\\'prep\\'], doc_1[\\'prep\\'])\\ndistance = papertest.calculate_distance()\\nprint(\"Distance: \", distance.fun)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "papertest = WMD()\n",
    "doc_1 = graph.run(\"MATCH (m:Article:_AI)-[]-(n:LtsFocusArea:_AI {name:'Autonomous Drive'}) RETURN m.title AS title, m.preprocessed AS prep LIMIT 1\").data()[0]\n",
    "doc_2 = graph.run(\"MATCH (m:Article:_AI)-[]-(n:Megatrend:_AI {name:'Technology development'}) RETURN m.title AS title, m.preprocessed AS prep LIMIT 1\").data()[0]\n",
    "print(doc_1['title'], doc_2['title'])\n",
    "papertest.load_docs(doc_2['prep'], doc_1['prep'])\n",
    "distance = papertest.calculate_distance()\n",
    "print(\"Distance: \", distance.fun)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_recommendations_single_article_Neo4j(art, recom, algorithm):\n",
    "    \"\"\"\n",
    "    Purpose: The function stores the recommendations based on an algorithm for one certain article defined. \n",
    "    Input:   art        - Refers to the current document liteID to store the recommendations for.\n",
    "             recom      - Refers to the recommendations performed by the algorithm for the 'art' document.\n",
    "             algorithm  - Refers to the algorithm used to create the recommendation.\n",
    "    Output:  None. The recommendations will be stored in the graph database.\n",
    "    \"\"\"\n",
    "    recommendations = []\n",
    "    for rank,recomid in enumerate(recom):\n",
    "        recommendations.append({'recom': recomid, 'rank': rank+1})\n",
    "        \n",
    "    query=\"\"\"\n",
    "    WITH $list_recommendations AS list\n",
    "    UNWIND list AS recommendations\n",
    "    MATCH (sou:Article:_AI {liteId: $query_liteid})\n",
    "    MATCH (rec:Article:_AI {liteId: recommendations.recom})\n",
    "    MERGE (sou)-[REL:%s]->(rec)\n",
    "    SET REL.rank = recommendations.rank\n",
    "    \"\"\" % (algorithm)\n",
    "    \n",
    "    graph.run(query, parameters={'query_liteid': art, 'list_recommendations': recommendations})\n",
    "    \n",
    "\n",
    "def store_recommendations_Neo4j():\n",
    "    \"\"\"\n",
    "    Purpose: The function will estimate the recommendations for each one of the articles in the database using the models.\n",
    "    Input:   None. Uses the database.\n",
    "    Output:  None. The recommendations will be stored in the graph database.\n",
    "    \"\"\"\n",
    "    all_artic = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.liteId AS lite_id ORDER BY m.liteId ASC\").data()\n",
    "    for article in all_artic:\n",
    "        #TF-IDF\n",
    "        rec = find_similardocs_tfidf(doc=article['lite_id'], return_results=True)\n",
    "        store_recommendations_single_article_Neo4j(article['lite_id'],rec,algorithm='TF_IDF')\n",
    "        \n",
    "        #LSA\n",
    "        rec = find_similardocs_lsi(doc=article['lite_id'], return_results=True)\n",
    "        store_recommendations_single_article_Neo4j(article['lite_id'],rec,algorithm='LSA')\n",
    "        \n",
    "        #LDA\n",
    "        #rec = find_similardocs_lda(doc=article['lite_id'], return_results=True)\n",
    "        #store_recommendations_single_article_Neo4j(article['lite_id'],rec,algorithm='LDA')\n",
    "        \n",
    "        #Word Embeddings\n",
    "        rec = find_similardocs_WE(doc=article['lite_id'], return_results=True)\n",
    "        store_recommendations_single_article_Neo4j(article['lite_id'],rec,algorithm='WORD_EMBEDDINGS')\n",
    "        \n",
    "def merge_recommendations_Neo4j():\n",
    "    \"\"\"\n",
    "    Purpose: The function will merge the recommendations among the articles in the database using the algorithms' recommendations.\n",
    "    Input:   None. Uses the database.\n",
    "    Output:  None. The recommendations will be stored in the graph database as a :RELATES_TO.\n",
    "    \"\"\"\n",
    "    query=\"\"\"\n",
    "    MATCH (a1:Article:_AI)-->(a2:Article_AI)\n",
    "    WHERE a1 <> a2\n",
    "    OPTIONAL MATCH (a1)-[r1:WORD_EMBEDDINGS]->(a2)\n",
    "    OPTIONAL MATCH (a1)-[r2:LSA]->(a2)\n",
    "    OPTIONAL MATCH (a1)-[r3:TF_IDF]->(a2)\n",
    "    MERGE (a1)-[com:RELATES_TO]->(a2)\n",
    "    SET com.weight = (toFloat((10-coalesce(r1.rank,10)) + (10-coalesce(r2.rank,10)) + (10-coalesce(r3.rank,10)))) / 27\n",
    "    \"\"\"\n",
    "    graph.run(query) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GUI Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import webbrowser\n",
    "from tkinter import font\n",
    "from PIL import Image, ImageTk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dictionary...\n",
      "Dictionary loaded.\n",
      "Dictionary(6878 unique tokens: ['2025', '32', '3d', 'abstract', 'accord']...)\n",
      "Loading corpus...\n",
      "Corpus loaded.\n",
      "MmCorpus(374 documents, 6878 features, 97868 non-zero entries)\n",
      "Loading dictionary...\n",
      "Dictionary loaded.\n",
      "Dictionary(10169 unique tokens: ['2025', '32', '3d', 'according', 'across']...)\n",
      "Loading corpus...\n",
      "Corpus loaded.\n",
      "MmCorpus(374 documents, 10169 features, 107162 non-zero entries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\illopis\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\similarities\\docsim.py:528: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  result = numpy.hstack(shard_results)\n"
     ]
    }
   ],
   "source": [
    "root = tk.Tk()\n",
    "root.title(\"Horizon Scanning AI GUI\")\n",
    "root.resizable(False,False)\n",
    "HEIGHT = 922\n",
    "WIDTH = 786\n",
    "\n",
    "TEXT_FONT = \"Volvo Serif Pro\"\n",
    "FONT_ARTICLES = 'Volvo Novum'\n",
    "FONT_NOT_FOUND = \"Volvo Novum Medium\"\n",
    "no_article = -1\n",
    "no_article_sim = -1\n",
    "VIEW_DOCUMENT = False\n",
    "COMPUTE_COHERENCE = False\n",
    "COMPUTE_RECALL = True\n",
    "ocult_train = True\n",
    "algorithm_training = False\n",
    "\n",
    "#Messages through GUI List of typical functions\n",
    "def not_implemented_message():\n",
    "    output_screen['text'] = \"This method has not been implemented yet.\\nPlease, give time to the engineer responsible.\"\n",
    "\n",
    "def showing_recommendations(algorithm):\n",
    "    output_screen['text'] = \"Showing recommendations based on:\\n\" + str(algorithm)\n",
    "    \n",
    "def not_valid_message():\n",
    "    output_screen['text'] = \"The number entered is not valid.\\nPlease, enter a valid amount.\"\n",
    "    \n",
    "def algorithm_not_found_message():\n",
    "    output_screen['text'] = \"It seems the engineer responsible has a non-existing algorithm.\\nPlease, review this problem.\"\n",
    "    \n",
    "def article_not_selected_message():\n",
    "    output_screen['text'] = \"Please, select the article you want to find similarities to.\"\n",
    "    \n",
    "def cannot_be_implemented():\n",
    "    output_screen['text'] = \"This method cannot be implemented\\nfrom the backend currently.\\nPlease, use Neo4j for such analysis.\"\n",
    "    \n",
    "def clean_spacelines(text):\n",
    "    rgx_clsp = re.compile(r'(\\n)+')\n",
    "    tgx_cltb = re.compile(r'(\\t)+')\n",
    "    sgx_clsp = re.compile(r'(\\s)+')\n",
    "    cltext = re.sub(rgx_clsp,'\\n',text)\n",
    "    cltext = re.sub(tgx_cltb,'\\t',cltext)\n",
    "    cltext = re.sub(sgx_clsp,' ',cltext)\n",
    "    return cltext\n",
    "\n",
    "\n",
    "#Functions for calculating relations, distances and training models based on metrics\n",
    "def train_algorithm():\n",
    "    \"\"\"\n",
    "    Purpose: The main function to handle training of algorithms.\n",
    "    Input:   None.\n",
    "    Output:  None. The models will be saved in disk, if applicable.\n",
    "    Note:    Some algorithms are using pre-trained models. However this function will still calculate the distance and relations\n",
    "             existing among the articles (euclidean distance or cosine similarity).\n",
    "    \"\"\"\n",
    "    global algorithm_training, stemmed_tfidf\n",
    "    if(algorithmvariable.get()==\"Word Embeddings\"):\n",
    "        output_screen['text'] = \"Training for Word Embeddings...\\nPlease wait, it may take long.\"\n",
    "        train_wordembeddings()\n",
    "        output_screen['text'] = \"Training completed.\"\n",
    "    elif(algorithmvariable.get()==\"TF-IDF\"):\n",
    "        pass\n",
    "    elif(algorithmvariable.get()==\"Doc2vec\"):\n",
    "        not_implemented_message()\n",
    "    elif(algorithmvariable.get()==\"LSA\"):\n",
    "        algorithm_training = True\n",
    "        if(COMPUTE_RECALL):\n",
    "            stemmed_tfidf = True\n",
    "            recall_scores, RBP_scores, RBPacc_scores, start, step, stop = maximum_recall_score('lsa')\n",
    "            stemmed_tfidf = False\n",
    "            recall_scores_nst, RBP_scores_nst, RBPacc_scores_nst, start, step, stop = maximum_recall_score('lsa')\n",
    "            x = range(start, stop+1, step)\n",
    "            plt.plot(x, recall_scores, 'g', x, recall_scores_nst, 'c')\n",
    "            plt.xlabel(\"Number of Topics\")\n",
    "            plt.ylabel(\"Recall score\")\n",
    "            plt.title(\"Recall of LSA models (stemmed tokens vs non-stemmed tokens)\")\n",
    "            plt.legend((\"Stemmed-words\", \"NST-words\"), loc='best')\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(x, RBP_scores, 'g', x, RBP_scores_nst, 'c')\n",
    "            plt.xlabel(\"Number of Topics\")\n",
    "            plt.ylabel(\"Rank-biased precision (RBP)\")\n",
    "            plt.title(\"RBP score of LSA models (stemmed tokens vs non-stemmed tokens)\")\n",
    "            plt.legend((\"Stemmed-words\", \"NST-words\"), loc='best')\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(x, RBPacc_scores, 'g', x, RBPacc_scores_nst, 'c')\n",
    "            plt.xlabel(\"Number of Topics\")\n",
    "            plt.ylabel(\"Rank-biased precision x recall (RBPacc)\")\n",
    "            plt.title(\"RBP x recall of LSA models (stemmed tokens vs non-stemmed tokens)\")\n",
    "            plt.legend((\"Stemmed-words\", \"NST-words\"), loc='best')\n",
    "            plt.show()\n",
    "        if(COMPUTE_COHERENCE): coherence_model('lsa')\n",
    "        algorithm_training = False\n",
    "    elif(algorithmvariable.get()==\"LDA\"):\n",
    "        algorithm_training = True\n",
    "        if(COMPUTE_RECALL):\n",
    "            stemmed_tfidf = True\n",
    "            recall_scores, RBP_scores, RBPacc_scores, start, step, stop = maximum_recall_score('lda')\n",
    "            stemmed_tfidf = False\n",
    "            recall_scores_nst, RBP_scores_nst, RBPacc_scores_nst, start, step, stop = maximum_recall_score('lda')\n",
    "            x = range(start, stop+1, step)\n",
    "            plt.plot(x, recall_scores, 'g', x, recall_scores_nst, 'y')\n",
    "            plt.xlabel(\"Number of Topics\")\n",
    "            plt.ylabel(\"Recall score\")\n",
    "            plt.title(\"Recall of LDA models (stemmed tokens vs non-stemmed tokens)\")\n",
    "            plt.legend((\"Stemmed-words\", \"NST-words\"), loc='best')\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(x, RBP_scores, 'g', x, RBP_scores_nst, 'y')\n",
    "            plt.xlabel(\"Number of Topics\")\n",
    "            plt.ylabel(\"Rank-biased precision (RBP)\")\n",
    "            plt.title(\"RBP of LSA models (stemmed tokens vs non-stemmed tokens)\")\n",
    "            plt.legend((\"Stemmed-words\", \"NST-words\"), loc='best')\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(x, RBPacc_scores, 'g', x, RBPacc_scores_nst, 'y')\n",
    "            plt.xlabel(\"Number of Topics\")\n",
    "            plt.ylabel(\"Rank-biased precision x recall (RBPacc)\")\n",
    "            plt.title(\"RBP x recall of LDA models (stemmed tokens vs non-stemmed tokens)\")\n",
    "            plt.legend((\"Stemmed-words\", \"NST-words\"), loc='best')\n",
    "            plt.show()\n",
    "        if(COMPUTE_COHERENCE): coherence_model('lda')\n",
    "        algorithm_training = False\n",
    "    elif(algorithmvariable.get()==\"Ensemble Method\"):\n",
    "        cannot_be_implemented()\n",
    "    elif(algorithmvariable.get()==\"Community Finding\"):\n",
    "        cannot_be_implemented()\n",
    "    else:\n",
    "        algorithm_not_found_message()\n",
    "\n",
    "    \n",
    "def train_wordembeddings():\n",
    "    \"\"\"\n",
    "    Purpose: Compute the similarity matrix to relate documents based on Word Embeddings\n",
    "    Input:   Word Embeddings, pre-processed documents\n",
    "    Output:  Serialized similarity matrix for documents\n",
    "    \n",
    "    Modes:   paper - Document distance by word embeddings using the Word Moving Distance method.\n",
    "                (Note!: This method takes a lot of time! Beware of this.\n",
    "                 Also, I am saving the results as we go in a txt file, so we can retrieve\n",
    "                 the computed results and not start from the beginning if it breaks).\n",
    "             title - Document distance and similarity of documents by word embeddings\n",
    "                     of words appearing in the Title.\n",
    "             glove - Document distance and similarity of documents by word embeddings using GloVe embeddings.\n",
    "             berglove - Document distance and similarity of documents by word embeddings using GloVe and BERT\n",
    "                        pre-trained embeddings.\n",
    "             \n",
    "    \"\"\"\n",
    "    mode = 'paper' #choose from\n",
    "    num_articles = graph.run(\"MATCH (n:Article:_AI) WHERE EXISTS(n.preprocessed) RETURN count(n) AS total\").data()[0]['total']\n",
    "    \n",
    "    def serialization(documents_embeddings, file_name_cosine_similarity, file_name_euclidean_distance):\n",
    "        ################################\n",
    "        ###      Serialization       ###\n",
    "        ################################\n",
    "        if(isinstance(documents_embeddings,list) and isinstance(file_name_cosine_similarity,str) and isinstance(file_name_euclidean_distance,str)):\n",
    "            try:\n",
    "                print(\"Initializing serialization...\")\n",
    "                #Cosine similarity:\n",
    "                cosim = cosine_similarity(X=documents_embeddings)\n",
    "                cosim.dump(file_name_cosine_similarity)\n",
    "\n",
    "                #Euclidean distance: to tune in many ways\n",
    "                euclidean = euclidean_distances(X=documents_embeddings)\n",
    "                euclidean.dump(file_name_euclidean_distance)\n",
    "                print(\"Finished serialization.\")\n",
    "            except:\n",
    "                print(\"Something went wrong during serialization.\")\n",
    "        else:\n",
    "            print(\"Bad format for the serialization. Please, check the data structure inputted for this.\")\n",
    "        \n",
    "        \n",
    "    if(mode=='glove'):\n",
    "        embed_dim = graph.run(\"MATCH (m:Word:_AI) RETURN size(m.embedding) AS size LIMIT 1\").data()[0]['size']\n",
    "        documents = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.liteId AS lite_id, m.preprocessed AS preprocessed ORDER BY m.liteId ASC\").data()\n",
    "        total_percentage_glove = 0.0\n",
    "        documents_embeddings = [] #this is the WE-based Doc2Vec for the documents in the database\n",
    "        for doc in documents:\n",
    "            sum_embedding = np.zeros(shape=(embed_dim), dtype=float)\n",
    "            count_words = 0\n",
    "            total_words = 0\n",
    "            for word in doc['preprocessed']:\n",
    "                we = graph.run(\"MATCH (n:Word:_AI {name: $word}) RETURN n.embedding AS embedding\", parameters={'word':word}).data()\n",
    "                total_words += 1\n",
    "                if we:\n",
    "                    sum_embedding += we[0]['embedding']\n",
    "                    count_words += 1\n",
    "            if(count_words > 0):\n",
    "                total_percentage_glove += count_words/total_words\n",
    "                doc_embed = sum_embedding/count_words\n",
    "            else: doc_embed = sum_embedding*count_words\n",
    "            documents_embeddings.append(doc_embed)\n",
    "        serialization(documents_embeddings=documents_embeddings,file_name_cosine_similarity=index_WE_model_cs_name_glove, file_name_euclidean_distance=index_WE_model_eu_name_glove)\n",
    "        print(\"Percentage of words in GloVe: \" + str(total_percentage_glove/num_articles))           \n",
    "    \n",
    "    \n",
    "    if(mode=='title'): \n",
    "        documents = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.liteId AS lite_id, m.preprocessed AS preprocessed, m.keywords AS keywords, m.title AS title ORDER BY m.liteId ASC\").data()\n",
    "        total_percentage_glove = 0.0\n",
    "        sim_test_title = []\n",
    "        for doc in documents:\n",
    "            sum_embedding = np.zeros(shape=(embed_dim), dtype=float)\n",
    "            count_words = 0\n",
    "            total_words = 0\n",
    "            title = preprocess_text(doc['title'])\n",
    "            for word in doc['preprocessed']:\n",
    "                if(word in title):\n",
    "                    we = graph.run(\"MATCH (n:Word:_AI {name: $word}) RETURN n.embedding AS embedding\", parameters={'word':word}).data()\n",
    "                    total_words += 1\n",
    "                    if we:\n",
    "                        sum_embedding += we[0]['embedding']\n",
    "                        count_words += 1\n",
    "            if(count_words == 0): doc_embed = sum_embedding*count_words\n",
    "            else:\n",
    "                doc_embed = sum_embedding/count_words\n",
    "                total_percentage_glove += count_words/total_words\n",
    "            sim_test_title.append(doc_embed)\n",
    "        serialization(documents_embeddings=sim_test_title, file_name_cosine_similarity=index_WE_model_cs_title, file_name_euclidean_distance=index_WE_model_eu_title)\n",
    "        print(\"Percentage of words in GloVe: \" + str(total_percentage_glove/num_articles))\n",
    "        \n",
    "    \n",
    "    if(mode=='berglove'):\n",
    "        glove_embedding = WordEmbeddings('en-glove')\n",
    "        bert_embedding = BertEmbeddings('bert-large-uncased')\n",
    "        #multi_forward = FlairEmbeddings('multi-forward')\n",
    "        #multi_backward = FlairEmbeddings('multi-backward')\n",
    "        stacked_embeddings = StackedEmbeddings([\n",
    "                                                glove_embedding,\n",
    "                                                bert_embedding,\n",
    "                                                #FlairEmbeddings('news-forward'), \n",
    "                                                #FlairEmbeddings('news-backward'),\n",
    "                                                #multi_forward, \n",
    "                                                #multi_backward,\n",
    "                                               ])\n",
    "\n",
    "        # Embedding dimension\n",
    "        se = Sentence('grass')\n",
    "        stacked_embeddings.embed(se)\n",
    "        embed_dim = len(se[0].embedding)\n",
    "        documents = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.liteId AS lite_id, m.preprocessed AS preprocessed ORDER BY m.liteId ASC\").data()\n",
    "        num_doc = 1\n",
    "        documents_embeddings = [] #this is the WE-based Doc2Vec for the documents in the database\n",
    "        for doc in documents:\n",
    "            sentences = []\n",
    "            sum_embedding = np.zeros(shape=(embed_dim), dtype=float)\n",
    "            print(\"Computing Doc WE embedding: \" + str(num_doc) + \"/\" + str(num_articles), end=\"\\r\")\n",
    "            count_words = 0\n",
    "            sen = \"\"\n",
    "            for word in doc['preprocessed']:\n",
    "                temp = sen + word + \" \"\n",
    "                if(len(temp)>512):\n",
    "                    sentences.append(sen)\n",
    "                    sen = word + \" \"\n",
    "                else: sen = temp\n",
    "            sentences.append(sen)\n",
    "            for sen in sentences:\n",
    "                sentence = Sentence(sen)\n",
    "                stacked_embeddings.embed(sentence)\n",
    "                for token in sentence:\n",
    "                    sum_embedding += np.array(token.embedding)\n",
    "                    count_words += 1\n",
    "            if(count_words > 0):\n",
    "                doc_embed = sum_embedding/count_words\n",
    "            else: doc_embed = sum_embedding*0\n",
    "            documents_embeddings.append(doc_embed)\n",
    "            num_doc += 1\n",
    "        print(\"Computing Doc WE embedding: \" + str(num_doc) + \"/\" + str(num_articles), end=\"\\n\")\n",
    "        serialization(documents_embeddings=documents_embeddings,file_name_cosine_similarity=index_WE_model_cs_name_bert_glove, file_name_euclidean_distance=index_WE_model_eu_name_bert_glove)\n",
    "    \n",
    "    if(mode=='paper'):\n",
    "        #Paper Document Distances\n",
    "        time_ini = time.time()\n",
    "        word_mover_distance = WMD()\n",
    "        document_distances_paper = np.random.rand(num_articles,num_articles)*10000\n",
    "        counter = 0\n",
    "        total_to_count = int(num_articles**2)\n",
    "        doclist1 = graph.run(\"MATCH (m:Article:_AI) RETURN m.liteId AS lite_id, m.preprocessed AS preprocessed ORDER BY m.liteId ASC\").data()\n",
    "        for idd1,doc1 in enumerate(doclist1):\n",
    "            doclist2 = graph.run(\"MATCH (m:Article:_AI) RETURN m.liteId AS lite_id, m.preprocessed AS preprocessed ORDER BY m.liteId ASC\").data()\n",
    "            for idd2,doc2 in enumerate(doclist2):\n",
    "                percentage_proc = counter/total_to_count*100\n",
    "                print(\"Calculating document distance \" + str(idd1+1) + \" --> \" + str(idd2+1) + \"\\t Total num. of articles: \" + str(num_articles) + \" ({0:.1f}%)\".format(percentage_proc), end='\\r')\n",
    "                word_mover_distance.load_docs(doc1['preprocessed'],doc2['preprocessed'])\n",
    "                sol = word_mover_distance.calculate_distance()\n",
    "                document_distances_paper[idd1,idd2] = sol.fun\n",
    "                f = open(\"paper_distances.txt\", \"a\")\n",
    "                f.write(str(idd1) + \" \" + str(idd2) + \" \" + str(sol.fun) + \"\\n\")\n",
    "                f.close()\n",
    "                counter += 1\n",
    "        print(\"Calculating document distance \" + str(idd1+1) + \" --> \" + str(idd2+1) + \"\\t Total num. of articles: \" + str(num_articles) + \" ({0:3d}%)\\n\".format(100))\n",
    "        print(\"Serializing...\")\n",
    "        document_distances_paper.dump(index_WE_model_eu_name_paper)\n",
    "        time_elapsed = time.time() - time_ini\n",
    "        elapsed_hours = int(time_elapsed/3600)\n",
    "        elapsed_minutes = int(int(time_elapsed%3600)/60)\n",
    "        elapsed_seconds = int(int(time_elapsed%3600)%60)\n",
    "        print(\"WMD Document distances serialized.\")\n",
    "        print(\"Elapsed time Document Distances Paper: \" + str(elapsed_hours) + \" h \" + str(elapsed_minutes) + \" min \" + str(elapsed_seconds) + \" sec\\n\")\n",
    "\n",
    "def find_similardocs_lsi_training(lsi_model, lsi_index, num=11, doc=-1, return_results=True):\n",
    "    \"\"\"\n",
    "    Purpose: Find\n",
    "    Input:   \n",
    "    Output:  \n",
    "    \"\"\"\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "\n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    if(stemmed_tfidf): sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) AND m.liteId = $query_liteid RETURN m.preprocessed_stemmed AS preprocessed\"\n",
    "    else: sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) AND m.liteId = $query_liteid RETURN m.preprocessed AS preprocessed\"\n",
    "    lsiquery = graph.run(sim_query, parameters={'query_liteid': doc}).data()\n",
    "    if(stemmed_tfidf): doc_bow = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in lsiquery]\n",
    "    else: doc_bow = [corpus_memory_friendly_NST.dictionary.doc2bow(doc['preprocessed']) for doc in lsiquery]\n",
    "    doc_lsi = lsi_model[doc_bow]\n",
    "    docs_similar = lsi_index[doc_lsi]\n",
    "    sort_docs_similar = [sorted(enumerate(val), key=lambda item: -item[1])[:num] for it,val in enumerate(docs_similar)][0]\n",
    "    recommend_docs_idd = []\n",
    "    for idd,simil_score in sort_docs_similar[1:]:\n",
    "        recommend_docs_idd.append(idd)\n",
    "        #show no similar articles if there are not articles above a certain threshold (OBS: Right now we only show the best 3)\n",
    "    if return_results: return recommend_docs_idd\n",
    "    \n",
    "def find_similardocs_lda_training(lda_model, lda_index, num=11, doc=-1, return_results=True):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Input:\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "\n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    if(stemmed_tfidf): sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) AND m.liteId = $query_liteid RETURN m.preprocessed_stemmed AS preprocessed\"\n",
    "    else: sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) AND m.liteId = $query_liteid RETURN m.preprocessed AS preprocessed\"\n",
    "    ldaquery = graph.run(sim_query, parameters={'query_liteid': doc}).data()\n",
    "    if(stemmed_tfidf): doc_bow = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in ldaquery]\n",
    "    else: doc_bow = [corpus_memory_friendly_NST.dictionary.doc2bow(doc['preprocessed']) for doc in ldaquery]\n",
    "    doc_lda = lda_model[doc_bow]\n",
    "    docs_similar = lda_index[doc_lda]\n",
    "    sort_docs_similar = [sorted(enumerate(val), key=lambda item: -item[1])[:num] for it,val in enumerate(docs_similar)][0]\n",
    "    recommend_docs_idd = []\n",
    "    for idd,simil_score in sort_docs_similar[1:]:\n",
    "        recommend_docs_idd.append(idd)\n",
    "        #show no similar articles if there are not articles above a certain threshold (OBS: Right now we only show the best 3)\n",
    "    if return_results: return recommend_docs_idd\n",
    "\n",
    "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3, algorithm='lsa'):\n",
    "    \"\"\"\n",
    "    Input   : dictionary : Gensim dictionary\n",
    "              corpus : Feedly Teams corpus\n",
    "              texts : List of articles\n",
    "              stop : Max num of topics\n",
    "    purpose : Compute c_v coherence for various number of topics\n",
    "    Output  : model_list : List of LSA topic models\n",
    "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    print(\"Computing coherence analysis for \" + algorithm + \" ...\\n\")\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    maximum_coherence = -1\n",
    "    optimum_ntopics = 0\n",
    "    for num_topics in range(start, stop, step):\n",
    "        if((algorithm == 'lsa') or (algorithm == 'lsi')):\n",
    "            # generate LSA model\n",
    "            model = models.LsiModel(doc_term_matrix, num_topics=num_topics, id2word = dictionary)  # train model\n",
    "        elif(algorithm == 'lda'):\n",
    "            # generate LSA model\n",
    "            model = models.LdaModel(doc_term_matrix, num_topics=num_topics, id2word = dictionary)  # train model\n",
    "        else:\n",
    "            model = models.LsiModel(doc_term_matrix, num_topics=num_topics, id2word = dictionary)  # train model\n",
    "        coherencemodel = models.CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "        model_list.append(model)\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        if(coherencemodel.get_coherence() > maximum_coherence):\n",
    "            maximum_coherence = coherencemodel.get_coherence()\n",
    "            optimum_ntopics = num_topics\n",
    "        print(\"Coherence analysis: \" + str(num_topics) + \"/\" + str(stop), end=\"\\r\")\n",
    "    return model_list, coherence_values, maximum_coherence, optimum_ntopics\n",
    "\n",
    "def coherence_model(algorithm):\n",
    "    start = 2\n",
    "    stop = 15\n",
    "    step = 1\n",
    "    if(stemmed_tfidf): data = graph.run(\"MATCH (n:Article:_AI) WHERE EXISTS(n.preprocessed_stemmed) RETURN n.preprocessed_stemmed AS preproc ORDER BY n.liteId ASC\").data()\n",
    "    else: data = graph.run(\"MATCH (n:Article:_AI) WHERE EXISTS(n.preprocessed) RETURN n.preprocessed AS preproc ORDER BY n.liteId ASC\").data()\n",
    "    new_data = [item['preproc'] for item in data] #Careful: you are loading all the articles here!\n",
    "    if(stemmed_tfidf): model_list, coherence_values, maximum_coherence, optimum_ntopics = compute_coherence_values(dictionary=corpus_memory_friendly.dictionary, doc_term_matrix=corpus_memory_friendly, doc_clean=new_data, start=start, stop=stop, step=step, algorithm=algorithm)\n",
    "    else: model_list, coherence_values, maximum_coherence, optimum_ntopics = compute_coherence_values(dictionary=corpus_memory_friendly_NST.dictionary, doc_term_matrix=corpus_memory_friendly_NST, doc_clean=new_data, start=start, stop=stop, step=step, algorithm=algorithm)\n",
    "    # Show graph\n",
    "    x = range(start, stop, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(\"Coherence\", loc='best')\n",
    "    plt.show()\n",
    "    print(\"Optimum number of topics: \" + str(optimum_ntopics))\n",
    "    print(\"Max. coherence: \" + str(maximum_coherence))\n",
    "    \n",
    "def maximum_recall_score(algorithm):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Input:\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    start = 2\n",
    "    stop = 120\n",
    "    step = 1\n",
    "    if(stemmed_tfidf):\n",
    "        print(\"Computing for STEMMED WORDS\")\n",
    "        compare_docs = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) RETURN m.preprocessed_stemmed AS preprocessed ORDER BY m.liteId ASC\").data()\n",
    "        compare_docs_bow = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in compare_docs]\n",
    "    else:\n",
    "        print(\"Computing for NON-STEMMED WORDS\")\n",
    "        compare_docs = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.preprocessed AS preprocessed ORDER BY m.liteId ASC\").data()\n",
    "        compare_docs_bow = [corpus_memory_friendly_NST.dictionary.doc2bow(doc['preprocessed']) for doc in compare_docs]\n",
    "    print(\"Computing maximum recall score analysis for \" + algorithm + \" ...\\n\")\n",
    "    recall_scores = []\n",
    "    RBP_scores = []\n",
    "    RBPacc_scores = []\n",
    "    model_list = []\n",
    "    index_list = []\n",
    "    maximum_recall = -1\n",
    "    maximum_RBP = -1\n",
    "    maximum_RBPacc = -1\n",
    "    optimum_ntopics_recall = 0\n",
    "    optimum_ntopics_RBP = 0\n",
    "    optimum_ntopics_RBPacc = 0\n",
    "    for num_topics in range(start, stop+1, step):\n",
    "        if((algorithm == 'lsa') or (algorithm == 'lsi')):\n",
    "            # generate LSA model\n",
    "            if(stemmed_tfidf):\n",
    "                model = models.LsiModel(corpus_memory_friendly, id2word=corpus_memory_friendly.dictionary, num_topics=num_topics)\n",
    "                comp_lsi = model[compare_docs_bow]\n",
    "                ind_lsi = similarities.Similarity(output_prefix=\"sim_lsi_idx\", corpus=comp_lsi, num_features=len(corpus_memory_friendly.dictionary))\n",
    "            else:\n",
    "                model = models.LsiModel(corpus_memory_friendly_NST, id2word=corpus_memory_friendly_NST.dictionary, num_topics=num_topics)\n",
    "                comp_lsi = model[compare_docs_bow]\n",
    "                ind_lsi = similarities.Similarity(output_prefix=\"sim_lsi_nst_idx\", corpus=comp_lsi, num_features=len(corpus_memory_friendly_NST.dictionary))\n",
    "            mod = [model,ind_lsi]\n",
    "            recall = evaluate_method(mod)\n",
    "        elif(algorithm == 'lda'):\n",
    "            # generate LDA model\n",
    "            if(stemmed_tfidf):\n",
    "                model = models.LdaModel(corpus_memory_friendly, id2word=corpus_memory_friendly.dictionary, num_topics=num_topics, passes=15, alpha='auto', eval_every=5)\n",
    "                comp_lda = model[compare_docs_bow]\n",
    "                ind_lda = similarities.Similarity(output_prefix=\"sim_lda_idx\", corpus=comp_lda, num_features=len(corpus_memory_friendly.dictionary))\n",
    "            else:\n",
    "                model = models.LdaModel(corpus_memory_friendly_NST, id2word=corpus_memory_friendly_NST.dictionary, num_topics=num_topics, passes=15, alpha='auto', eval_every=5)\n",
    "                comp_lda = model[compare_docs_bow]\n",
    "                ind_lda = similarities.Similarity(output_prefix=\"sim_lda_nst_idx\", corpus=comp_lda, num_features=len(corpus_memory_friendly_NST.dictionary))\n",
    "            mod = [model,ind_lda]\n",
    "            recall = evaluate_method(mod)\n",
    "            model_list.append(model)\n",
    "            index_list.append(ind_lda)\n",
    "        recall_scores.append(recall[0])\n",
    "        RBP_scores.append(recall[1])\n",
    "        RBPacc_scores.append(recall[2])\n",
    "        if(recall[0] > maximum_recall):\n",
    "            maximum_recall = recall[0]\n",
    "            optimum_ntopics_recall = num_topics\n",
    "        if(recall[1] > maximum_RBP):\n",
    "            maximum_RBP = recall[1]\n",
    "            optimum_ntopics_RBP = num_topics\n",
    "        if(recall[2] > maximum_RBPacc):\n",
    "            maximum_RBPacc = recall[2]\n",
    "            optimum_ntopics_RBPacc = num_topics\n",
    "        print(\"Maximum recall analysis: \" + str(num_topics) + \"/\" + str(stop), end=\"\\r\")\n",
    "    #model_list[optimum_ntopics_RBPacc].save(os.path.join(dir_name, \"LDA_MAX.model\"))\n",
    "    #index_list[optimum_ntopics_RBPacc].save(os.path.join(dir_name, \"LDA_MAX_INDEX.index\"))\n",
    "    \n",
    "    return [recall_scores, RBP_scores, RBPacc_scores, start, step, stop]\n",
    "\n",
    "#Some callbacks for GUI events\n",
    "def keyentertitlecallback(event):\n",
    "    search_documents()\n",
    "\n",
    "def keyenterdocsamountcallback(event):\n",
    "    find_similardocuments()\n",
    "\n",
    "# Feature: Give me some insight! (click in icon at the center of the GUI)\n",
    "def create_input_wordcloud(input_item):\n",
    "    string_wordcloud = \"\"\n",
    "    if isinstance(input_item, str): #community\n",
    "        query = \"MATCH (n:Article:_AI) WHERE n.community_louvain_filtered_1 = $community_query RETURN n.keywords_nst AS result\" #we can do it with content, keywords...\n",
    "        data = graph.run(query, parameters={'community_query': input_item}).data()\n",
    "        for article in data: #by content also?\n",
    "            for word in article['result']:\n",
    "                string_wordcloud += word + \" \"\n",
    "    elif isinstance(input_item, int): #lite_id document\n",
    "        query = \"MATCH (n:Article:_AI {liteId: $lite_id}) RETURN n.preprocessed AS result\" #we can do it with content, keywords...\n",
    "        data = graph.run(query, parameters={'lite_id': input_item}).data()\n",
    "        for word in data[0]['result']:\n",
    "            string_wordcloud += word + \" \"\n",
    "    return string_wordcloud\n",
    "\n",
    "def build_wordcloud_ideas(tipology):\n",
    "    if(doc_insight.get() == 0): #query document\n",
    "        if(doc_var_idx.get() != no_article):\n",
    "            if(tipology==\"community\"):\n",
    "                community = graph.run(\"MATCH (n:Article:_AI {liteId: $lite_id}) RETURN n.community_louvain_filtered_1 AS community LIMIT 1\", parameters={'lite_id': doc_var_idx.get()}).data()[0]['community']\n",
    "                item_ext = community\n",
    "            elif(tipology==\"document\"):\n",
    "                item_ext = doc_var_idx.get()\n",
    "        else:\n",
    "            output_screen['text'] = \"You must select a document first.\"\n",
    "            \n",
    "    else: #recommended document\n",
    "        if(doc_sim_idx.get() != no_article):\n",
    "            if(tipology==\"community\"):\n",
    "                community = graph.run(\"MATCH (n:Article:_AI {liteId: $lite_id}) RETURN n.community_louvain_filtered_1 AS community LIMIT 1\", parameters={'lite_id': doc_sim_idx.get()}).data()[0]['community']\n",
    "                item_ext = community\n",
    "            elif(tipology==\"document\"):\n",
    "                item_ext = doc_sim_idx.get()\n",
    "        else:\n",
    "            output_screen['text'] = \"You must select a document first.\"\n",
    "            \n",
    "    directory_wordcloud = os.path.join(dir_name, 'wordclouds/')\n",
    "    file_wordcloud = os.path.join(directory_wordcloud, tipology + \"_\" + str(item_ext) + \".png\") \n",
    "    \n",
    "    if not os.path.isfile(file_wordcloud):\n",
    "        mask = np.array(Image.open(os.path.join(directory_wordcloud, 'cloud.png')))\n",
    "        wc = WordCloud(background_color=\"white\", mask=mask, max_words=200, stopwords=stopwords.words('english'))\n",
    "        text = create_input_wordcloud(item_ext)\n",
    "        wc.generate(text)\n",
    "        wc.to_file(file_wordcloud)\n",
    "    window_wordcloud = tk.Toplevel(root, height=HEIGHT, width=WIDTH*2)\n",
    "    wordcloud_image = ImageTk.PhotoImage(Image.open(file_wordcloud), master=window_wordcloud)\n",
    "    wordcloud_label = tk.Label(window_wordcloud, image=wordcloud_image)\n",
    "    wordcloud_label.place(anchor='n', relx=0.5, rely=0, relwidth=1, relheight=1)\n",
    "    window_wordcloud.mainloop()\n",
    "    \n",
    "def find_relations():\n",
    "    if(doc_var_idx.get() != no_article):\n",
    "        if(doc_sim_idx.get() != no_article):\n",
    "            first_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.keywords_nst) AND m.liteId = $query_liteid RETURN m.keywords_nst AS keywords\"\n",
    "            query = graph.run(first_query, parameters={'query_liteid': doc_var_idx.get()}).data()\n",
    "            if query:\n",
    "                sec_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.keywords) AND m.liteId = $query_liteid RETURN m.keywords AS keywords\"\n",
    "                second_query = graph.run(sec_query, parameters={'query_liteid': doc_sim_idx.get()}).data()\n",
    "                if second_query:\n",
    "                    stemmer = PorterStemmer()\n",
    "                    list_common_themes = []\n",
    "                    list_common_stems = []\n",
    "                    for keyword in query[0]['keywords']:\n",
    "                        stword = stemmer.stem(keyword)\n",
    "                        if stword in second_query[0]['keywords'] and stword not in list_common_stems: #\n",
    "                            list_common_themes.append(keyword)\n",
    "                            list_common_stems.append(stword)\n",
    "                            \n",
    "                    if not list_common_themes:\n",
    "                        tk.messagebox.showinfo(\"Info\", \"There were no words in common or the documents are not actually related.\")\n",
    "                    else:\n",
    "                        text_insight = \"The documents share the next concepts:\\n\"\n",
    "                        for word in list_common_themes[:-1]:\n",
    "                            text_insight += word + \", \"\n",
    "                        text_insight += list_common_themes[-1]\n",
    "                        output_screen['text'] = \"Showing insight of two documents.\"\n",
    "                        tk.messagebox.showinfo(\"Info\", text_insight)\n",
    "                else:\n",
    "                    tk.messagebox.showwarning(\"Info\", \"The application could not find the second document.\")\n",
    "            else:\n",
    "                tk.messagebox.showwarning(\"Info\", \"The application could not find the queried document.\")\n",
    "        else:\n",
    "            tk.messagebox.showinfo(\"Info\", \"You must select a second document.\")\n",
    "    else:\n",
    "        tk.messagebox.showinfo(\"Info\", \"You must select a first document.\")\n",
    "\n",
    "def insightinfocallback(event):\n",
    "    global doc_insight\n",
    "    window_insights = tk.Toplevel(root, height=HEIGHT/2-10, width=WIDTH/2-10)\n",
    "    title_insights = tk.Label(window_insights, text=\"Select an insight\")\n",
    "    title_insights.config(font=(\"Volvo Broad Pro\", 13))\n",
    "    title_insights.place(anchor='n', relx=0.5, rely=0.05, relwidth=0.6, relheight=0.08)\n",
    "    frame_select_document = tk.Frame(window_insights)\n",
    "    frame_select_document.place(relx=0.05, rely=0.15, anchor='nw', relwidth=0.7, relheight=0.15)\n",
    "    r1 = tk.Radiobutton(frame_select_document, text=\"For the query document\", selectcolor='#e6f2ff', wraplength=255, variable=doc_insight, value=0)\n",
    "    r1.config(font=(FONT_ARTICLES, 8))\n",
    "    r1.pack(anchor = 'w')\n",
    "    r2 = tk.Radiobutton(frame_select_document, text=\"For the recommended document\", selectcolor='#e6f2ff', wraplength=255, variable=doc_insight, value=1)\n",
    "    r2.config(font=(FONT_ARTICLES, 8))\n",
    "    r2.pack(anchor = 'w')\n",
    "    button_source_material = tk.Button(window_insights, text=\"Read source material\")\n",
    "    button_source_material.config(font=(TEXT_FONT, 10))\n",
    "    button_source_material.config(command=lambda: read_source_material())\n",
    "    button_source_material.place(anchor='n', relx=0.5, rely=0.32, relwidth=0.5, relheight=0.1)\n",
    "    button_wordcloud_document = tk.Button(window_insights, text=\"Idea of the document\")\n",
    "    button_wordcloud_document.config(font=(TEXT_FONT, 10))\n",
    "    button_wordcloud_document.config(command=lambda: build_wordcloud_ideas('document'))\n",
    "    button_wordcloud_document.place(anchor='n', relx=0.5, rely=0.43, relwidth=0.5, relheight=0.1)\n",
    "    button_wordcloud_community = tk.Button(window_insights, text=\"What else is in the topic?\")\n",
    "    button_wordcloud_community.config(font=(TEXT_FONT, 10))\n",
    "    button_wordcloud_community.config(command=lambda: build_wordcloud_ideas('community'))\n",
    "    button_wordcloud_community.place(anchor='n', relx=0.5, rely=0.54, relwidth=0.62, relheight=0.1)\n",
    "    button_why_related = tk.Button(window_insights, text=\"Why are they related?\")\n",
    "    button_why_related.config(font=(TEXT_FONT, 10))\n",
    "    button_why_related.config(command=lambda: find_relations())\n",
    "    button_why_related.place(anchor='n', relx=0.5, rely=0.65, relwidth=0.62, relheight=0.1)\n",
    "    \"\"\"\n",
    "    if(doc_var_idx.get() != no_article):\n",
    "        if(doc_sim_idx.get() != no_article):\n",
    "            first_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.keywords) AND m.liteId = $query_liteid RETURN m.keywords_nst AS keywords\"\n",
    "            query = graph.run(first_query, parameters={'query_liteid': doc_var_idx.get()}).data()\n",
    "            if query:\n",
    "                sec_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.keywords) AND m.liteId = $query_liteid RETURN m.keywords AS keywords\"\n",
    "                second_query = graph.run(sec_query, parameters={'query_liteid': doc_sim_idx.get()}).data()\n",
    "                if second_query:\n",
    "                    stemmer = PorterStemmer()\n",
    "                    list_common_themes = []\n",
    "                    list_common_stems = []\n",
    "                    for keyword in query[0]['keywords']:\n",
    "                        stword = stemmer.stem(keyword)\n",
    "                        if stword in second_query[0]['keywords'] and stword not in list_common_stems: #\n",
    "                            list_common_themes.append(keyword)\n",
    "                            list_common_stems.append(stword)\n",
    "                            \n",
    "                    if not list_common_themes:\n",
    "                        tk.messagebox.showinfo(\"Info\", \"There were no words in common or the documents are not actually related.\")\n",
    "                    else:\n",
    "                        text_insight = \"The documents share the next concepts:\\n\"\n",
    "                        for word in list_common_themes[:-1]:\n",
    "                            text_insight += word + \", \"\n",
    "                        text_insight += list_common_themes[-1]\n",
    "                        tk.messagebox.showinfo(\"Info\", text_insight)\n",
    "                else:\n",
    "                    tk.messagebox.showwarning(\"Info\", \"We couldn't find the second document.\\nCheck with Ivan.\")\n",
    "            else:\n",
    "                tk.messagebox.showwarning(\"Info\", \"We couldn't find the document.\\nCheck with Ivan.\")\n",
    "            #tk.messagebox.showinfo(\"Info\", \"This is meant to show insight.\")\n",
    "        else:\n",
    "            tk.messagebox.showinfo(\"Info\", \"You must select a second document.\")\n",
    "    else:\n",
    "        tk.messagebox.showinfo(\"Info\", \"You must select a document.\")\n",
    "    \"\"\"\n",
    "    window_insights.mainloop()\n",
    "\n",
    "def insightlabcallback(event):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Input:\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    global LST_features, ocult_train, stemmed_tfidf, tfidf, index_tfidf\n",
    "    if(ocult_train):\n",
    "        answerst = tk.messagebox.askyesno(\"Want to use the Non-stemmed version?\",\"Do you want to change to the NST version of the TF-IDF?\\nIf not, the standard stemmed version will be loaded.\")\n",
    "        stemmed_tfidf = not answerst\n",
    "        train_button = tk.Button(footer_frame, text=\"Train Algorithm\")\n",
    "        train_button.config(font=(TEXT_FONT, 10))\n",
    "        train_button.config(command=lambda: train_algorithm())\n",
    "        train_button.place(anchor='nw', relx=0.505, rely=0, relwidth=0.2, relheight=0.47)\n",
    "        evaluate_button = tk.Button(footer_frame, text=\"Evaluate\")\n",
    "        evaluate_button.config(font=(TEXT_FONT, 10))\n",
    "        evaluate_button.config(command=lambda: evaluate_method())\n",
    "        evaluate_button.place(anchor='sw', relx=0.505, rely=1, relwidth=0.2, relheight=0.53)\n",
    "        LST_features.append(train_button)\n",
    "        LST_features.append(evaluate_button)\n",
    "        output_screen['text'] = \"Wow, you discovered a new feature!\\n(Only for developers)\" #change icon to developer?\n",
    "        ocult_train = not ocult_train\n",
    "    else:\n",
    "        for i in LST_features:\n",
    "            i.destroy()\n",
    "        ocult_train = not ocult_train\n",
    "        output_screen['text'] = \"\" #change icon to developer?\n",
    "\n",
    "#Functions: Scrolling with the mouse\n",
    "def _bound_to_mousewheel(event):\n",
    "    left_dlistcanvas.bind_all(\"<MouseWheel>\", scroll_documentscallback)\n",
    "\n",
    "def _unbound_to_mousewheel(event):\n",
    "    left_dlistcanvas.unbind_all(\"<MouseWheel>\")\n",
    "\n",
    "def _bound_to_mousewheelsim(event):\n",
    "    right_dlistcanvas.bind_all(\"<MouseWheel>\", scroll_documentscallbacksim)\n",
    "\n",
    "def _unbound_to_mousewheelsim(event):\n",
    "    right_dlistcanvas.unbind_all(\"<MouseWheel>\")\n",
    "\n",
    "def scroll_documentscallback(event):\n",
    "    if(left_dlistframe.winfo_height() > left_dlistcanvas.winfo_height()):\n",
    "        left_dlistcanvas.yview_scroll(-1*int((event.delta/120)), \"units\")\n",
    "        \n",
    "def scroll_documentscallbacksim(event):\n",
    "    if(right_dlistframe.winfo_height() > right_dlistcanvas.winfo_height()):\n",
    "        right_dlistcanvas.yview_scroll(-1*int((event.delta/120)), \"units\")\n",
    "\n",
    "##################################################\n",
    "#  Functions: Give me related/similar documents  #\n",
    "#------------------------------------------------#\n",
    "##################################################\n",
    "\n",
    "def find_similardocuments():\n",
    "    \"\"\"\n",
    "    Purpose: Using the algorithms to find the closest or most similar documents.\n",
    "    Input:   None. The function will use the models trained in disk and the selected article from the GUI.\n",
    "    Output:  None. The representation of related documents will be shown in the GUI.\n",
    "    \"\"\"\n",
    "    global doc_sim_idx\n",
    "    pred_amount_doc = 10\n",
    "    valid_number = True\n",
    "    if(doc_var_idx.get() != no_article):\n",
    "        for label in LIST_NOT_FOUND_LABELS:\n",
    "            label.destroy()\n",
    "    \n",
    "        #Check if the user has defined an amount#\n",
    "        if entry_docs.get():\n",
    "            try:\n",
    "                amount = int(entry_docs.get(),10)\n",
    "            except:\n",
    "                valid_number = False\n",
    "                amount = pred_amount_doc\n",
    "            else:\n",
    "                if(amount <= 0): valid_number=False\n",
    "                else: annex = \"\\nShowing the \" + str(amount) + \" most similar article/s.\"\n",
    "        else:\n",
    "            annex = \"\\nPredifined: showing the \" + str(pred_amount_doc) + \" most similar articles.\"\n",
    "            amount = pred_amount_doc\n",
    "        if valid_number:\n",
    "            amount += 1\n",
    "            if(algorithmvariable.get()==\"Word Embeddings\"):\n",
    "                showing_recommendations(algorithmvariable.get()+annex)\n",
    "                find_similardocs_WE(amount)\n",
    "            elif(algorithmvariable.get()==\"TF-IDF\"):\n",
    "                showing_recommendations(algorithmvariable.get()+annex)\n",
    "                if(stemmed_tfidf): find_similardocs_tfidf(amount)\n",
    "                else: find_similardocs_tfidf_nst(amount)\n",
    "            elif(algorithmvariable.get()==\"Doc2vec\"):\n",
    "                not_implemented_message()\n",
    "            elif(algorithmvariable.get()==\"LSA\"):\n",
    "                showing_recommendations(\"Latent Semantic Analysis\"+annex)\n",
    "                if(stemmed_tfidf): find_similardocs_lsi(amount)\n",
    "                else: find_similardocs_lsi_nst(amount)\n",
    "            elif(algorithmvariable.get()==\"LDA\"):\n",
    "                showing_recommendations(\"Latent Dirichlet Allocation\"+annex)\n",
    "                find_similardocs_lda(amount)\n",
    "            elif(algorithmvariable.get()==\"Ensemble Method\"):\n",
    "                amount -= 1\n",
    "                find_similardocs_ensemble(amount)\n",
    "            elif(algorithmvariable.get()==\"Community Finding\"):\n",
    "                amount -= 1\n",
    "                find_similardocs_community(amount)\n",
    "            else:\n",
    "                algorithm_not_found_message()\n",
    "        else:\n",
    "            not_valid_message()\n",
    "        \n",
    "    else: article_not_selected_message()\n",
    "    update_idle()\n",
    "        \n",
    "\n",
    "def find_similardocs_tfidf(num=11, doc=-1, return_results=False):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "    \n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) AND m.liteId = $query_liteid RETURN m.preprocessed_stemmed AS preprocessed\"\n",
    "    query = graph.run(sim_query, parameters={'query_liteid': doc}).data()\n",
    "    doc_bow = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in query]\n",
    "    doc_tfidf = tfidf[doc_bow]\n",
    "    docs_similar = index_tfidf[doc_tfidf]\n",
    "    sort_docs_similar = [sorted(enumerate(val), key=lambda item: -item[1])[:num] for it,val in enumerate(docs_similar)][0]\n",
    "    recommend_docs_idd = show_results(sort_docs_similar)\n",
    "    if return_results: return recommend_docs_idd\n",
    "\n",
    "def find_similardocs_tfidf_nst(num=11, doc=-1, return_results=False):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "    \n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) AND m.liteId = $query_liteid RETURN m.preprocessed AS preprocessed\"\n",
    "    query = graph.run(sim_query, parameters={'query_liteid': doc}).data()\n",
    "    doc_bow = [corpus_memory_friendly_NST.dictionary.doc2bow(doc['preprocessed']) for doc in query]\n",
    "    doc_tfidf = tfidf_nst[doc_bow]\n",
    "    docs_similar = index_tfidf_nst[doc_tfidf]\n",
    "    sort_docs_similar = [sorted(enumerate(val), key=lambda item: -item[1])[:num] for it,val in enumerate(docs_similar)][0]\n",
    "    recommend_docs_idd = show_results(sort_docs_similar)\n",
    "    if return_results: return recommend_docs_idd\n",
    "    \n",
    "def find_similardocs_lsi(num=11, doc=-1, return_results=False):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "\n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) AND m.liteId = $query_liteid RETURN m.preprocessed_stemmed AS preprocessed\"\n",
    "    lsiquery = graph.run(sim_query, parameters={'query_liteid': doc}).data()\n",
    "    doc_bow = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in lsiquery]\n",
    "    doc_lsi = lsi[doc_bow]\n",
    "    docs_similar = index_lsi[doc_lsi]\n",
    "    sort_docs_similar = [sorted(enumerate(val), key=lambda item: -item[1])[:num] for it,val in enumerate(docs_similar)][0]\n",
    "    recommend_docs_idd = show_results(sort_docs_similar)\n",
    "    if return_results: return recommend_docs_idd\n",
    "\n",
    "def find_similardocs_lsi_nst(num=11, doc=-1, return_results=False):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "\n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) AND m.liteId = $query_liteid RETURN m.preprocessed AS preprocessed\"\n",
    "    lsiquery = graph.run(sim_query, parameters={'query_liteid': doc}).data()\n",
    "    doc_bow = [corpus_memory_friendly_NST.dictionary.doc2bow(doc['preprocessed']) for doc in lsiquery]\n",
    "    doc_lsi = lsi_nst[doc_bow]\n",
    "    docs_similar = index_lsi_nst[doc_lsi]\n",
    "    sort_docs_similar = [sorted(enumerate(val), key=lambda item: -item[1])[:num] for it,val in enumerate(docs_similar)][0]\n",
    "    recommend_docs_idd = show_results(sort_docs_similar)\n",
    "    if return_results: return recommend_docs_idd\n",
    "\n",
    "def find_similardocs_lda(num=11, doc=-1, return_results=False):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "\n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) AND m.liteId = $query_liteid RETURN m.preprocessed_stemmed AS preprocessed\"\n",
    "    ldaquery = graph.run(sim_query, parameters={'query_liteid': doc}).data()\n",
    "    doc_bow = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in ldaquery]\n",
    "    doc_lda = lda[doc_bow]\n",
    "    docs_similar = index_lda[doc_lda]\n",
    "    sort_docs_similar = [sorted(enumerate(val), key=lambda item: -item[1])[:num] for it,val in enumerate(docs_similar)][0]\n",
    "    recommend_docs_idd = show_results(sort_docs_similar)\n",
    "    if return_results: return recommend_docs_idd\n",
    "    \n",
    "def find_similardocs_lda_nst(num=11, doc=-1, return_results=False):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "\n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    sim_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) AND m.liteId = $query_liteid RETURN m.preprocessed AS preprocessed\"\n",
    "    ldaquery = graph.run(sim_query, parameters={'query_liteid': doc}).data()\n",
    "    doc_bow = [corpus_memory_friendly_NST.dictionary.doc2bow(doc['preprocessed']) for doc in ldaquery]\n",
    "    doc_lda = lda_nst[doc_bow]\n",
    "    docs_similar = index_lda_nst[doc_lda]\n",
    "    sort_docs_similar = [sorted(enumerate(val), key=lambda item: -item[1])[:num] for it,val in enumerate(docs_similar)][0]\n",
    "    recommend_docs_idd = show_results(sort_docs_similar)\n",
    "    if return_results: return recommend_docs_idd\n",
    "    \n",
    "def find_similardocs_WE(num=11, doc=-1, return_results=False):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "    tech = 'pp'\n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    if (tech == 'cs'):\n",
    "        sim_matrix = np.load(index_WE_model_cs_name_bert_glove)[doc]\n",
    "        sort_docs_similar = [val for it,val in enumerate(sim_matrix)]\n",
    "        similar_docs = sorted(enumerate(sort_docs_similar), key=lambda item: -item[1])[:num]\n",
    "    elif (tech == 'eu'):\n",
    "        dist_matrix = np.load(index_WE_model_eu_name_bert_glove)[doc]\n",
    "        sort_docs_distances = [val for it,val in enumerate(dist_matrix)]\n",
    "        similar_docs = sorted(enumerate(sort_docs_distances), key=lambda item: item[1])[:num]\n",
    "    elif (tech == 'pp'):\n",
    "        dist_matrix = np.load(index_WE_model_eu_name_paper)[doc]\n",
    "        sort_docs_distances = [val for it,val in enumerate(dist_matrix)]\n",
    "        similar_docs = sorted(enumerate(sort_docs_distances), key=lambda item: item[1])[:num]\n",
    "    recommend_docs_idd = show_results(similar_docs)\n",
    "    if return_results: return recommend_docs_idd\n",
    "    \n",
    "def find_similardocs_ensemble(num=10, doc=-1, return_results=False):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "\n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    similarity_query = \"MATCH (m:Article:_AI {liteId: $query_liteid})-[r:RELATES_TO]->(a2:Article:_AI) RETURN a2.title AS title, a2.liteId AS lite_id, r.weight AS weight ORDER BY r.weight DESC LIMIT $num_docs\"\n",
    "    community_query = graph.run(similarity_query, parameters={'query_liteid': doc, 'num_docs': num}).data()\n",
    "    recommend_docs_idd = []\n",
    "    for article in community_query:\n",
    "        label = article['title'] + \" (\" + str(article['weight']) + \")\"\n",
    "        r = tk.Radiobutton(right_dlistframe, text=label, selectcolor='#e6f2ff', wraplength=255, bg='white', relief='ridge', overrelief='ridge', indicatoron=False, variable=doc_sim_idx, value=article['lite_id'])\n",
    "        r.config(font=(FONT_ARTICLES, 9))\n",
    "        r.pack(anchor = 'w', fill='x')\n",
    "        LIST_SIM_DOCS.append(r)\n",
    "        recommend_docs_idd.append(article['lite_id'])\n",
    "    if return_results: return recommend_docs_idd\n",
    "\n",
    "def find_similardocs_community(num=10, doc=-1, return_results=False):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "\n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    if (doc == -1): doc = doc_var_idx.get()\n",
    "    LIST_SIM_DOCS = []\n",
    "    similarity_query = \"MATCH (a1:Article:_AI {liteId: $query_liteid})-[r:RELATES_TO]->(a2:Article:_AI) WHERE a1.community_louvain_filtered_1 = a2.community_louvain_filtered_1 RETURN a2.title AS title, a2.liteId AS lite_id, r.weight AS weight ORDER BY r.weight DESC LIMIT $num_docs\"\n",
    "    community_query = graph.run(similarity_query, parameters={'query_liteid': doc, 'num_docs': num}).data()\n",
    "    recommend_docs_idd = []\n",
    "    for article in community_query:\n",
    "        label = article['title'] + \" (\" + str(article['weight']) + \")\"\n",
    "        r = tk.Radiobutton(right_dlistframe, text=label, selectcolor='#e6f2ff', wraplength=255, bg='white', relief='ridge', overrelief='ridge', indicatoron=False, variable=doc_sim_idx, value=article['lite_id'])\n",
    "        r.config(font=(FONT_ARTICLES, 9))\n",
    "        r.pack(anchor = 'w', fill='x')\n",
    "        LIST_SIM_DOCS.append(r)\n",
    "        recommend_docs_idd.append(article['lite_id'])\n",
    "    if return_results: return recommend_docs_idd\n",
    "\n",
    "def show_results(docs):\n",
    "    global doc_sim_idx, LIST_SIM_DOCS\n",
    "    recommend_docs_idd = []\n",
    "    for idd,simil_score in docs[1:]:\n",
    "        query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.content) AND m.liteId = $lite_id RETURN m.title AS title, m.content AS content, m.liteId AS lite_id\"\n",
    "        retrieve_document = graph.run(query, parameters={'lite_id': idd}).data()\n",
    "        label = retrieve_document[0]['title'] + \" (\" + str(simil_score) + \")\"\n",
    "        r = tk.Radiobutton(right_dlistframe, text=label, selectcolor='#e6f2ff', wraplength=255, bg='white', relief='ridge', overrelief='ridge', indicatoron=False, variable=doc_sim_idx, value=retrieve_document[0]['lite_id'])\n",
    "        r.config(font=(FONT_ARTICLES, 9))\n",
    "        r.pack(anchor = 'w', fill='x')\n",
    "        LIST_SIM_DOCS.append(r)\n",
    "        recommend_docs_idd.append(idd)\n",
    "    return recommend_docs_idd\n",
    "\n",
    "#Function: update the GUI, so new content can be showed!\n",
    "def update_idle():\n",
    "    right_dlistcanvas.update_idletasks()\n",
    "    doc_sim_idx.set(no_article)\n",
    "    right_dlistcanvas.configure(scrollregion=right_dlistcanvas.bbox('all'))\n",
    "\n",
    "#Secret feature: Evaluate accuracy with the test set!\n",
    "def evaluate_method(models=[]):\n",
    "    \"\"\"\n",
    "    Purpose: Have a metric to objectively compare different models, also different versions of the same algorithm.\n",
    "    Input:   Normally it is not used. Only in the version when training a model to maximize the evaluation score\n",
    "             will be necessary (LSI/LDA for max recall).\n",
    "             models - [lsi_model, lsi_index] list of the model and index for similarity evaluation.\n",
    "    Output:  None. The result will be shown in the GUI.\n",
    "             Provides:\n",
    "                 Recall: Accuracy metric over what documents of the same cluster appear in a direct recommendation.\n",
    "                 RBP:    Score that values the rank of the recommendations. The smaller the rank, the higher relevance.\n",
    "                 RBPacc: A multiplication of the two previous metrics. This is in order to value not only accuracy\n",
    "                         but also the order in which the recommendations appear in the GUI.\n",
    "                 \n",
    "                 For more information, read the paper \"Feeling Lucky? Multi-armed Bandits for Ordering Judgements in\n",
    "                    Pooling-based Evaluation\" by David E. Losada et al. (2016) OR the project documentation.\n",
    "    \"\"\"\n",
    "    evaluation_matrix = [[37, 104, 145, 144],\n",
    "                         [175, 113, 44, 11],\n",
    "                         [160, 153, 23, 25, 135, 222],\n",
    "                         [329, 212, 152],\n",
    "                         [179, 190, 29, 122, 277],\n",
    "                         [57, 2, 42, 81, 39],\n",
    "                         [58, 104, 116],\n",
    "                         [147, 18, 224, 347],\n",
    "                         [10, 225],\n",
    "                         [304, 303, 340],\n",
    "                         [3, 98],\n",
    "                         [98, 337, 196, 304],\n",
    "                         [3, 346],\n",
    "                         [61, 196],\n",
    "                         [1, 209, 328, 267, 287, 281],\n",
    "                         [8, 94]] #test set evaluation with known clusters manually evaluated\n",
    "    total_recommendations = 0\n",
    "    total_expected_recommendations = 0\n",
    "    decay = 0.85 #decay coefficient\n",
    "    RBP = 0 #rank-biased precision\n",
    "    RBPacc = 0 #rank-biased precision x recall accuracy measurement\n",
    "    if(algorithmvariable.get()==\"Word Embeddings\"):\n",
    "        for cluster in evaluation_matrix:\n",
    "            n_doc_cluster = len(cluster)\n",
    "            for itarticle in cluster:\n",
    "                recom = find_similardocs_WE(doc=itarticle, return_results=True)\n",
    "                for i,rec in enumerate(recom):\n",
    "                    if (rec in cluster):\n",
    "                        total_recommendations += 1\n",
    "                        RBP += decay**i\n",
    "                total_recommendations += 1 #to include itself in the cluster recommendation\n",
    "                total_expected_recommendations += n_doc_cluster\n",
    "    elif(algorithmvariable.get()==\"TF-IDF\"):\n",
    "         for cluster in evaluation_matrix:\n",
    "            n_doc_cluster = len(cluster)\n",
    "            for itarticle in cluster:\n",
    "                if(stemmed_tfidf): recom = find_similardocs_tfidf(doc=itarticle, return_results=True)\n",
    "                else: recom = find_similardocs_tfidf_nst(doc=itarticle, return_results=True)\n",
    "                for i,rec in enumerate(recom):\n",
    "                    if (rec in cluster):\n",
    "                        total_recommendations += 1\n",
    "                        RBP += decay**i\n",
    "                total_recommendations += 1 #to include itself in the cluster recommendation\n",
    "                total_expected_recommendations += n_doc_cluster\n",
    "    elif(algorithmvariable.get()==\"Doc2vec\"):\n",
    "        not_implemented_message()\n",
    "    elif(algorithmvariable.get()==\"LSA\"):\n",
    "        for cluster in evaluation_matrix:\n",
    "            n_doc_cluster = len(cluster)\n",
    "            for itarticle in cluster:\n",
    "                if(algorithm_training):\n",
    "                    recom = find_similardocs_lsi_training(doc=itarticle, lsi_model=models[0], lsi_index=models[1], return_results=True)\n",
    "                else:\n",
    "                    if(stemmed_tfidf): recom = find_similardocs_lsi(doc=itarticle, return_results=True)\n",
    "                    else: recom = find_similardocs_lsi_nst(doc=itarticle, return_results=True)\n",
    "                for i,rec in enumerate(recom):\n",
    "                    if (rec in cluster):\n",
    "                        total_recommendations += 1\n",
    "                        RBP += decay**i\n",
    "                total_recommendations += 1 #to include itself in the cluster recommendation\n",
    "                total_expected_recommendations += n_doc_cluster\n",
    "    elif(algorithmvariable.get()==\"LDA\"):\n",
    "        for cluster in evaluation_matrix:\n",
    "            n_doc_cluster = len(cluster)\n",
    "            for itarticle in cluster:\n",
    "                if(algorithm_training):\n",
    "                    print(\"Doc: \" + str(itarticle))\n",
    "                    recom = find_similardocs_lda_training(doc=itarticle, lda_model=models[0], lda_index=models[1], return_results=True)\n",
    "                else:\n",
    "                    if(stemmed_tfidf): recom = find_similardocs_lda(doc=itarticle, return_results=True)\n",
    "                    else: recom = find_similardocs_lda_nst(doc=itarticle, return_results=True)\n",
    "                for i,rec in enumerate(recom):\n",
    "                    if (rec in cluster):\n",
    "                        total_recommendations += 1\n",
    "                        RBP += decay**i\n",
    "                total_recommendations += 1 #to include itself in the cluster recommendation\n",
    "                total_expected_recommendations += n_doc_cluster\n",
    "    elif(algorithmvariable.get()==\"Ensemble Method\"):\n",
    "        for cluster in evaluation_matrix:\n",
    "            n_doc_cluster = len(cluster)\n",
    "            for itarticle in cluster:\n",
    "                recom = find_similardocs_ensemble(doc=itarticle, return_results=True)\n",
    "                for i,rec in enumerate(recom):\n",
    "                    if (rec in cluster):\n",
    "                        total_recommendations += 1\n",
    "                        RBP += decay**i\n",
    "                total_recommendations += 1 #to include itself in the cluster recommendation\n",
    "                total_expected_recommendations += n_doc_cluster\n",
    "    elif(algorithmvariable.get()==\"Community Finding\"):\n",
    "        for cluster in evaluation_matrix:\n",
    "            n_doc_cluster = len(cluster)\n",
    "            dictionary_evaluation = dict()\n",
    "            for itarticle in cluster:\n",
    "                community_found = graph.run(\"MATCH (n:Article:_AI {liteId: $lite_id}) RETURN DISTINCT n.community_louvain_filtered_1 AS community LIMIT 1\", parameters={'lite_id': itarticle}).data()\n",
    "                dictionary_evaluation[community_found[0]['community']] = dictionary_evaluation.get(community_found[0]['community'], 0) + 1\n",
    "                recom = find_similardocs_community(doc=itarticle, return_results=True)\n",
    "                for i,rec in enumerate(recom):\n",
    "                    if (rec in cluster):\n",
    "                        RBP += decay**i\n",
    "            dictionary_evaluation = sorted(dictionary_evaluation.items(), key= lambda item: -item[1])\n",
    "            total_recommendations += dictionary_evaluation[0][1]\n",
    "            total_expected_recommendations += n_doc_cluster\n",
    "    else:\n",
    "        algorithm_not_found_message()\n",
    "        \n",
    "    if(total_expected_recommendations > 0):\n",
    "        recomendation_evaluation = total_recommendations/total_expected_recommendations\n",
    "        RBPacc = RBP*recomendation_evaluation\n",
    "        output_screen['text'] = \"Recall score: %.2f\\nThe RBP score: %.2f\\nThe RBP-accuracy is: %.2f\" % (recomendation_evaluation,RBP,RBPacc)\n",
    "        if(algorithm_training): return [recomendation_evaluation,RBP,RBPacc]\n",
    "    update_idle() #update the GUI to represent the new things!\n",
    "\n",
    "#Feature: Searching and visualizing an article in the web browser\n",
    "def read_source_material():\n",
    "    \"\"\"\n",
    "    Purpose: Open a new tab in the web browser with the url to the selected article in the GUI.\n",
    "    Input: None. Selection from the GUI (global variable).\n",
    "    Output: Tab in the preferred web browser.\n",
    "    \"\"\"\n",
    "    query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.url) AND m.liteId = $lite_id RETURN m.url AS url\"\n",
    "    if doc_insight.get() == 0:\n",
    "        if(doc_var_idx.get() != no_article):\n",
    "            url = graph.run(query, parameters={'lite_id': doc_var_idx.get()}).data()\n",
    "        else:\n",
    "            output_screen['text'] = \"You must select a document first.\"\n",
    "    else:\n",
    "        if(doc_sim_idx.get() != no_article):\n",
    "            url = graph.run(query, parameters={'lite_id': doc_sim_idx.get()}).data()\n",
    "        else:\n",
    "            output_screen['text'] = \"You must select a document first.\"\n",
    "    if url:\n",
    "        output_screen['text'] = \"\"\n",
    "        webbrowser.open_new_tab(url[0]['url'][0])\n",
    "    else:\n",
    "        output_screen['text'] = \"You must select a document first.\"\n",
    "    \n",
    "#Feature: Using the search bar for articles\n",
    "def search_documents():\n",
    "    \"\"\"\n",
    "    Purpose: Queries the articles in the database that match the words in the search bar.\n",
    "    Input: None. The input search is done in the GUI (accessible variable).\n",
    "    Output: List of articles returned by the search, in the left column of the GUI.\n",
    "    Note: The matching words are only applied by the title.\n",
    "          The query varies depending on the words and filters used in the GUI.\n",
    "    \"\"\"\n",
    "    global LIST_DOCUMENTS, LIST_SIM_DOCS, LIST_NOT_FOUND_LABELS, LIST_OF_VIEWERS, VIEW_DOCUMENT, entry_title, doc_var_idx, doc_sim_idx\n",
    "    \n",
    "    for radiobutton in LIST_DOCUMENTS:\n",
    "        radiobutton.destroy()\n",
    "        \n",
    "    for radiobutton in LIST_SIM_DOCS:\n",
    "        radiobutton.destroy()\n",
    "    \n",
    "    for label in LIST_NOT_FOUND_LABELS:\n",
    "        label.destroy()\n",
    "    \n",
    "    LIST_DOCUMENTS = []\n",
    "    LIST_SIM_DOCS = []\n",
    "    LIST_NOT_FOUND_LABELS = []\n",
    "    title_search = str(entry_title.get()).replace('\"', '')\n",
    "    title_search = title_search.replace(\"'\",\"\")\n",
    "    words = title_search.split()\n",
    "    body_query = \"\"\n",
    "    for word in words:\n",
    "        body_query += \" AND m.title =~ '(?i).*\" + word.lower() + \".*'\"\n",
    "    footer_query = \" RETURN m.title AS title, m.liteId AS LID ORDER BY m.liteId ASC\"\n",
    "    \n",
    "    if(typevar.get() == TYPES[0]):\n",
    "        header_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.content)\"\n",
    "    else:\n",
    "        missmatch = False\n",
    "        if(typevar.get() == TYPES[1]):\n",
    "            header_query = \"MATCH (m:Article:_AI)-[]-(n:HorizonScanningArea:_AI) WHERE EXISTS(m.content)\"\n",
    "        elif(typevar.get() == TYPES[2]):\n",
    "            header_query = \"MATCH (m:Article:_AI)-[]-(n:LtsFocusArea:_AI) WHERE EXISTS(m.content)\"\n",
    "        elif(typevar.get() == TYPES[3]):\n",
    "            header_query = \"MATCH (m:Article:_AI)-[]-(n:Megatrend:_AI) WHERE EXISTS(m.content)\"\n",
    "        else:\n",
    "            tk.messagebox.showerror(\"Error: Name Missmatch\", \"The names appearing do not correspond to the type of nodes in the database.\\nPlease check the names in code.\\nNo filters will be applied in this case.\")\n",
    "            header_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.content)\"\n",
    "            missmatch = True\n",
    "        if(not missmatch):\n",
    "            body_query += \" AND n.name =~ '(?i)\" + catvar.get()+ \"'\"\n",
    "    search_query = graph.run(header_query + body_query + footer_query).data()\n",
    "    entry_title.delete(0,'end')\n",
    "    if(not search_query):\n",
    "        blank_article = tk.Label(left_dlistframe, text=\"(Article not found)\", bg='white')\n",
    "        blank_article.config(font=(FONT_NOT_FOUND, 9), width=310)\n",
    "        blank_article.pack(anchor='nw', fill='x')\n",
    "        LIST_NOT_FOUND_LABELS.append(blank_article)\n",
    "    else:\n",
    "        for idd,doc in enumerate(search_query):\n",
    "            r = tk.Radiobutton(left_dlistframe, text=doc['title'], selectcolor='#e6f2ff', wraplength=255, bg='white', relief='ridge', overrelief='ridge', indicatoron=False, variable=doc_var_idx, value=doc['LID'])\n",
    "            r.config(font=(FONT_ARTICLES, 9))\n",
    "            r.pack(anchor = 'w', fill='x')\n",
    "            LIST_DOCUMENTS.append(r)\n",
    "            \n",
    "    left_dlistcanvas.update_idletasks()\n",
    "    doc_var_idx.set(no_article)\n",
    "    left_dlistcanvas.configure(scrollregion=left_dlistcanvas.bbox('all'))\n",
    "    \n",
    "    blank_simarticle = tk.Label(right_dlistframe, text=\"\", bg='white')\n",
    "    blank_simarticle.config(font=(FONT_NOT_FOUND, 9), width=310)\n",
    "    blank_simarticle.pack(anchor='nw', fill='x')\n",
    "    LIST_NOT_FOUND_LABELS.append(blank_simarticle)\n",
    "    \n",
    "    right_dlistcanvas.update_idletasks()\n",
    "    doc_sim_idx.set(no_article)\n",
    "    right_dlistcanvas.configure(scrollregion=right_dlistcanvas.bbox('all'))\n",
    "    \n",
    "    if VIEW_DOCUMENT:\n",
    "        change_view_button['text'] = 'Preview'\n",
    "        for element in LIST_OF_VIEWERS:\n",
    "            element.destroy()\n",
    "        LIST_OF_VIEWERS = []\n",
    "        output_screen['text'] = \"\"\n",
    "        VIEW_DOCUMENT = False\n",
    "\n",
    "def change_view():\n",
    "    \"\"\"\n",
    "    Purpose: Provide a pre-visualization for articles in the GUI. Highlights the predicted key-words for the article.\n",
    "    Input: None. The article is selected in the GUI, and the LiteID accessible from this function.\n",
    "    Output: None. A visualization window is opened in the GUI. Close it by clicking again in the button in the GUI.\n",
    "    \"\"\"\n",
    "    global LIST_OF_VIEWERS, VIEW_DOCUMENT\n",
    "    if(doc_var_idx.get() != no_article): #Makes sure that an article has been selected\n",
    "        if(VIEW_DOCUMENT == False):\n",
    "            if search_query:\n",
    "                #if document selected\n",
    "                change_view_button['text'] = 'Close Preview'\n",
    "                vquery = \"MATCH (m:Article:_AI) WHERE EXISTS(m.content) AND m.liteId = $lite_id RETURN m.title AS title, m.content AS content, m.keywords_viewer_nst AS keywordsview\"\n",
    "                graph_query = graph.run(vquery, parameters={'lite_id': doc_var_idx.get()}).data()\n",
    "                nliteid_found = len(graph_query)\n",
    "                if (nliteid_found > 1):\n",
    "                    print(\"Wait, something is wrong with your LiteIDs. There should be a unique one per document.\")\n",
    "                    print(\"A have found %d documents with the LiteId: %d\" % (nliteid_found, doc_var_idx.get()))\n",
    "                title = 'Title: ' + graph_query[0]['title'] + '\\n'\n",
    "                text = clean_spacelines(extract_markups(html.unescape(graph_query[0]['content'])))\n",
    "                text = cleanClipText(text)\n",
    "                keywords = graph_query[0]['keywordsview']\n",
    "                viewer = tk.Text(inner_dwn_frame_b, bg='white', wrap='word', padx=20)\n",
    "                viewer.insert(tk.INSERT, title)\n",
    "                viewer.insert(tk.INSERT,text)\n",
    "                search_list = []\n",
    "                for keyword in keywords:\n",
    "                    if(len(keyword) > 2): search_list.append(keyword)\n",
    "                    search_list.append(keyword.capitalize()) #look for capitalized version of the words\n",
    "                    search_list.append(keyword.upper()) #look for upper-cased version of the words\n",
    "                for keyword in search_list:\n",
    "                    start = 1.0\n",
    "                    long = len(keyword)\n",
    "                    while True:\n",
    "                        pos = viewer.search(keyword, start, stopindex='end') #this is CASE-SENSITIVE\n",
    "                        if pos == \"\": break\n",
    "                        viewer.tag_add(\"here\", pos, pos+\"+%dc\" % (long))\n",
    "                        start = pos+\"+%dc\" % (long)\n",
    "                viewer.tag_config(\"here\", background=\"yellow\", foreground=\"blue\")\n",
    "                viewer.place(anchor='nw', relx=0, rely=0, relwidth=1, relheight=1)\n",
    "                LIST_OF_VIEWERS.append(viewer)\n",
    "                output_screen['text'] = \"You're previewing the text.\\n Highlighted you will find its keywords.\"\n",
    "        else:\n",
    "            change_view_button['text'] = 'Preview'\n",
    "            for element in LIST_OF_VIEWERS:\n",
    "                element.destroy()\n",
    "            LIST_OF_VIEWERS = []\n",
    "            output_screen['text'] = \"\"\n",
    "        VIEW_DOCUMENT = not VIEW_DOCUMENT\n",
    "    else:\n",
    "        output_screen['text'] = 'Select a document to view it.'\n",
    "    \n",
    "def sort_categories():\n",
    "    #Changes menu options in the GUI depending on the filter chosen (LtsFocusAreas, Megatrends, HScanning, None)\n",
    "    if(typevar.get() == TYPES[0]):\n",
    "        CATEGORIES = [\n",
    "        \"None\"\n",
    "        ]\n",
    "    elif(typevar.get() == TYPES[1]):\n",
    "        CATEGORIES = [\n",
    "        \"Business And Economy\",\n",
    "        \"Environment And Resources\",\n",
    "        \"Politics And Law\",\n",
    "        \"Society And Individuals\",\n",
    "        \"Technologies And Innovation\",\n",
    "        \"Miscellaneous\"\n",
    "        ]\n",
    "    elif(typevar.get() == TYPES[2]):\n",
    "        CATEGORIES = [\n",
    "        \"Autonomous Drive\",\n",
    "        \"Collaboration\",\n",
    "        \"Continuous Learning\",\n",
    "        \"Cyber Security\",\n",
    "        \"Data And Intelligence\",\n",
    "        \"Fleet Operators\",\n",
    "        \"Handling Complexity\",\n",
    "        \"Mobility Infrastructure\",\n",
    "        \"Playable Platform\",\n",
    "        \"Services\",\n",
    "        \"Urban Mobility\",\n",
    "        \"UX And Interactions\"\n",
    "        ]\n",
    "    elif(typevar.get() == TYPES[3]):\n",
    "        CATEGORIES = [\n",
    "        \"Demographic Changes\",\n",
    "        \"Diffusion of Power\",\n",
    "        \"Economic Growth\",\n",
    "        \"Globalization\",\n",
    "        \"Health And Well-being\",\n",
    "        \"Immaterialization\",\n",
    "        \"Individualization\",\n",
    "        \"Knowledge Society\",\n",
    "        \"Sustainability\",\n",
    "        \"Technology Development\"\n",
    "        ]\n",
    "    catvar.set(CATEGORIES[0])\n",
    "    \n",
    "    #Clean the categories menu\n",
    "    localmenu = catmenu[\"menu\"]\n",
    "    localmenu.delete(0,\"end\")\n",
    "    \n",
    "    for cat in CATEGORIES:\n",
    "        localmenu.add_command(label=cat, command=lambda value=cat: catvar.set(value))\n",
    "\n",
    "###########################\n",
    "##  HERE STARTS THE GUI  ##\n",
    "## --------------------  ##\n",
    "###########################\n",
    "\n",
    "canvas = tk.Canvas(root, height= HEIGHT, width= WIDTH)\n",
    "\n",
    "#Header\n",
    "header_frame = tk.Frame(root, bd=5, bg='#d9d9d9')\n",
    "header_frame.place(relx=0.5, rely=0, anchor='n', relwidth=1, relheight=0.1)\n",
    "title_h = tk.Label(header_frame, text=\"AI Applied to Knowledge Graphs\") #Title\n",
    "title_h.config(font=(\"Volvo Broad Pro\", 19))\n",
    "title_h.place(anchor='n', relx=0.5, rely=0, relwidth=0.6, relheight=0.5)\n",
    "title_s = tk.Label(header_frame, text=\"Horizon Scanning AI\") #Sub-title\n",
    "title_s.config(font=(\"Volvo Broad Pro\", 13))\n",
    "title_s.place(anchor='n', relx=0.5, rely=0.5, relwidth=0.6, relheight=0.5)\n",
    "insightlab_image = ImageTk.PhotoImage(Image.open('./img/insightlab_logo.png').resize((72,69)), master=root)\n",
    "insightlab_label = tk.Label(header_frame, image=insightlab_image) #Insight Lab logo\n",
    "insightlab_label.bind(\"<Triple-Button-3>\", insightlabcallback)\n",
    "insightlab_label.place(anchor='nw', relx=0.8, rely=0, relwidth=0.2, relheight=1)\n",
    "volvo_image = ImageTk.PhotoImage(Image.open('./img/volvo_logo2.png').resize((75,75)), master=root)\n",
    "volvo_label = tk.Label(header_frame, image=volvo_image) #Volvo Cars logo\n",
    "volvo_label.place(anchor='nw', relx=0, rely=0, relwidth=0.2, relheight=1)\n",
    "\n",
    "#Body\n",
    "body_frame = tk.Frame(root)\n",
    "body_frame.place(relx=0.5, rely=0.1, anchor='n', relwidth=1, relheight=0.77)\n",
    "\n",
    "# Body-header\n",
    "inner_upp_frame_b = tk.Frame(body_frame, bd=3)\n",
    "inner_upp_frame_b.place(anchor='nw', relx=0, rely=0, relwidth=1, relheight=0.1)\n",
    "inner_upp_left_frame = tk.Frame(inner_upp_frame_b)\n",
    "inner_upp_left_frame.place(anchor='nw', relx=0, rely=0, relwidth=0.2, relheight=1)\n",
    "inner_upp_center_frame = tk.Frame(inner_upp_frame_b)\n",
    "inner_upp_center_frame.place(anchor='nw', relx=0.2, rely=0, relwidth=0.50, relheight=1)\n",
    "inner_upp_right_frame = tk.Frame(inner_upp_frame_b)\n",
    "inner_upp_right_frame.place(anchor='nw', relx=0.7, rely=0, relwidth=0.3, relheight=1)\n",
    "\n",
    "lb_search = tk.Label(inner_upp_left_frame, text=\"Search by:\")\n",
    "lb_search.config(font=(TEXT_FONT, 10))\n",
    "lb_search.place(anchor='n', relx=0.5, rely=0.04, relwidth=0.92, relheight=0.44)\n",
    "\n",
    "lb_title = tk.Label(inner_upp_center_frame, text=\"Title\")\n",
    "lb_title.config(font=(TEXT_FONT, 9))\n",
    "lb_title.place(anchor='nw', relx=0.03, rely=0.04, relwidth=0.31, relheight=0.44)\n",
    "\n",
    "entry_title = tk.Entry(inner_upp_center_frame, justify='center') #Search bar\n",
    "entry_title.config(font=(TEXT_FONT, 9))\n",
    "entry_title.place(anchor='ne', relx=0.97, rely=0.04, relwidth=0.60, relheight=0.44)\n",
    "\n",
    "lb_type = tk.Label(inner_upp_right_frame, text=\"Type\")\n",
    "lb_type.config(font=(TEXT_FONT, 9))\n",
    "lb_type.place(anchor='nw', relx=0.04, rely=0.04, relwidth=0.3, relheight=0.44)\n",
    "\n",
    "TYPES = [\n",
    "\"None\",\n",
    "\"H. Scanning\",\n",
    "\"Lts Focus Area\",\n",
    "\"Megatrend\"\n",
    "]\n",
    "\n",
    "typevar = tk.StringVar(inner_upp_right_frame)\n",
    "typevar.set(TYPES[0]) # default value\n",
    "\n",
    "typemenu = tk.OptionMenu(inner_upp_right_frame, typevar, *TYPES, command=lambda e: sort_categories()) #Filter (Focus Areas, Megatrend, etc.)\n",
    "typemenu.config(font=(TEXT_FONT,9))\n",
    "typemenu.place(anchor='ne', relx=0.96, rely=0.04, relwidth=0.6, relheight=0.44)\n",
    "\n",
    "search_button = tk.Button(inner_upp_left_frame, text=\"Search\")\n",
    "search_button.config(font=(TEXT_FONT, 10))\n",
    "search_button.place(anchor='n', relx=0.5, rely=0.56, relwidth=0.6, relheight=0.40)\n",
    "\n",
    "change_view_button = tk.Button(inner_upp_right_frame, text=\"Preview\", command=lambda: change_view()) #Pre-visualization button\n",
    "change_view_button.config(font=(TEXT_FONT, 10))\n",
    "change_view_button.place(anchor='ne', relx=0.96, rely=0.56, relwidth=0.6, relheight=0.40)\n",
    "\n",
    "lb_cat = tk.Label(inner_upp_center_frame, text=\"Categories\")\n",
    "lb_cat.config(font=(TEXT_FONT, 9))\n",
    "lb_cat.place(anchor='nw', relx=0.03, rely=0.52, relwidth=0.31, relheight=0.44)\n",
    "\n",
    "catvar = tk.StringVar(inner_upp_center_frame)\n",
    "catvar.set(\"None\") # default value\n",
    "\n",
    "catmenu = tk.OptionMenu(inner_upp_center_frame, catvar, \"None\") #Categories existing under the filter\n",
    "catmenu.config(font=(TEXT_FONT,9))\n",
    "catmenu.place(anchor='ne', relx=0.97, rely=0.52, relwidth=0.6, relheight=0.44)\n",
    "\n",
    "# Body-main\n",
    "inner_dwn_frame_b = tk.Frame(body_frame)\n",
    "inner_dwn_frame_b.place(anchor='nw', relx=0, rely=0.1, relwidth=1, relheight=0.9)\n",
    "\n",
    "infoinsight_image = ImageTk.PhotoImage(Image.open('./img/info_insight.png').resize((69,40)), master=root)\n",
    "infoinsight_label = tk.Label(inner_dwn_frame_b, image=infoinsight_image) #Logo button: \"Give me some insight!\"\n",
    "infoinsight_label.bind(\"<Button-1>\", insightinfocallback)\n",
    "infoinsight_label.place(anchor='n', relx=0.5, rely=0.90, relwidth=0.25, relheight=0.1)\n",
    "\n",
    "\n",
    "#Footer\n",
    "footer_frame = tk.Frame(root, bd=8)\n",
    "footer_frame.place(relx=0.5, rely=0.87, anchor='n', relwidth=1, relheight=0.11)\n",
    "output_screen = tk.Label(footer_frame, justify='left', bg=\"white\", text=\"Welcome to the application.\") # GUI screen for messages and information\n",
    "output_screen.place(relx=0.025, rely=0.5, anchor='w', relheight=1, relwidth=0.48)\n",
    "inner_frame_f = tk.Frame(footer_frame)\n",
    "inner_frame_f.place(relx=0.975, rely=1, anchor='se', relwidth=0.27, relheight=0.8)\n",
    "fcc_button = tk.Button(inner_frame_f, text=\"Find Closest\") #Button for executing the search for closest documents\n",
    "fcc_button.config(font=(TEXT_FONT, 10))\n",
    "fcc_button.place(anchor='nw', relx=0, rely=0, relwidth=0.55, relheight=0.4)\n",
    "entry_docs = tk.Entry(inner_frame_f, justify='right') #Defines the amount of documents to be returned\n",
    "entry_docs.place(anchor='nw', relx=0.6, rely=0, relwidth=0.4, relheight=0.4)\n",
    "\n",
    "LST_features = []\n",
    "\n",
    "#Dropdown menu\n",
    "ALGORITHMS = [\n",
    "\"Word Embeddings\",\n",
    "\"TF-IDF\",\n",
    "#\"Doc2vec\",\n",
    "\"LSA\",\n",
    "\"LDA\",\n",
    "\"Ensemble Method\",\n",
    "\"Community Finding\"\n",
    "]\n",
    "\n",
    "algorithmvariable = tk.StringVar(inner_frame_f)\n",
    "algorithmvariable.set(ALGORITHMS[0]) # default value\n",
    "\n",
    "algorithmenu = tk.OptionMenu(inner_frame_f, algorithmvariable, *ALGORITHMS) #Selection of the algorithm to use!\n",
    "algorithmenu.config(font=(\"Volvo Broad Pro\",11))\n",
    "algorithmenu.place(anchor=\"se\", relx=1, rely=1, relwidth=1, relheight=0.5)\n",
    "\n",
    "canvas.pack()\n",
    "\n",
    "#Initialization of variables, load models, initialize the py2neo v4 Neo4j backend, checks the status of the database\n",
    "try:\n",
    "    #Connect to Neo4j Database\n",
    "    graph = Graph(auth=('user','password'), host=\"gotsvl1706.got.volvocars.net\", port=7687, secure=True)\n",
    "    exists_database = graph.run(\"MATCH (n:Article:_AI) RETURN n LIMIT 1\").data() #Checks if there is something in the database or is empty\n",
    "except Exception as e:\n",
    "    #This happens if there is no connection with the database, normally\n",
    "    tk.messagebox.showwarning(\"Warning: No Connection\", \"Warning: No connection with Neo4j. Please, make sure that Neo4j is running and you have entered the correct password.\")\n",
    "    root.destroy() #This destroys the application (close it)\n",
    "else:\n",
    "    if not exists_database: #If the connection has been made, but the database is empty (it will create one)\n",
    "        tk.messagebox.showinfo(\"Info\",\"Info: The Knowledge Graph database is empty.\\nWe will need to prepare it for you.\\nThis may take several minutes.\")\n",
    "        # Load Neo4j JSON\n",
    "        try:\n",
    "            load_articlesNeo4j() # Load JSON file from the import folder\n",
    "            preprocess_articlesNeo4j() # Pre-filter the documents imported\n",
    "            process_documentsNeo4j() # Process the content and extract significant words\n",
    "            clean_empty_processed_docs() #Post-filter the documents that have been processed\n",
    "            create_LiteId_documents() #This has to be done only once\n",
    "        except Exception as e:\n",
    "            print(\"Something went wrong loading the articles into Neo4j.\\nCheck that the JSON file is in the import folder and try again.\")\n",
    "            print(\"If the problem persists, check the code functions.\")\n",
    "        try:\n",
    "            process_wordembeddingsNeo4j() #OBS!: Only do this if you don't have GloVe in Neo4j and you want to.\n",
    "        except Exception as e:\n",
    "            print(\"Something went wrong loading GloVe embeddings into Neo4j.\\nCheck that the file exists in the import folder and try again.\")\n",
    "            print(\"If the problem persists, check the code function, the format and dimensions of the CSV file.\")\n",
    "    check_coherence = check_documents()\n",
    "    if check_coherence:\n",
    "        # Load/Create the corpus\n",
    "        # Note: All this strings should maybe appear in the GUI\n",
    "        corpus_memory_friendly = MyCorpusDashNeo()\n",
    "        corpus_memory_friendly_NST = MyCorpusNeoNST()\n",
    "        #(Careful, these are right now generators, or they will yield those)\n",
    "        #Some functions might be deprecated for generators in the future.\n",
    "        \n",
    "        #TF-IDF Load\n",
    "        if not os.path.isfile(index_tfidf_name):\n",
    "            # Create the TF-IDF model\n",
    "            tfidf = models.TfidfModel((bow for bow in corpus_memory_friendly), normalize=True)\n",
    "            compare_docs_query = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) RETURN m.preprocessed_stemmed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "            # Note: Tests can be done in in-memory batches for large datasets\n",
    "            compare_docs_bow = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in compare_docs_query]\n",
    "            compare_tfidf = tfidf[compare_docs_bow]\n",
    "            index_tfidf = similarities.Similarity(output_prefix=\"sim_tfidf_idx\", corpus=compare_tfidf, num_features=len(corpus_memory_friendly.dictionary))\n",
    "            tfidf.save(index_tfidf_model_name)\n",
    "            index_tfidf.save(index_tfidf_name)\n",
    "        \n",
    "        #TF-IDF Load (NST-Version) (Non-stemmed)\n",
    "        if not os.path.isfile(index_tfidf_name_nst):\n",
    "            # (This will be Feedly dashboards)\n",
    "            tfidf_nst = models.TfidfModel((bow for bow in corpus_memory_friendly_NST), normalize=True)\n",
    "            compare_docs_query_nst = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.preprocessed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "            # Note: Tests can be done in in-memory batches for large datasets !\n",
    "            compare_docs_bow_nst = [corpus_memory_friendly_NST.dictionary.doc2bow(doc['preprocessed']) for doc in compare_docs_query_nst]\n",
    "            compare_tfidf_nst = tfidf_nst[compare_docs_bow_nst]\n",
    "            index_tfidf_nst = similarities.Similarity(output_prefix=\"sim_tfidf_idx_nst\", corpus=compare_tfidf_nst, num_features=len(corpus_memory_friendly_NST.dictionary))\n",
    "            # will the length of features be same if we include the entire Feedly? (Nope)\n",
    "            tfidf_nst.save(index_tfidf_model_name_nst)\n",
    "            index_tfidf_nst.save(index_tfidf_name_nst)\n",
    "        stemmed_tfidf = True #This variable defines whether to use the stemmed version or the nst-version in the application\n",
    "        try:\n",
    "            tfidf = models.TfidfModel.load(index_tfidf_model_name)\n",
    "            index_tfidf = similarities.Similarity.load(index_tfidf_name)\n",
    "            tfidf_nst = models.TfidfModel.load(index_tfidf_model_name_nst)\n",
    "            index_tfidf_nst = similarities.Similarity.load(index_tfidf_name_nst)\n",
    "        except RuntimeError:\n",
    "            print(\"Something went wrong. Please check that the TF-IDF index exists.\")\n",
    "        evaluate_keywordsNeo4j()    #predict and extract the key-words from the articles (stem version)\n",
    "        evaluate_keywordsNeo4jNST() #predict and extract the key-words from the articles (nst-version)\n",
    "        \n",
    "        #Latent Semantic Analysis (LSA) Load\n",
    "        if not os.path.isfile(index_lsi_model_name):\n",
    "            # (This will be Feedly dashboards)\n",
    "            lsi = models.LsiModel(corpus_memory_friendly, id2word=corpus_memory_friendly.dictionary, num_topics=37) #to tune (49 prev)\n",
    "            compare_docs_query = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) RETURN m.preprocessed_stemmed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "            # Note: Tests can be done in in-memory batches for large datasets !\n",
    "            compare_docs_bow = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in compare_docs_query]\n",
    "            compare_lsi = lsi[compare_docs_bow]\n",
    "            index_lsi = similarities.Similarity(output_prefix=\"sim_lsi_idx\", corpus=compare_lsi, num_features=len(corpus_memory_friendly.dictionary))\n",
    "            lsi.save(index_lsi_model_name)\n",
    "            index_lsi.save(index_lsi_name)\n",
    "        \n",
    "        #Latent Semantic Analysis (LSA) Load (NST-Version)\n",
    "        if not os.path.isfile(index_lsi_model_name_nst):\n",
    "            lsi_nst = models.LsiModel(corpus_memory_friendly_NST, id2word=corpus_memory_friendly_NST.dictionary, num_topics=40) #to tune (49 prev)\n",
    "            compare_docs_query_nst = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.preprocessed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "            # Note: Tests can be done in in-memory batches for large datasets !\n",
    "            compare_docs_bow_nst = [corpus_memory_friendly_NST.dictionary.doc2bow(doc['preprocessed']) for doc in compare_docs_query_nst]\n",
    "            compare_lsi_nst = lsi_nst[compare_docs_bow_nst]\n",
    "            index_lsi_nst = similarities.Similarity(output_prefix=\"sim_lsi_nst_idx\", corpus=compare_lsi_nst, num_features=len(corpus_memory_friendly_NST.dictionary))\n",
    "            lsi_nst.save(index_lsi_model_name_nst)\n",
    "            index_lsi_nst.save(index_lsi_name_nst)\n",
    "        try:\n",
    "            lsi = models.LsiModel.load(index_lsi_model_name)\n",
    "            index_lsi = similarities.Similarity.load(index_lsi_name)\n",
    "            lsi_nst = models.LsiModel.load(index_lsi_model_name_nst)\n",
    "            index_lsi_nst = similarities.Similarity.load(index_lsi_name_nst)\n",
    "        except RuntimeError:\n",
    "            print(\"Something went wrong. Please check that the LSI index exists.\")\n",
    "        \n",
    "        #Latent Dirichlet Allocation (LDA) Load\n",
    "        if not os.path.isfile(index_lda_name):\n",
    "            # (This will be Feedly dashboards)\n",
    "            lda = models.LdaModel(corpus_memory_friendly, id2word=corpus_memory_friendly.dictionary, num_topics=12, passes=12, alpha='auto') #to tune, eval_every=5\n",
    "            ldacompare_docs_query = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) RETURN m.preprocessed_stemmed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "            # Note: Training can be done online for large datasets !\n",
    "            compare_docs_bow = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in ldacompare_docs_query]\n",
    "            compare_lda = lda[compare_docs_bow]\n",
    "            index_lda = similarities.Similarity(output_prefix=\"sim_lda_idx\", corpus=compare_lda, num_features=len(corpus_memory_friendly.dictionary))\n",
    "            # will the length of features be same if we include the entire Feedly?\n",
    "            lda.save(index_lda_model_name)\n",
    "            index_lda.save(index_lda_name)\n",
    "        \n",
    "        #Latent Dirichlet Allocation (LDA) Load (NST-Version)\n",
    "        if not os.path.isfile(index_lda_name_nst):\n",
    "            # (This will be Feedly dashboards)\n",
    "            lda_nst = models.LdaModel(corpus_memory_friendly_NST, id2word=corpus_memory_friendly_NST.dictionary, num_topics=12, passes=10, alpha='auto') #to tune, eval_every=5\n",
    "            ldacompare_docs_query = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed) RETURN m.preprocessed AS preprocessed ORDER BY m.liteId ASC\")\n",
    "            # Note: Training can be done online for large datasets !\n",
    "            compare_docs_bow = [corpus_memory_friendly_NST.dictionary.doc2bow(doc['preprocessed']) for doc in ldacompare_docs_query]\n",
    "            compare_lda = lda_nst[compare_docs_bow]\n",
    "            index_lda_nst = similarities.Similarity(output_prefix=\"sim_lda_idx\", corpus=compare_lda, num_features=len(corpus_memory_friendly_NST.dictionary))\n",
    "            # will the length of features be same if we include the entire Feedly?\n",
    "            lda_nst.save(index_lda_model_name_nst)\n",
    "            index_lda_nst.save(index_lda_name_nst)\n",
    "        try:\n",
    "            #lda = models.LdaModel.load(os.path.join(dir_name, \"LDA_MAX.model\"))\n",
    "            #index_lda = similarities.Similarity.load(os.path.join(dir_name, \"LDA_MAX_INDEX.index\"))\n",
    "            lda = models.LdaModel.load(index_lda_model_name)\n",
    "            index_lda = similarities.Similarity.load(index_lda_name)\n",
    "            lda_nst = models.LdaModel.load(index_lda_model_name_nst)\n",
    "            index_lda_nst = similarities.Similarity.load(index_lda_name_nst)\n",
    "        except RuntimeError:\n",
    "            print(\"Something went wrong. Please check that the LSI index exists.\")\n",
    "            \n",
    "        # List-of-documents for the first visualization and Welcome to the application's GUI\n",
    "        left_dlist = tk.Frame(inner_dwn_frame_b, bg='white')\n",
    "        left_dlist.place(anchor='nw', relx=0.05, rely=0.05, relwidth=0.43, relheight=0.85)\n",
    "        left_dlistcanvas = tk.Canvas(left_dlist, bg='white')\n",
    "        left_scrollbar = tk.Scrollbar(left_dlist, orient='vertical')\n",
    "        left_dlistframe = tk.Frame(left_dlistcanvas)\n",
    "        window = left_dlistcanvas.create_window(0, 0, anchor='nw', window=left_dlistframe, width=314)\n",
    "        left_scrollbar.pack(fill='y', side='right')\n",
    "        LIST_OF_VIEWERS = []\n",
    "        doc_insight = tk.IntVar() #for the Insight! functionality\n",
    "        doc_insight.set(0) #set query document as pre-defined\n",
    "        doc_var_idx = tk.IntVar() #to track the selected document in the left column\n",
    "        LIST_DOCUMENTS = []\n",
    "        LIST_NOT_FOUND_LABELS = []\n",
    "        header_query = \"MATCH (m:Article:_AI) WHERE EXISTS(m.content) RETURN m.title AS title, m.liteId AS LID ORDER BY m.liteId ASC\"\n",
    "        search_query = graph.run(header_query).data()\n",
    "        for idd,doc in enumerate(search_query):\n",
    "            r = tk.Radiobutton(left_dlistframe, text=doc['title'], selectcolor='#e6f2ff', wraplength=255, bg='white', relief='ridge', overrelief='ridge', indicatoron=False, variable=doc_var_idx, value=doc['LID'])\n",
    "            r.config(font=(FONT_ARTICLES, 9))\n",
    "            r.pack(anchor = 'w', fill='x')\n",
    "            LIST_DOCUMENTS.append(r)\n",
    "        left_dlistcanvas.update_idletasks()\n",
    "        doc_var_idx.set(no_article)\n",
    "        left_scrollbar.config(command=left_dlistcanvas.yview)\n",
    "        left_dlistcanvas.configure(scrollregion=left_dlistcanvas.bbox('all'), yscrollcommand=left_scrollbar.set)\n",
    "        left_dlistcanvas.pack(fill='both', side='left')\n",
    "        left_dlistcanvas.bind('<Enter>', _bound_to_mousewheel)\n",
    "        left_dlistcanvas.bind('<Leave>', _unbound_to_mousewheel)\n",
    "\n",
    "        doc_sim_idx = tk.IntVar()\n",
    "        LIST_SIM_DOCS = []\n",
    "\n",
    "        # List of similar documents\n",
    "        right_dlist = tk.Frame(inner_dwn_frame_b, bg='white')\n",
    "        right_dlist.place(anchor='ne', relx=0.95, rely=0.05, relwidth=0.43, relheight=0.85)\n",
    "        right_dlistcanvas = tk.Canvas(right_dlist, bg='white')\n",
    "        right_scrollbar = tk.Scrollbar(right_dlist, orient='vertical')\n",
    "        right_dlistframe = tk.Frame(right_dlistcanvas)\n",
    "        rwindow = right_dlistcanvas.create_window(0, 0, anchor='nw', window=right_dlistframe, width=314)\n",
    "        right_scrollbar.pack(fill='y', side='right')\n",
    "        doc_sim_idx.set(no_article)\n",
    "        right_scrollbar.config(command=right_dlistcanvas.yview)\n",
    "        right_dlistcanvas.configure(scrollregion=right_dlistcanvas.bbox('all'), yscrollcommand=right_scrollbar.set)\n",
    "        right_dlistcanvas.pack(fill='both', side='left')\n",
    "        right_dlistcanvas.bind('<Enter>', _bound_to_mousewheelsim)\n",
    "        right_dlistcanvas.bind('<Leave>', _unbound_to_mousewheelsim)\n",
    "        \n",
    "        search_button.config(command=lambda : search_documents())\n",
    "        fcc_button.config(command=lambda: find_similardocuments())\n",
    "        entry_title.bind(\"<Return>\", keyentertitlecallback)\n",
    "        entry_docs.bind(\"<Return>\", keyenterdocsamountcallback)\n",
    "    else:\n",
    "        tk.messagebox.showerror(\"Error\", \"Error: The Graph database is not coherent. The LiteID do not match with the articles. Please, check Neo4j.\")\n",
    "        delete_database = tk.messagebox.askyesno(\"\",\"Do you want to delete the database before exiting?\\nNext time you open, it will be created again.\")\n",
    "        if delete_database:\n",
    "            try:\n",
    "                graph.run(\"MATCH (n:_AI) WHERE NOT '_GlobalConfigurationControl' IN labels(n) DETACH DELETE n\")\n",
    "                for the_file in os.listdir(dir_name):\n",
    "                    file_path = os.path.join(dir_name, the_file)\n",
    "                    if os.path.isfile(file_path):\n",
    "                        os.unlink(file_path)\n",
    "            except Exception as e:\n",
    "                tk.messagebox.showinfo(\"\", \"Couldn't delete the database. The error is:\\n\" + e)\n",
    "            else:\n",
    "                tk.messagebox.showinfo(\"\", \"Database deleted.\")\n",
    "        root.destroy()\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUTURE UPDATE FUNCTION\n",
    "def update_database_Neo4j():\n",
    "    #TODO: API Request JSON, do the middle step conversion, load merge new documents, clean the ones without content, preprocess them, update the tf-idf model and the dictionary, assign keywords, assign LiteId to them \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check in the dictionary for weird or composed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,word in iteritems(corpus_memory_friendly_NST.dictionary):\n",
    "    for punt in punctuation_marks_extended:\n",
    "        if punt in word:\n",
    "            print(word)\n",
    "    if '-' in word:\n",
    "        print(word)\n",
    "    if(len(word) < 2): print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculator for different scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Example_to_calculate = [4,2,'x'] # Ranks: [TF-IDF, LSA, Word_Embeddings]\n",
    "#This is just a few ways to combine the results form the algorithms into one single weight. ('x' means that there is no recommendation from that algorithm)\n",
    "MRR = 0\n",
    "RBP = 0\n",
    "p = 0.85\n",
    "maxrank = 10\n",
    "semscore = 0\n",
    "occurrences = 0\n",
    "length = len(Example_to_calculate)\n",
    "for index, rank in enumerate(Example_to_calculate):\n",
    "    if not isinstance(rank,int): Example_to_calculate[index] = maxrank\n",
    "    else: occurrences += 1\n",
    "\n",
    "for rank in Example_to_calculate:\n",
    "    MRR += 1/rank\n",
    "    semscore += (maxrank - rank)\n",
    "    RBP += p**rank\n",
    "\n",
    "#Normal Score used here\n",
    "score = semscore/((maxrank - 1)*length)\n",
    "print(\"The unmodified Score is: \" + str(score))\n",
    "\n",
    "#Modified Score with penalization for unmatching algorithms\n",
    "final_score = ((occurrences-1)+semscore/((maxrank-1)*occurrences))/length\n",
    "print(\"The modified Score: \" + str(final_score))\n",
    "\n",
    "#Mean reciprocal rank (MRR)\n",
    "MRR /= length\n",
    "print(\"The Mean Reciprocal Rank is: \" + str(MRR))\n",
    "\n",
    "#RBP score normalized\n",
    "RBP /= p*length\n",
    "print(\"The rank exponent score is: \" + str(RBP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the results from the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = False\n",
    "if merge:\n",
    "    store_recommendations_Neo4j()\n",
    "    merge_recommendations_Neo4j()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing results in Matlab format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import scipy.io\n",
    "\n",
    "export_MATLAB = False #Choose if you want to export some models to MATLAB format\n",
    "\n",
    "if(export_MATLAB):\n",
    "    \n",
    "    #TF-IDF\n",
    "    tf_idfmatrix = similarities.Similarity.load(index_tfidf_name)\n",
    "    corpus_test = MyCorpusDashNeo()\n",
    "    tfidf_test = models.TfidfModel.load(index_tfidf_model_name)\n",
    "    index_tfidf_test = similarities.Similarity.load(index_tfidf_name)\n",
    "    corpus_tfidf = tfidf_test[corpus_test]\n",
    "    sim = index_tfidf_test[corpus_tfidf]\n",
    "    scipy.io.savemat('matrix_tf_idf.mat', dict(x=sim))\n",
    "\n",
    "    #LSA\n",
    "    docs = graph.run(\"MATCH (m:Article:_AI) WHERE EXISTS(m.preprocessed_stemmed) RETURN m.preprocessed_stemmed AS preprocessed ORDER BY m.liteId ASC\").data()\n",
    "    bow_test = [corpus_memory_friendly.dictionary.doc2bow(doc['preprocessed']) for doc in docs]\n",
    "    lsi = models.LsiModel.load(index_lsi_model_name)\n",
    "    index = similarities.Similarity.load(index_lsi_name)\n",
    "    docs_lsi = lsi[bow_test]\n",
    "    simlsa = index[docs_lsi]\n",
    "    scipy.io.savemat('matrix_lsa.mat', dict(x=simlsa))\n",
    "    \n",
    "    #Word Embeddings\n",
    "    x = np.load(index_WE_model_eu_name_paper)\n",
    "    scipy.io.savemat('matrix_euclidean_dist_paper.mat', dict(x=x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lpa = False\n",
    "if run_lpa:\n",
    "    for i in range(1000):\n",
    "        query = \"\"\"\n",
    "        MATCH (e1:Article:`_AI` {cluster:\"cluster_9.495788516814935\"})<-[d1:RELATES_TO]-(e2:Article:`_AI` {cluster:\"cluster_9.495788516814935\"})\n",
    "        WHERE exists(e2.cluster_sp1)\n",
    "        WITH e1, count(e2.cluster_sp1) AS cluster_count, e2.cluster_sp1 AS cluster_prop\n",
    "        SET e1.cluster_sp1 = CASE WHEN cluster_count >= e1.clusterCount THEN cluster_prop ELSE e1.cluster_sp1 END,\n",
    "        e1.clusterCount = CASE WHEN cluster_count >= e1.clusterCount THEN cluster_count ELSE e1.clusterCount END\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        MATCH (e1:Article:`_AI`)<-[d1:RELATES_TO]-(e2:Article:`_AI`)\n",
    "        WHERE exists(e2.cluster)\n",
    "        WITH e1, count(e2.cluster) AS cluster_count, e2.cluster AS cluster_prop\n",
    "        SET e1.cluster = CASE WHEN cluster_count >= e1.clusterCount THEN cluster_prop ELSE e1.cluster END,\n",
    "        e1.clusterCount = CASE WHEN cluster_count >= e1.clusterCount THEN cluster_count ELSE e1.clusterCount END\n",
    "        \"\"\"\n",
    "        graph.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_wordcloud = True\n",
    "def create_input_wordcloud(input_item):\n",
    "    if isinstance(input_item, str): #community\n",
    "        pass\n",
    "    elif isinstance(input_item, int): #lite_id document\n",
    "        query = \"MATCH (n:Article:_AI {liteId: $lite_id}) RETURN n.preprocessed_stemmed AS result\" #we can do it with content, keywords...\n",
    "        data = graph.run(query, parameters={'lite_id': input_item}).data()\n",
    "        string_wordcloud = \"\"\n",
    "        for word in data[0]['result']:\n",
    "            string_wordcloud += word + \" \"\n",
    "    return string_wordcloud\n",
    "        \n",
    "    \n",
    "if run_wordcloud:\n",
    "    mask = np.array(Image.open(os.path.join(dir_name, 'wordclouds/cloud.png')))\n",
    "    wc = WordCloud(background_color=\"white\", mask=mask, max_words=200, stopwords=stopwords.words('english'))\n",
    "    text = create_input_wordcloud(135)\n",
    "    wc.generate(text)\n",
    "    wc.to_file(os.path.join(dir_name, 'wordclouds/cloud_135_stem.png'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = graph.run(\"MATCH (n:Article) WHERE n.title=~ '(?i).*Molly Sauter.*' RETURN n.content AS content, n.preprocessed_stemmed AS prep LIMIT 1\").data()\n",
    "bowdata = corpus_memory_friendly.dictionary.doc2bow(data[0]['prep'])\n",
    "print(\"Original content:\")\n",
    "print(data[0]['content'], end=\"\\n\\n\")\n",
    "print(\"Pre-processed content:\")\n",
    "print(data[0]['prep'], end=\"\\n\\n\")\n",
    "print(\"Bag of words:\")\n",
    "print(bowdata, end=\"\\n\\n\")\n",
    "tfidfdata = tfidf[bowdata]\n",
    "print(\"TF-IDF transformation:\")\n",
    "print(tfidfdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example of topics:\")\n",
    "print(\"------------------\")\n",
    "lsi.show_topics(num_topics=12, num_words=6, formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = graph.run(\"MATCH (n:Article) WHERE n.liteId = 135 RETURN DISTINCT n.preprocessed AS prep LIMIT 1\").data()\n",
    "test = corpus_memory_friendly.dictionary.doc2bow(a[0]['prep'])\n",
    "test = lsi[test]\n",
    "ind = index_lsi[test]\n",
    "print(\"\\n\\nSimilarity scores from Gensim:similarities.docsim.Similarity\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(ind)\n",
    "print(\"\\n\\nSimilarity scores from Gensim:similarities.docsim.Similarity (enumerated)\")\n",
    "print(\"-------------------------------------------------------------------------\")\n",
    "print([val for val in enumerate(ind)])\n",
    "#docs_similar = index_tfidf[doc_tfidf]\n",
    "print(\"\\n\\nSimilarity scores from Gensim:similarities.docsim.Similarity (enumerated and ordered)\")\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "sort_ind = sorted(enumerate(ind), key=lambda item: -item[1])\n",
    "print(sort_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"EU puts weight behind Wi-Fi over 5G for connected cars. As the industry decides what connectivity should be used for connected cars, the EU appears to be putting its weight behind Wi-Fi over 5G\"\n",
    "text_2= \"The European Union (EU) has agreed to cut carbon emissions from cars by 37.5% within a decade. The new targets are part of a wide EU push to reduce total greenhouse gas emissions\"\n",
    "\n",
    "text_1_proc = preprocess_text(text_1)\n",
    "text_2_proc = preprocess_text(text_2)\n",
    "\n",
    "print(text_1_proc)\n",
    "print(text_2_proc, end=\"\\n\\n\")\n",
    "\n",
    "dictionary_texts = corpora.Dictionary(text for text in [text_1_proc, text_2_proc])\n",
    "print(dictionary_texts)\n",
    "print(dictionary_texts.token2id, end=\"\\n\\n\")\n",
    "\n",
    "text_1_bow = dictionary_texts.doc2bow(text_1_proc)\n",
    "print(text_1_bow)\n",
    "text_2_bow = dictionary_texts.doc2bow(text_2_proc)\n",
    "print(text_2_bow, end=\"\\n\\n\")\n",
    "\n",
    "tfidf_texts = models.TfidfModel([bow for bow in [text_1_bow, text_2_bow]], normalize=True)\n",
    "text_1_tfidf = tfidf_texts[text_1_bow]\n",
    "text_2_tfidf = tfidf_texts[text_2_bow]\n",
    "\n",
    "print(text_1_tfidf)\n",
    "print(text_2_tfidf, end=\"\\n\\n\")\n",
    "\n",
    "lsa_texts = models.LsiModel([text_1_bow, text_2_bow], id2word=dictionary_texts, num_topics=2)\n",
    "text_1_lsa = lsa_texts[text_1_bow]\n",
    "text_2_lsa = lsa_texts[text_2_bow]\n",
    "\n",
    "print(text_1_lsa)\n",
    "print(text_2_lsa, end=\"\\n\\n\")\n",
    "\n",
    "lsa_texts.print_topics(num_words = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
